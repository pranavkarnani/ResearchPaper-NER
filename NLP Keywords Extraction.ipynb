{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['structured_chinese.json',\n",
       " '.DS_Store',\n",
       " 'structured_persian.json',\n",
       " 'structured_turkish.json',\n",
       " 'structured_spanish.json',\n",
       " 'structured_english.json',\n",
       " 'structured_bengali.json',\n",
       " 'NLP-progress-master',\n",
       " 'structured_korean.json',\n",
       " 'structured_russian.json',\n",
       " 'NLP Keywords Extraction.ipynb',\n",
       " 'structured_german.json',\n",
       " 'structured_nepali.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'structured_vietnamese.json',\n",
       " 'structured_hindi.json',\n",
       " 'structured_arabic.json',\n",
       " 'structured_french.json']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['english','korean','vietnamese','hindi','german','turkish','russian','portugese','french','spanish','chinese','persian','arabic','nepali','bengali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing `question_answering.md`...\n",
      "Processing `vietnamese.md`...\n",
      "Processing `hindi.md`...\n",
      "Processing `common_sense.md`...\n",
      "Processing `data_to_text_generation.md`...\n",
      "Processing `summarization.md`...\n",
      "Processing `taxonomy_learning.md`...\n",
      "Processing `dialogue.md`...\n",
      "Processing `automatic_speech_recognition.md`...\n",
      "Processing `shallow_syntax.md`...\n",
      "Processing `coreference_resolution.md`...\n",
      "Processing `sentiment_analysis.md`...\n",
      "Processing `semantic_role_labeling.md`...\n",
      "Processing `constituency_parsing.md`...\n",
      "Processing `word_sense_disambiguation.md`...\n",
      "Processing `text_classification.md`...\n",
      "Processing `intent_detection_slot_filling.md`...\n",
      "Processing `grammatical_error_correction.md`...\n",
      "Processing `dependency_parsing.md`...\n",
      "Processing `semantic_textual_similarity.md`...\n",
      "Processing `simplification.md`...\n",
      "Processing `semantic_parsing.md`...\n",
      "Processing `information_extraction.md`...\n",
      "Processing `entity_linking.md`...\n",
      "Processing `ccg.md`...\n",
      "Processing `named_entity_recognition.md`...\n",
      "Processing `missing_elements.md`...\n",
      "Processing `lexical_normalization.md`...\n",
      "Processing `temporal_processing.md`...\n",
      "Processing `question_answering.md`...\n",
      "Processing `relationship_extraction.md`...\n",
      "Processing `domain_adaptation.md`...\n",
      "Processing `paraphrase-generation.md`...\n",
      "Processing `part-of-speech_tagging.md`...\n",
      "Processing `relation_prediction.md`...\n",
      "Processing `machine_translation.md`...\n",
      "Processing `natural_language_inference.md`...\n",
      "Processing `multimodal.md`...\n",
      "Processing `language_modeling.md`...\n",
      "Processing `stance_detection.md`...\n",
      "Processing `multi-task_learning.md`...\n",
      "Processing `summarization.md`...\n",
      "Processing `question_answering.md`...\n",
      "Processing `summarization.md`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This row doesn't have enough columns, skipping: | Goyal et al. (2019) | 19.06 | [LTRC-MT Simple & Effective Hindi-English Neural Machine Translation Systems at WAT 2019](https://www.aclweb.org/anthology/D19-5216.pdf) \n",
      "\n",
      "This row doesn't have enough columns, skipping: | DAH-CRF (Li et al., 2019) | 82.3 | [A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification](https://www.aclweb.org/anthology/K19-1036.pdf)\n",
      "\n",
      "This row doesn't have enough columns, skipping: | ALDMN (Wan et al., 2018) | 81.5 | [Improved Dynamic Memory Network for Dialogue Act Classification with Adversarial Training](https://arxiv.org/pdf/1811.05021.pdf)\n",
      "\n",
      "This row doesn't have enough columns, skipping: | DAH-CRF (Li et al., 2019) | 92.2 | [A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification](https://www.aclweb.org/anthology/K19-1036.pdf)\n",
      "\n",
      "This row doesn't have enough columns, skipping: | CASA (Raheja et al., 2019) | 91.1 | [Dialogue Act Classification with Context-Aware Self-Attention](https://www.aclweb.org/anthology/N19-1373.pdf)\n",
      "\n",
      "This row doesn't have enough columns, skipping: | SGNN (Ravi et al., 2018) | 86.7 | [Self-Governing Neural Networks for On-Device Short Text Classification](https://www.aclweb.org/anthology/D18-1105.pdf)\n",
      "\n",
      "ERROR parsing the subdataset SOTA tables\n",
      "[['| Data   | Model           |  R_100@1    |  R_100@10   |  R_100@50   |  MRR        |  Paper / Source |\\n', '| ------ | -------------   | :---------: | :---------: | :---------: | :---------: |---------------|\\n', '| DSTC 8 (main) | Wu et. al., (2020) | 76.1 | 97.9 | - | 84.8 | Enhancing Response Selection with Advanced Context Modeling and Post-training |\\n', '| DSTC 8 (subtask 2) | Wu et. al., (2020) | 70.6 | 95.7 | - | 79.9 | Enhancing Response Selection with Advanced Context Modeling and Post-training |\\n', '| DSTC 7 | Seq-Att-Network (Chen and Wang, 2019) | 64.5 | 90.2 | 99.4 | 73.5 | [Sequential Attention-based Network for Noetic End-to-End Response Selection](http://workshop.colips.org/dstc7/papers/07.pdf) |\\n'], ['| Data   | Model           | R_2@1       |  R_10@1      |  Paper / Source |\\n', '| ------ | -------------   | :---------: | :---------: |---------------|\\n', '| UDC v2 | DAM (Zhou et al. 2018) | 93.8 | 76.7 | [Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network](http://www.aclweb.org/anthology/P18-1103) |\\n', '| UDC v2  | SMN (Wu et al. 2017) | 92.3 | 72.3 | [Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots](https://arxiv.org/pdf/1612.01627.pdf) |\\n', '| UDC v2  | Multi-View (Zhou et al. 2017) | 90.8 | 66.2 | [Multi-view Response Selection for Human-Computer Conversation](https://aclweb.org/anthology/D16-1036) |\\n', '| UDC v2  | Bi-LSTM (Kadlec et al. 2015) | 89.5 | 63.0 | [Improved Deep Learning Baselines for Ubuntu Corpus Dialogs](https://arxiv.org/pdf/1510.03753.pdf) |\\n']]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/faizankhan/Desktop/Academics/Sem3/ANLP/Assignment2/NLP-progress-master/structured/export.py\", line 477, in <module>\n",
      "    out.extend(parse_markdown_file(path))\n",
      "  File \"/Users/faizankhan/Desktop/Academics/Sem3/ANLP/Assignment2/NLP-progress-master/structured/export.py\", line 329, in parse_markdown_file\n",
      "    with open(md_file, \"r\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/Users/faizankhan/Desktop/Academics/Sem3/ANLP/Assignment2/NLP-progress-master/portugese'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing `summarization.md`...\n",
      "Processing `question_answering.md`...\n",
      "Processing `sentiment-analysis.md`...\n",
      "Processing `summarization.md`...\n",
      "Processing `question_answering.md`...\n",
      "Processing `summarization.md`...\n",
      "Processing `entity_linking.md`...\n",
      "Processing `named_entity_recognition.md`...\n",
      "Processing `chinese_word_segmentation.md`...\n",
      "Processing `chinese.md`...\n",
      "Processing `question_answering.md`...\n",
      "Processing `summarization.md`...\n",
      "Processing `named_entity_recognition.md`...\n",
      "Processing `natural_language_inference.md`...\n",
      "Processing `language_modeling.md`...\n",
      "Processing `nepali.md`...\n",
      "Processing `sentiment_analysis.md`...\n",
      "Processing `part_of_speech_tagging.md`...\n",
      "Processing `question_answering.md`...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "#subprocess.Popen(\"script2.py 1\", shell=True)\n",
    "for folder in folders:\n",
    "    complete_folder_path = \"/Users/faizankhan/Desktop/Academics/Sem3/ANLP/Assignment2/NLP-progress-master/\" + folder\n",
    "    command = \"python3 /Users/faizankhan/Desktop/Academics/Sem3/ANLP/Assignment2/NLP-progress-master/structured/export.py \" + complete_folder_path + \" --output structured_\" + folder + \".json\"\n",
    "    subprocess.Popen(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = ['structured_chinese.json','structured_persian.json','structured_turkish.json','structured_spanish.json',\n",
    " 'structured_english.json',\\\n",
    " 'structured_bengali.json',\\\n",
    " 'structured_korean.json',\\\n",
    " 'structured_russian.json',\\\n",
    " 'structured_german.json',\\\n",
    " 'structured_nepali.json',\\\n",
    " 'structured_vietnamese.json',\\\n",
    " 'structured_hindi.json',\\\n",
    " 'structured_arabic.json',\\\n",
    " 'structured_french.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "all_task_names = []\n",
    "all_datasets = []\n",
    "all_model_names = []\n",
    "all_metrics = []\n",
    "for f in all_files:\n",
    "    data = json.load(open(f,'r'))\n",
    "    for row in data:\n",
    "        #print(row.keys())\n",
    "        if 'datasets' in row.keys():\n",
    "            all_task_names.append(row['task'])\n",
    "            for dataset in row['datasets']:\n",
    "                all_datasets.append(dataset['dataset'])\n",
    "                if 'sota' in dataset.keys():\n",
    "                    for models in dataset['sota']['rows']:\n",
    "                        all_model_names.append(models['model_name'])\n",
    "                        all_metrics.extend(list(models['metrics'].keys()))\n",
    "        elif 'subtasks' in row.keys():\n",
    "            for subtask in row['subtasks']:\n",
    "                all_task_names.append(subtask['task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tasks = pd.DataFrame({\"Task_Names\":list(set(all_task_names))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame({\"Datasets\":list(set(all_datasets))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame({\"Models\":list(set(all_model_names))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame({\"Metrics\":list(set(all_metrics))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(\"Task_Names.csv\")\n",
    "df_data.to_csv(\"Datasets.csv\")\n",
    "df_models.to_csv(\"Models.csv\")\n",
    "df_metrics.to_csv(\"Metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tasks.to_csv(\"Task_Names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers With Code Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"evaluation-tables.json\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "all_task_names = []\n",
    "all_datasets = []\n",
    "all_model_names = []\n",
    "all_metrics = []\n",
    "\n",
    "for row in data:\n",
    "    #print(row.keys())\n",
    "    if 'Natural Language Processing' in row['categories']:\n",
    "        if 'datasets' in row.keys():\n",
    "            all_task_names.append(row['task'])\n",
    "            for dataset in row['datasets']:\n",
    "                all_datasets.append(dataset['dataset'])\n",
    "                if 'sota' in dataset.keys():\n",
    "                    for models in dataset['sota']['rows']:\n",
    "                        all_model_names.append(models['model_name'])\n",
    "                        all_metrics.extend(list(models['metrics'].keys()))\n",
    "        elif 'subtasks' in row.keys():\n",
    "            for subtask in row['subtasks']:\n",
    "                all_task_names.append(subtask['task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_task_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1636"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4567"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_model_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tasks = pd.DataFrame({\"Task_Names\":list(set(all_task_names))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame({\"Datasets\":list(set(all_datasets))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame({\"Models\":list(set(all_model_names))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame({\"Metrics\":list(set(all_metrics))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tasks.to_csv(\"Task_Names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(\"Datasets.csv\")\n",
    "df_models.to_csv(\"Models.csv\")\n",
    "df_metrics.to_csv(\"Metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_excel(\"Keyword Lists.xlsx\",sheet_name=\"Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_excel(\"Keyword Lists.xlsx\",sheet_name=\"Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_excel(\"Keyword Lists.xlsx\",sheet_name=\"Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame({\"Metrics\":list(set(list(metrics) + list(set(all_metrics))))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame({\"Models\":list(set(list(models) + list(set(all_model_names))))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame({\"Datasets\":list(set(list(ds) + list(set(all_datasets))))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(\"Datasets.csv\")\n",
    "df_models.to_csv(\"Models.csv\")\n",
    "df_metrics.to_csv(\"Metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n",
      "5 6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug  5 2022, 15:21:02) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
