Proceedings O
of O
the O
12th O
Workshop O
on O
Computational O
Approaches O
to O
Subjectivity, O
Sentiment O
& O
Social O
Media O
Analysis O
, O
pages O
315 O
- O
323 O
May O
26, O
2022 O
c O
2022 O
Association O
for O
Computational O
Linguistics O
Items O
from O
Psychometric O
Tests O
as O
Training O
Data O
for O
Personality B-TaskName
Profiling I-TaskName
Models I-TaskName
of O
Twitter O
Users O
Anne O
Kreuter1, O
Kai O
Sassenberg2,3,and O
Roman O
Klinger1 O
1Institut O
für O
Maschinelle O
Sprachverarbeitung, O
University O
of O
Stuttgart, O
Germany O
2Leibniz-Institut O
für O
Wissensmedien, O
Tübingen, O
Germany O
3University O
of O
Tübingen, O
Germany O
{anne.kreuter,roman.klinger}@ims.uni-stuttgart.de O
k.sassenberg@iwm-tuebingen.de O
Abstract O
Machine-learned O
models O
for O
author B-TaskName
profiling I-TaskName
in O
social O
media O
often O
rely O
on O
data O
acquired O
via O
self-reporting-based O
psychometric O
tests O
(ques- O
tionnaires) O
filled O
out O
by O
social O
media O
users. O
This O
is O
an O
expensive O
but O
accurate O
data O
collec- O
tion O
strategy. O
Another, O
less O
costly O
alternative, O
which O
leads O
to O
potentially O
more O
noisy O
and O
bi- O
ased O
data, O
is O
to O
rely O
on O
labels O
inferred O
from O
publicly O
available O
information O
in O
the O
profiles O
of O
the O
users, O
for O
instance O
self-reported O
diag- O
noses O
or O
test O
results. O
In O
this O
paper, O
we O
explore O
a O
third O
strategy, O
namely O
to O
directly O
use O
a O
corpus O
of O
items O
from O
validated B-DatasetName
psychometric I-DatasetName
tests I-DatasetName
as O
training O
data. O
Items O
from O
psychometric O
tests O
often O
consist O
of O
sentences O
from O
an O
I-perspective O
(e.g., O
“I O
make O
friends O
easily.”). O
Such O
corpora O
of O
test O
items O
constitute O
‘small O
data’, O
but O
their O
avail- O
ability O
for O
many O
concepts O
is O
a O
rich O
resource. O
We O
investigate O
this O
approach O
for O
personality B-TaskName
profil- I-TaskName
ing, I-TaskName
and O
evaluate O
BERT B-MethodName
classifiers I-MethodName
fine-tuned O
on O
such O
psychometric O
test O
items O
for O
the O
big O
five O
personality O
traits O
(openness, O
conscientiousness, O
extraversion, O
agreeableness, O
neuroticism) O
and O
analyze O
various O
augmentation O
strategies O
regard- O
ing O
their O
potential O
to O
address O
the O
challenges O
coming O
with O
such O
a O
small O
corpus. O
Our O
eval- O
uation O
on O
a O
publicly O
available O
Twitter B-DatasetName
corpus O
shows O
a O
comparable O
performance O
to O
in-domain O
training O
for O
4/5 O
personality O
traits O
with O
T5-based B-MethodName
data O
augmentation. O
1 O
Introduction O
The O
field O
of O
author B-TaskName
profiling I-TaskName
originally O
emerged O
from O
the O
study O
of O
stylometry O
(Lutoslawski, O
1898) O
and, O
with O
the O
rise O
of O
social O
media O
(Bilan O
and O
Zhekova, O
2016), O
now O
considers O
a O
variety O
of O
at- O
tributes, O
including O
demographic O
data O
such O
as O
age, O
sex, O
gender, O
nationality O
(Schwartz O
et O
al., O
2013), O
personality O
traits O
(Golbeck O
et O
al., O
2011), O
or O
psycho- O
logical O
states O
such O
as O
emotions, O
or O
medical O
condi- O
tions O
like O
mental O
disorders O
(De O
Choudhury O
et O
al., O
2013). O
Such O
automatic O
methods O
enable O
large-scale O
social O
media O
data O
analyses O
even O
for O
(combinationsof) O
variables O
for O
which O
results O
from O
surveys O
are O
not O
available. O
Therefore, O
personality O
profiling O
in O
social O
media O
helps O
to O
paint O
a O
more O
comprehensive, O
complete, O
and O
timely O
picture O
for O
parts O
of O
a O
society. O
State-of-the-art O
models O
reconstruct O
personality O
traits O
or O
mental O
health O
states O
from O
posts O
of O
social O
media O
users O
by O
relying O
on O
ground-truth O
data O
that O
links O
such O
posts O
to O
the O
correct O
annotation O
(Guntuku O
et O
al., O
2017). O
The O
ground-truth O
data O
is O
typically O
obtained O
by O
either O
(1) O
asking O
participants O
to O
com- O
plete O
a O
validated O
survey O
that O
measures O
the O
desired O
variable O
and O
asking O
the O
participants O
to O
share O
their O
social O
media O
profiles, O
(2), O
by O
relying O
on O
self-reports O
of O
users, O
e.g., O
disclosure O
of O
a O
condition O
in O
the O
user’s O
profile O
description, O
or O
(3), O
by O
having O
experts O
anno- O
tate O
profiles O
for O
particular O
properties. O
The O
quality O
of O
data O
obtained O
might O
therefore O
suffer O
from O
social- O
desirability O
bias, O
from O
being O
a O
non-representative O
subsample, O
from O
a O
lack O
of O
validated O
diagnoses, O
or O
from O
noise O
stemming O
from O
the O
challenge O
that O
anno- O
tators O
do O
not O
have O
access O
to O
the O
actual O
characteris- O
tics O
of O
users O
(Ernala O
et O
al., O
2019). O
We O
explore O
another O
route O
for O
which O
we O
hypoth- O
esize O
that O
it O
addresses O
these O
issues, O
but O
at O
the O
cost O
of O
only O
having O
access O
to O
very O
small O
data O
sets: O
We O
propose O
to O
leverage O
the O
existing O
set O
of O
high-quality, O
validated, O
and O
reliable O
psychometric O
instruments O
to O
measure O
psychological O
traits O
directly O
. O
Psychomet- B-DatasetName
ric I-DatasetName
tests I-DatasetName
often O
come O
in O
the O
form O
of O
questionnaires O
which O
contain O
items, O
allowing O
a O
person O
to O
report O
Variable O
Cor. O
Item O
Text O
Openness O
+ O
“Am O
interested O
in O
many O
things.” O
Openness O
− O
“Do O
not O
like O
art.” O
Extraversion O
+ O
“Warm O
up O
quickly O
to O
others.” O
Extraversion O
− O
“Am O
hard O
to O
get O
to O
know.” O
Table O
1: O
Example O
items O
from O
a O
psychometric O
test O
to O
assess O
personality O
traits O
(Lee O
and O
Ashton, O
2018). O
‘Cor.’ O
indicates O
if O
the O
item O
has O
been O
shown O
to O
correlate O
posi- O
tively O
or O
negatively O
to O
the O
respective O
concept.315about O
themselves. O
These O
items O
are O
sentences O
for- O
mulated O
as O
descriptions O
of O
the O
self O
(Table O
1 O
shows O
some O
examples). O
This O
structure O
motivates O
our O
hy- O
pothesis O
that O
such O
psychometric O
tests O
can O
be O
used O
directly O
to O
induce O
classifiers O
that O
profile O
individu- O
als O
in O
social O
media O
without O
the O
existence O
of O
desig- O
nated, O
manually O
annotated O
in-domain O
training O
data. O
If O
indeed O
possible, O
this O
would O
lead O
to O
a O
straight- O
forward O
route O
to O
develop O
a O
myriad O
of O
classifiers O
for O
all O
those O
concepts O
for O
which O
psychometric O
tests O
exist. O
To O
dampen O
the O
issue O
of O
these O
sets O
of O
items O
be- O
ing O
comparably O
small, O
we O
make O
use O
of O
pre-trained O
language O
models O
(Howard O
and O
Ruder, O
2018; O
Devlin O
et O
al., O
2019; O
Brown O
et O
al., O
2020) O
to O
transfer O
knowl- O
edge O
acquired O
through O
pretraining O
rich O
semantic O
representations. O
Some O
subtypes O
of O
such O
models O
can O
be O
considered O
few-shot O
learners O
(Brown O
et O
al., O
2020; O
Ruder O
et O
al., O
2019), O
however, O
the O
transfer O
might O
not O
be O
successful O
to O
data O
outside O
of O
the O
pre- O
training O
domain. O
Therefore, O
we O
evaluate O
if O
various O
data O
augmentation O
methods O
can O
further O
leverage O
the O
challenges O
coming O
with O
such O
small O
corpora. O
Thus, O
our O
contributions O
in O
this O
paper O
are O
that O
we O
(1) O
assemble O
a O
corpus O
from O
publicly O
available O
psy- O
chometric O
tests O
for O
the O
‘Big O
Five’ O
variables O
of O
open- O
ness, O
conscientiousness, O
extraversion, O
agreeable- O
ness, O
and O
neuroticism O
(Costa O
and O
McCrae, O
1992), O
which O
have O
been O
shown O
to O
be O
principled O
factors O
of O
personality O
(Cattell, O
1945). O
Based O
on O
these O
data, O
we O
(2) O
fine-tune O
BERT B-MethodName
(Devlin O
et O
al., O
2019) O
and O
evalu- O
ate O
it O
on O
an O
existing O
personality O
trait O
corpus O
(Rangel O
et O
al., O
2015). O
Furthermore, O
(3) O
we O
evaluate O
three O
data O
augmentation O
methods, O
namely O
paraphrasing O
with O
T5 B-MethodName
(Raffel O
et O
al., O
2020), O
and O
item O
generation O
with O
GPT-2 B-MethodName
(Radford O
et O
al., O
2019) O
and O
synonym O
replacements O
with O
Easy O
Data O
Augmentation O
(Wei O
and O
Zou, O
2019). O
Our O
results, O
(4), O
show O
that O
the O
models O
perform O
en O
par O
with O
in-domain O
training O
for O
4/5 O
personality O
trait O
variables. O
2 O
Related O
Work O
Psychometric O
Personality O
Tests. O
A O
psychometric O
test O
is O
a O
standardized O
instrument O
used O
to O
measure O
the O
cognitive, O
behavioral, O
or O
emotional O
character- O
istics O
of O
a O
person. O
One O
possible O
form O
are O
question- O
naires, O
which O
can O
be O
designed O
for O
self-reporting. O
For O
each O
item O
the O
information O
is O
available O
if O
it O
is O
correlated O
positively O
or O
negatively O
with O
the O
concept O
to O
be O
measured. O
Publicly O
available O
psychometrictests O
can O
be O
found O
in O
various O
online O
repositories.1 O
An O
established O
test O
for O
personality O
traits O
follow- O
ing O
the O
so-called O
‘big O
five’ O
variables O
is O
the O
Interna- O
tional O
Personality O
Item O
Pool O
Representation O
of O
the O
NEO O
PI-R O
with O
300 O
items2(IPIP-NEO-300, O
Gold- O
berg O
et O
al., O
1999). O
This O
test O
is O
a O
proxy O
of O
the O
Revised O
NEO O
Personality O
Inventory O
(NEO O
PI-R) O
by O
Costa O
and O
McCrae O
(1992), O
which O
is O
copyrighted O
and O
can O
only O
be O
ordered O
by O
professionals O
and O
used O
with O
per- O
mission. O
We O
use O
all O
items O
of O
the O
IPIP-NEO-300 O
as O
the O
source O
of O
our O
training O
corpus. O
Another O
test O
of O
personality O
traits O
would O
be O
the O
HEXACO O
Personality O
Inventory-Revised O
(Lee O
and O
Ashton, O
2008). O
It O
measures O
six O
factors O
of O
per- O
sonality O
(Ashton O
et O
al., O
2004) O
with O
200 O
items, O
namely O
Honesty-Humility, O
Emotionality, O
Extraver- O
sion, O
Agreeableness, O
Conscientiousness, O
and O
Open- O
ness O
to O
experience. O
Data. O
Psychometric O
tests O
found O
application O
in O
the O
analysis O
of O
social O
media O
user’s O
personality O
in O
the O
past. O
An O
influential O
study O
has O
been O
the O
work O
by O
Schwartz O
et O
al. O
(2014), O
who O
collected O
Facebook O
data O
with O
a O
dedicated O
application O
(Stillwell O
and O
Kosinski, O
2004) O
in O
which O
users O
completed O
the O
100- O
item O
IPIP-NEO-100 O
questionnaire O
(Goldberg O
et O
al., O
1999). O
The O
users O
further O
shared O
access O
to O
their O
status O
updates. O
This O
data O
is O
not O
available O
any O
longer. O
The O
data O
for O
the O
PAN-author-profiling O
shared O
task O
in O
2015 O
has O
been O
collected O
in O
a O
similar O
way O
(Rangel O
et O
al., O
2015).3It O
consists O
of O
Tweets O
of O
294 O
English O
Twitter O
profiles O
(besides O
Spanish, O
Italian O
and O
Dutch O
Twitter O
profiles), O
which O
are O
annotated O
with O
gender, O
age, O
and O
the O
‘Big O
Five’ O
personality O
traits. O
The O
personality O
traits O
were O
self-assessed O
by O
the O
Twitter O
users O
with O
the O
BFI-10 O
(Rammstedt O
and O
John, O
2007), O
which O
is O
an O
economic O
psychometric O
test O
that O
allows O
the O
personality O
to O
be O
recorded O
with O
only O
10 O
items. O
We O
use O
this O
corpus O
for O
evaluation. O
Combining O
Tests O
and O
Social O
Media O
Data. O
An O
interesting O
combination O
of O
psychometric O
tests O
with O
social O
media O
posts, O
which O
is O
likely O
the O
one O
most O
similar O
to O
our O
paper, O
is O
the O
work O
by O
Vu O
et O
al. O
(2020). O
The O
authors O
make O
use O
of O
social O
media O
data O
of O
users O
to O
automatically O
fill O
the O
IPIP-NEO O
(Goldberg O
et O
al., O
1999) O
psychometric O
test O
to O
predict O
the O
social O
media O
user’s O
‘Big O
Five’ O
personality O
traits. O
They O
do O
so O
by O
embedding O
sentences O
and O
items O
with O
BERT B-MethodName
into O
the O
same O
distributional O
space, O
followed O
by O
a O
k-nearest- O
1https://psychology-tools.com/, O
https://ipip.ori.org/, O
https://www.psychometrictest.org.uk/ O
2http://www.personal.psu.edu/~j5j/IPIP/ O
3https://zenodo.org/record/3745945,https://pan.webis.de/316Original O
Items O
Data O
Augmentation O
Augm. O
Items O
Fine-tune O
BERTModelIndividual B-MethodName
TweetsTwitter O
Profiles O
Aggregated O
Evaluation O
Evaluationgold O
labels O
Figure O
1: O
Workflow O
of O
our O
approach. O
neighbor O
classification. O
This O
approach O
constitutes O
the O
opposing O
approach O
that O
we O
chose O
in O
our O
paper O
– O
Vu O
et O
al. O
(2020) O
use O
social O
media O
data O
to O
fill O
a O
psychometric O
test. O
We O
use O
psychometric O
tests O
to O
classify O
social O
media O
data. O
We O
refer O
the O
reader O
to O
Stajner O
and O
Yenikent O
(2020) O
for O
a O
more O
comprehensive O
overview O
of O
re- O
lated O
work. O
3 O
Methods O
3.1 O
Workflow O
We O
depict O
the O
general O
workflow O
in O
Figure O
1. O
The O
original O
items O
from O
the O
questionnaire O
are O
first O
aug- O
mented. O
The O
resulting O
augmented O
items O
inherit O
the O
labels O
from O
the O
respective O
original O
items. O
We O
fine- O
tune O
BERT B-MethodName
with O
these O
items O
which O
leads O
to O
a O
model O
to O
make O
predictions O
for O
comparably O
short O
instances, O
like O
Tweets. O
From O
the O
labeled O
corpus O
of O
Twitter B-DatasetName
profiles, I-DatasetName
we O
obtain O
labels O
for O
each O
individual O
tweet O
with O
the O
BERT-based B-MethodName
model O
and O
then O
aggregate O
the O
individual O
labels O
to O
obtain O
a O
label O
for O
the O
whole O
pro- O
file. O
In O
the O
evaluation, O
this O
predicted O
profile O
label O
is O
compared O
to O
the O
annotated O
gold O
label. O
3.2 O
Corpora O
We O
use O
all O
items O
of O
the O
psychometric O
test O
IPIP- O
NEO-300 O
(Goldberg O
et O
al., O
1999) O
as O
training O
data O
and O
label O
each O
item O
following O
the O
evaluation O
guide- O
lines O
accompanying O
the O
IPIP-NEO-300 O
(see O
also O
Table O
1). O
These O
guidelines O
provide O
the O
informa- O
tion O
if O
a O
confirmative O
answer O
to O
the O
item O
indicates O
a O
positive O
correlation O
or O
negative O
correlation O
with O
the O
target O
variable, O
which O
leads O
to O
a O
binary O
label. O
For O
evaluation, O
we O
use O
the O
English O
subset O
of O
the O
PAN-author-profiling-2015 O
data O
(Rangel O
et O
al.,IPIP-NEO O
Profiles O
Tweets O
Class. O
+− O
+− O
+ O
− O
We O
extracted O
all O
profiles O
with O
positive O
or O
negative O
scores O
and O
excluded O
profiles O
with O
neutral O
scores. O
2015) O
with O
annotated O
Twitter O
profiles. O
Table O
2 O
sum- O
marizes O
the O
corpus O
statistics. O
Note O
that O
the O
distri- O
bution O
of O
the O
items O
from O
the O
test O
data O
is O
skewed O
towards O
positive O
instances O
– O
this O
might O
be O
a O
direct O
consequence O
of O
people O
with O
particular O
personality O
traits O
being O
more O
likely O
to O
share O
particular O
informa- O
tion O
on O
social O
media. O
3.3 O
Classification O
Model O
As O
our O
source O
domain, O
we O
consider O
a O
set O
of O
items O
QC={(qi, O
yi)}n O
i=1from O
a O
reliable O
psychometric O
test. O
Each O
of O
these O
items O
corresponds O
to O
one O
psy- O
chological O
concept O
Cand O
consists O
of O
the O
item O
text O
qiand O
the O
label O
yi∈ O
{pos,neg}which O
stems O
from O
the O
evaluation O
guidelines O
for O
this O
test. O
The O
task O
is O
to O
find O
a O
parameterized O
function O
fC,λ(u)which O
takes O
as O
input O
all O
posts O
of O
a O
user O
uand O
predicts O
a O
label O
for O
each O
concept O
C. O
The O
important O
aspect O
in O
our O
setup O
is O
that O
the O
parameters O
λare O
only O
optimized O
on O
the O
psychometric O
data O
QC. O
This O
is O
a O
mismatch O
– O
we O
train O
a O
classifier O
to O
label O
short O
texts O
but O
need O
as O
output O
a O
prediction O
for O
a O
set O
of O
tweets O
which O
represents O
the O
user. O
Hence, O
to O
obtain O
a O
label O
for O
each O
user, O
we O
aggregate O
the O
labels O
for O
all O
their O
posts O
by O
accepting O
the O
majority O
class, O
for O
each O
concept O
separately. O
To O
obtain O
the O
text O
classifier, O
we O
fine-tune O
BERT B-MethodName
(Devlin O
et O
al., O
2019) O
to O
approximate O
each O
function O
fC,λ, O
based O
on O
bert-base-uncased O
. O
The O
sequence O
classification O
head O
is O
randomly O
initialized O
on O
top O
of O
the O
encoder.4For O
each O
concept O
C, O
we O
fine-tune O
a O
separate O
BERT B-MethodName
model O
(no O
multi-task O
learning). O
3.4 O
Data O
Augmentation O
With O
60 O
items O
per O
personality O
trait, O
our O
training O
cor- O
pora O
are O
small. O
To O
address O
this O
issue, O
we O
perform O
data O
augmentation O
with O
three O
different O
methods. O
4https://huggingface.co/transformers/model_doc/bert. O
html#bertforsequenceclassification317 O
0 O
20 O
40 O
60 O
80 O
100PlainEDAT5GPT2InDomPlainEDAT5GPT2InDomPlainEDAT5GPT2InDomPlainEDAT5GPT2InDomPlainEDAT5GPT2InDom O
Open O
Conscient. O
Extrav. O
Agreeabl. O
Neurot.Weighted O
Average O
F O
1Figure O
2: O
F O
1scores O
for O
all O
models O
and O
classes. O
Horizontal O
lines O
depict O
the O
baseline. O
For O
every O
ninstances O
(qi, O
yi)∈QC, O
we O
perform O
each O
data O
augmentation O
mtimes O
(obtaining O
n·m O
instances). O
Thus, O
we O
generate O
maugmented O
items O
qa O
ifor O
each O
qi. O
Each O
newly O
generated O
instance O
in- O
herits O
the O
label O
yiof O
its O
original O
instance O
qi. O
We O
show O
examples O
for O
automatically O
generated O
items O
in O
the O
Appendix O
A.3. O
Easy O
Data O
Augmentation. O
Easy O
Data O
Augmen- O
tation O
(EDA, O
Wei O
and O
Zou, O
2019) O
consists O
of O
four O
operations O
on O
the O
sentence O
level: O
synonym O
replace- O
ment, O
random O
insertion, O
random O
deletion, O
and O
ran- O
dom O
swap. O
We O
use O
the O
default O
parameter O
of O
10% O
of O
words O
in O
the O
sentence O
being O
changed O
(30% O
for O
ran- O
dom O
deletion) O
to O
perform O
each O
operation O
of O
EDA O
on O
each O
sentence O
(item) O
5 O
times, O
hence O
generate O
20 O
instances O
out O
of O
each O
original O
instance. O
This O
leads O
to O
1,160 O
items O
for O
openness, O
1,130 O
for O
con- O
scientiousness, O
1,080 O
for O
extraversion, O
1,160 O
for O
agreeableness, O
and O
1080 O
for O
neuroticism. O
T5 O
item O
paraphrasing. O
We O
use O
T5 O
(Raffel O
et O
al., O
2020) O
to O
paraphrase O
each O
item, O
based O
on O
the O
T5ForConditionalGeneration B-MethodName
model O
provided O
by O
HuggingFace5. O
We O
do O
not O
perform O
fine-tuning O
to O
our O
domain, O
but O
rely O
on O
the O
original O
pre-trained O
pa- O
rameters. O
For O
each O
item, O
we O
generate O
up O
to O
50 O
para- O
phrases O
which O
leads O
to O
2,285 O
items O
for O
openness, O
2,383 O
for O
conscientiousness, O
2,149 O
for O
extraversion, O
2,126 O
for O
agreeableness, O
2,130 O
for O
neuroticism. O
GPT-2 B-MethodName
item O
generation. O
We O
fine-tune O
GPT-2 O
(Rad- O
ford O
et O
al., O
2019) O
for O
each O
personality O
trait O
sepa- O
rately O
in O
150 O
epochs, O
based O
on O
gpt-2-simple6. O
We O
generate O
3000 O
items O
for O
each O
class O
label O
with O
a O
sen- B-HyperparameterName
tence I-HyperparameterName
length I-HyperparameterName
of O
100 B-HyperparameterValue
tokens I-HyperparameterValue
and O
a O
temperature B-HyperparameterName
of O
1.5. B-HyperparameterValue
This O
leads O
to O
6,279 O
items O
for O
openness, O
6,177 O
for O
conscientiousness, O
6,204 O
for O
extraversion, O
6,271 O
for O
agreeableness, O
6,242 O
for O
neuroticism. O
5https://huggingface.co/transformers/model_doc/t5. O
html#t5forconditionalgeneration O
6https://github.com/minimaxir/gpt-2-simple4 O
Experiments O
4.1 O
Experimental O
Settings O
We O
split O
the O
psychometric O
test O
data O
to O
80 O
% O
for O
training O
and O
use O
20 O
% O
for O
hyperparameter O
optimiza- O
tion, O
while O
we O
ensure O
that O
augmented O
items O
stay O
in O
one O
set O
with O
their O
original O
item.7To O
avoid O
over- O
fitting, O
we O
apply O
early O
stopping O
via O
observing O
the O
loss O
on O
the O
validation O
data. O
The O
maximum O
number O
of O
epochs B-HyperparameterName
is O
set O
to O
200. B-HyperparameterValue
For O
a O
comparison O
to O
an O
“upper-bound” O
of O
in- O
domain O
training O
on O
Twitter, B-DatasetName
we O
split O
the O
corpora O
of O
social O
media O
profiles O
such O
that O
50% O
of O
the O
Twitter O
profiles O
are O
in O
the O
test O
set. O
The O
remaining O
50 O
% O
are O
used O
for O
training O
and O
further O
split O
into O
90 O
% O
for O
training O
and O
10 O
% O
for O
hyperparameter O
optimiza- O
tion O
of O
the O
in-domain O
model. O
The O
settings O
for O
fine- O
tuning O
the O
in-domain O
models O
are O
identical O
to O
the O
settings O
of O
the O
psychometric O
models. O
4.2 O
Results O
We O
show O
our O
main O
results O
as O
weighted O
F B-MetricName
1values I-MetricName
in O
Figure O
2 O
(complete O
results O
in O
Table O
4 O
in O
the O
Ap- O
pendix). O
We O
compare O
the O
“plain” O
models O
without O
data O
augmentation O
to O
the O
augmented O
methods O
(as O
bar O
plots) O
and O
a O
random O
baseline O
(as O
horizontal O
line). O
We O
further O
show O
the O
performance O
of O
the O
in- O
domain O
model. O
All O
“plain”, O
non-data O
augmented O
models O
get O
out- O
performed O
by O
the O
random O
baseline, O
except O
for O
the O
personality O
trait O
neuroticism O
(F B-MetricName
1=.63 I-MetricName
versus O
F O
1 O
=.46 O
random O
baseline). O
The O
plain O
psychometric O
models O
are O
inferior O
to O
the O
in-domain O
models O
for O
all O
concepts, O
but O
to O
various O
extends: O
Neuroticism O
is O
the O
only O
trait O
where O
the O
plain O
model O
shows O
a O
performance O
en O
par O
with O
the O
in-domain O
model. O
7Seed O
set O
to O
42 O
via O
PyTorchLighting O
seed_everything O
, O
learning B-HyperparameterName
rate I-HyperparameterName
10−5, B-HyperparameterValue
batch B-HyperparameterName
size I-HyperparameterName
of O
16, B-HyperparameterValue
optimization B-HyperparameterName
with O
Adam B-HyperparameterValue
(Kingma O
and O
Ba, O
2014).318Regarding O
the O
augmentation O
methods, O
T5 B-MethodName
shows O
considerable O
improvements O
for O
conscientiousness, O
extraversion, O
and O
agreeableness O
(F B-MetricName
1=.89, I-MetricName
F B-MetricName
1=.82, I-MetricName
F1=.65, B-MetricName
respectively, O
vs. O
.73, O
.87, O
.84 O
for O
in-domain O
models). O
This O
is O
also O
the O
best-performing O
aug- O
mentation O
method B-HyperparameterName
for O
conscientiousness O
and O
ex- O
traversion, O
however, O
EDA O
shows O
a O
further O
improve- O
ment O
for O
agreeableness O
(.84). O
T5 B-MethodName
does O
not O
harm O
the O
performance O
for O
neuroticism O
in O
comparison O
to O
the O
plain O
model. O
Therefore, O
we O
conclude O
that O
T5 B-MethodName
augmentation O
is O
a O
promising O
choice O
for O
4/5 O
traits, O
while O
the O
other O
augmentation O
methods O
appear O
less O
stable O
in O
their O
contribution. O
In O
summary, O
we O
obtain O
a O
substantial O
model O
per- O
formance O
without O
the O
use O
of O
in-domain O
training O
data O
for O
Conscientiousness, O
Extraversion, O
Agree- O
ableness, O
and O
Neuroticism. O
The O
transfer O
to O
or O
the O
difficulty O
of O
these O
concepts O
appears O
not O
to O
be O
the O
same, O
the O
performance O
for O
Conscientiousness O
is O
substantially O
higher O
than O
for O
Neuroticism. O
These O
results O
can O
only O
be O
partially O
compared O
to O
previous O
work O
due O
to O
the O
differences O
in O
the O
evaluation O
setup. O
However, O
it O
should O
be O
noted O
that O
the O
concepts O
that O
appear O
to O
be O
more O
challenging O
in O
our O
setup O
show O
also O
lower O
evaluation O
measures O
in O
related O
work O
(see O
for O
instance O
Table O
3 O
in O
Rangel O
et O
al., O
2015, O
note O
that O
their O
evaluation O
measure O
is O
an O
RSME O
, O
lower O
is O
there- O
fore O
better). O
4.3 O
Model O
Introspection O
To O
provide O
some O
insights O
on O
the O
decision O
process O
by O
the O
classification O
models, O
we O
provide O
one O
exam- O
ple O
for O
each O
personality O
trait O
from O
the O
Tweet O
corpus O
with O
LIME B-MethodName
explanations O
(Ribeiro O
et O
al., O
2016) O
in O
Table O
3. O
In O
the O
example O
for O
openness, O
the O
classifier O
relies O
on O
the O
word O
“love” O
as O
a O
positive O
indicator. O
This O
word O
can O
indeed O
be O
found O
in O
items O
from O
the O
test, O
namely O
in O
“Love O
to O
daydream”, O
“Love O
flow- O
ers”, O
and O
“Love O
to O
read O
challenging O
material”. O
It O
is O
also O
a O
term O
that O
appears O
frequently O
in O
augmented O
data, O
such O
as O
in O
“Love O
problem O
solving” O
or O
in O
“Love O
flowers. O
Is O
it O
not O
hard O
to O
tell O
if O
you O
like O
something O
that’s O
especially O
beautiful?”. O
A O
positive O
indicator O
for O
conscientiousness O
is O
the O
word O
“August” O
and O
“year”. O
This O
is O
interesting, O
given O
that O
these O
words O
appear O
not O
to O
be O
directly O
related O
to O
conscientious- O
ness, O
and O
they O
do O
not O
appear O
in O
the O
original O
items O
of O
the O
test. O
However, O
the O
augmented O
data O
contains O
items O
that O
refer O
to O
“year”, O
such O
as O
in O
“I O
truly O
love O
Excel O
and O
have O
used O
it O
for O
years.”.T O
Tweet O
O O
@username O
What O
my O
love O
life O
will O
hold O
instore O
for O
me O
in O
the O
future. O
I’d O
never O
askwhen O
I’m O
gonna O
die...???????? O
C O
“@username: O
@username O
I O
like O
your O
profile O
photo. O
Very O
nice!!! O
You O
look O
very O
pretty. O
:)" O
THANK O
YOU! O
Took O
this O
photo O
in O
August O
this O
year. O
E O
@username O
Slade!!! O
Cool O
memories O
of O
my O
grammar O
school O
days!! O
A O
@username O
I O
rocked O
so O
much O
totheir O
music! O
N O
“@username O
: O
Karma O
hasnomenu O
. O
You O
get O
served O
what O
you O
deserve." O
Table O
3: O
Examples O
of O
LIME O
explanations. O
Green O
in- O
dicates O
a O
positive O
contribution O
of O
the O
word, O
and O
reda O
negative O
contribution. O
The O
augmentation O
approach O
used O
in O
each O
example O
is O
the O
best-performing O
method O
for O
the O
respective O
concept. O
All O
examples O
are O
true O
positives. O
5 O
Conclusion O
& O
Future O
Work O
We O
outlined O
a O
novel O
methodology O
for O
automatic O
author O
profiling O
in O
social O
media O
users O
without O
a O
costly O
collection O
of O
annotated O
social O
media O
data. O
Instead, O
we O
directly O
train O
on O
items O
from O
validated O
psychometric O
tests. O
This O
data O
selection O
procedure O
has O
some O
advantages: O
items O
of O
psychometric O
tests O
are O
carefully O
validated O
textual O
instances. O
Such O
cor- O
pora O
of O
such O
items O
constitute O
“small O
data”, O
but O
are O
available O
for O
a O
large O
number O
of O
concepts. O
Therefore, O
developing O
a O
method O
to O
induce O
classifiers O
directly O
from O
psychometric O
tests O
is O
also O
a O
promising O
avenue O
for O
future O
research. O
For O
the O
tasks O
of O
developing O
models O
measuring O
the O
big O
five O
personality O
traits, O
we O
tested O
on O
Twitter O
data O
that O
has O
been O
collected O
by O
asking O
users O
to O
fill O
out O
a O
(different) O
test. O
The O
transfer O
appears O
to O
be O
achievable, O
we O
obtain O
results O
for O
four O
out O
of O
five O
personality O
traits O
which O
are O
en O
par O
with O
in-domain O
models, O
using O
T5 B-MethodName
data O
augmentation O
(except O
Open- O
ness, O
which O
has O
very O
few O
test O
instances). O
An O
important O
remaining O
research O
question O
is O
how O
models O
can O
be O
obtained O
that O
show O
consistently O
good O
results O
across O
concepts. O
In O
a O
real-world O
setup, O
test O
data O
from O
the O
target O
domain O
would O
not O
be O
available O
to O
make O
model O
selection O
decisions. O
One O
way O
to O
go O
might O
be O
to O
combine O
various O
augmentation O
meth- O
ods. O
Another O
approach O
would O
be O
to O
use O
items O
as O
prompts O
in O
a O
zero-shot O
learning O
setup. O
Acknowledgements O
This O
work O
was O
supported O
by O
Deutsche O
Forschungs- O
gemeinschaft O
(project O
CEAT, O
KL O
2869/1-2).319Ethical O
considerations O
The O
fact O
that O
the O
current O
research O
deals O
with O
the O
sen- O
sitive O
topic O
of O
personality O
warrants O
for O
some O
ethical O
considerations. O
First, O
the O
study O
has O
been O
conducted O
with O
anonymized O
publicly O
available O
data. O
We O
did O
not O
collect O
data O
ourselves O
and O
importantly O
the O
data O
did O
not O
allow O
to O
identify O
subjects. O
Therefore, O
it O
is O
neither O
required O
nor O
possible O
to O
request O
IRB O
ap- O
proval O
for O
the O
current O
research, O
given O
that O
IRB O
is O
concerned O
with O
the O
protection O
of O
human O
subjects. O
We O
had O
no O
reasons O
to O
doubt O
that O
the O
parties, O
who O
originally O
collected O
the O
data O
got O
IRB O
approval O
and O
informed O
consent O
form O
the O
participants O
who O
pro- O
vided O
their O
data. O
However, O
we O
acknowledge O
that O
automatic O
sys- O
tems O
for O
personality O
trait O
analysis O
can O
be O
misused. O
Further, O
the O
application O
of O
our O
proposed O
model O
cre- O
ation O
strategy O
can O
also O
be O
used O
for O
other O
more O
sensi- O
ble O
concepts, O
for O
instance O
regarding O
mental O
health. O
We O
propose O
that O
such O
systems O
are O
only O
made O
avail- O
able O
in O
such O
a O
manner O
that O
no O
personalized O
results O
can O
be O
retrieved. O
The O
random O
baseline O
generates O
predictions O
by O
respecting O
the O
training O
sets’ O
class O
distribution. O
The O
weighted O
average O
values O
for O
P, O
R, O
F O
1correspond O
to O
the O
average O
across O
all O
labels O
considering O
the O
proportion O
for O
each O
label O
in O
the O
data O
set. O
The O
bold O
typo O
highlights O
our O
best O
performing O
model O
w.r.t. O
the O
highest O
w-avg. O
−: O
scored O
negative, O
+: O
scored O
positive. O
A.2 O
Implementation O
Details O
We O
performed O
the O
experiments O
on O
4 O
NVIDIA O
GeForce O
GTX O
1080 O
Ti O
GPUs O
with O
Intel(R) O
Xeon(R) O
CPU O
E5-2650 O
v4 O
@ O
2.20GH. O
The O
number O
of O
parameters O
is O
defined O
by O
the O
base O
model O
that O
we O
used, O
namely O
BERT B-MethodName
base, O
with O
110 O
M O
parameters. O
We O
show O
the O
run-time O
of O
models O
(training O
+ O
testing) O
in O
Table O
5. O
The O
numbers O
do O
not O
include O
startup/loading O
times. O
Note O
that O
the O
test O
data O
is O
(sometimes O
dramatically) O
larger O
than O
the O
training O
data. O
Examples O
for O
Augmentation O
Methods O
•As O
an O
example O
for O
the O
EDA O
augmentation O
method, O
synonym O
replacement O
lead O
to O
“Love O
thinking O
about O
things” O
based O
on O
the O
IPIP-NEO-300 O
item O
“Enjoy O
thinking O
about O
things” O
for O
the O
trait O
of O
openness. O
•As O
an O
example O
for O
the O
T5 B-MethodName
augmentation O
method, O
T5-paraphrasing O
lead O
to O
“Have O
fun O
and O
be O
wildly O
inspired O
by O
wild O
fantasy O
dreams” O
based O
on O
the O
IPIP-NEO-300 O
item O
“Enjoy O
wild O
flights O
of O
fantasy” O
for O
the O
trait O
of O
openness. O
• O
An O
example O
for O
a O
GPT-2 B-MethodName
generated O
item O
measuring O
agreeableness O
is O
“I O
am O
an O
average O
person”.323 O