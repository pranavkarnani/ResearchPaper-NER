Proceedings O
of O
the O
CoNLL–SIGMORPHON O
2018 O
Shared O
Task: O
Universal O
Morphological O
Reinﬂection O
, O
pages O
116–120, O
Brussels, O
Belgium, O
October O
31, O
2018. O
c O
2018 O
Association O
for O
Computational O
Linguistics O
Combining O
Neural O
and O
Non-Neural O
Methods O
for O
Low-Resource B-TaskName
Morphological I-TaskName
Reinﬂection I-TaskName
Saeed O
Najaﬁ, O
Bradley O
Hauer, O
Rashed O
Rubby O
Riyadh, O
Leyuan O
Yu, O
Grzegorz O
Kondrak O
Department O
of O
Computing O
Science O
University O
of O
Alberta, O
Edmonton, O
Canada O
{snajafi,bmhauer,riyadh,leyuan,gkondrak O
}@ualberta.ca O
Abstract O
We O
describe O
our O
systems O
and O
results O
in O
the O
type-level O
low-resource O
setting O
of O
the O
CoNLL– O
SIGMORPHON O
2018 O
Shared O
Task O
on O
Univer- O
sal O
Morphological O
Reinﬂection. O
We O
test O
non- O
neural O
transduction O
models, O
as O
well O
as O
more O
recent O
neural O
methods. O
We O
also O
investigate O
the O
effect O
of O
leveraging O
unannotated O
corpora O
to O
improve O
the O
performance O
of O
selected O
methods. O
Our O
best O
system O
obtains O
the O
highest O
accuracy B-MetricName
on O
34 B-MetricValue
out O
of O
103 O
languages. O
1 O
Introduction O
In O
this O
system O
paper, O
we O
discuss O
our O
submissions O
to O
the O
CoNLL–SIGMORPHON O
2018 O
Shared O
Task O
on O
Universal B-TaskName
Morphological I-TaskName
Reinﬂection I-TaskName
(Cot- O
terell O
et O
al., O
2018). O
We O
focus O
on O
the O
sub-task O
of O
type-level B-TaskName
inﬂection I-TaskName
under I-TaskName
the I-TaskName
low-resource I-TaskName
sce- I-TaskName
nario, I-TaskName
in O
which O
the O
training O
data O
is O
limited O
to O
100 O
labelled O
examples. O
Because O
of O
the O
sheer O
number O
of O
tested O
languages, O
we O
attempted O
no O
language- O
speciﬁc O
modiﬁcations. O
The O
results O
demonstrate O
that O
our O
non-neural O
transduction O
models O
perform O
better O
on O
average O
than O
our O
neural O
models. O
How- O
ever, O
combining O
neural O
and O
non-neural O
models O
yields O
the O
highest O
accuracy. B-MetricName
In O
addition O
to O
standard O
submissions, O
we O
test O
novel O
methods O
of O
leveraging O
additional O
monolin- O
gual O
corpora, O
from O
which O
we O
derive O
target O
lan- O
guage O
models O
and/or O
word O
lists. O
We O
show O
that O
substantial O
gains O
in O
accuracy B-MetricName
can O
be O
obtained O
in O
the O
way. O
Again, O
a O
combination O
of O
neural O
and O
non- O
neural O
systems O
produces O
the O
best O
non-standard O
re- O
sults. O
The O
paper O
has O
the O
following O
structure. O
In O
Sec- O
tion O
2, O
we O
describe O
four O
standard O
systems, O
as O
well O
as O
our O
weighted-voting O
method O
of O
combining O
them. O
Our O
two O
non-standard O
systems O
and O
their O
lin- O
ear O
combination O
are O
introduced O
in O
Section O
3. O
Sec- O
tion O
4 O
discusses O
the O
results.2 O
Standard O
Systems O
In O
this O
section, O
we O
brieﬂy O
describe O
the O
four O
in- O
dividual O
standard O
systems O
that O
we O
experimented O
with, O
followed O
by O
our O
voting O
method O
for O
combin- O
ing O
them. O
2.1 O
B B-MethodName
ASELINE I-MethodName
(UA-01) O
The O
shared O
task O
organizers O
have O
provided O
a O
base- O
line O
system O
for O
the O
type-level O
sub-task.1For O
each O
training O
instance, O
the O
baseline O
system O
aligns O
the O
in- O
put O
and O
output O
forms, O
and O
uses O
leading O
and O
trail- O
ing O
null O
alignments O
to O
identify O
preﬁx O
and O
sufﬁx O
boundaries. O
Thus, O
the O
input O
and O
output O
are O
each O
divided O
into O
a O
preﬁx, O
stem, O
and O
sufﬁx, O
with O
the O
preﬁx O
and O
sufﬁx O
possibly O
being O
empty. O
The O
pairs O
of O
aligned O
characters O
from O
the O
sufﬁx, O
and O
option- O
ally O
a O
trailing O
substring O
of O
the O
stem, O
are O
recorded O
as O
sufﬁxing O
rules O
for O
the O
morphological O
tag O
of O
the O
instance O
in O
question. O
Preﬁxing O
rules O
are O
identiﬁed O
in O
an O
analogous O
way. O
In O
this O
way, O
a O
series O
of O
inﬂec- O
tion O
rules O
are O
generated O
from O
aligned O
training O
pairs O
for O
each O
morphological O
tag O
attested O
in O
the O
training O
data. O
To O
perform O
reinﬂection O
on O
an O
unseen O
instance, O
the O
longest O
applicable O
sufﬁxing O
rule O
for O
the O
given O
tag O
is O
selected O
and O
applied, O
as O
is O
the O
most O
frequent O
preﬁxing O
rule. O
Since O
some O
languages O
tend O
to O
pre- O
fer O
preﬁxing O
over O
sufﬁxing, O
a O
heuristic O
is O
used O
to O
detect O
which O
of O
the O
two O
types O
of O
afﬁxation O
is O
pre- O
dominant. O
If O
a O
preference O
for O
preﬁxing O
is O
detected, O
all O
input O
and O
output O
strings O
for O
that O
language O
are O
reversed. O
During O
development, O
we O
found O
that O
the O
output O
ﬁles O
produced O
by O
the O
B B-MethodName
ASELINE I-MethodName
system O
for O
these O
languages O
had O
the O
lemmas O
reversed; O
we O
rec- O
tiﬁed O
this O
issue O
for O
our O
experiments O
and O
submis- O
sions. O
1https://github.com/sigmorphon/conll2018/tree/master/ O
task1/baseline1162.2 O
HAEM B-MethodName
(UA-02) O
The O
hard B-MethodName
attention I-MethodName
model I-MethodName
over I-MethodName
edit I-MethodName
actions I-MethodName
(HAEM) I-MethodName
of I-MethodName
Makarov I-MethodName
et O
al. O
(2017) O
performed O
very O
well O
in O
the O
low-resource O
setting O
of O
the O
2017 O
edi- O
tion O
of O
the O
shared O
task. O
We O
use O
the O
implementation O
made O
available O
by O
the O
authors.2The O
method O
learns O
a O
neural O
state-transition O
model O
with O
hard O
mono- O
tonic O
attention. O
It O
produces O
sequences O
of O
insertion O
and O
deletion O
operations O
on O
the O
lemma O
that O
trans- O
duce O
it O
into O
the O
appropriate O
inﬂected O
form. O
The O
system O
that O
achieved O
the O
top O
results O
in O
the O
2017 O
shared O
task O
was O
an O
ensemble O
of O
up O
to O
15 O
different O
models, O
each O
trained O
with O
multiple O
seeds. O
Because O
of O
time O
constraints, O
and O
the O
difﬁculties O
with O
using O
the O
implementation, O
we O
derive O
only O
a O
single O
transi- O
tion O
inﬂector O
model O
for O
each O
language, O
eschewing O
the O
complex O
ensemble O
procedures O
described O
in O
the O
original O
paper. O
The O
process O
of O
compiling O
and O
running O
the O
pro- O
vided O
code O
was O
non-trivial. O
In O
particular, O
libraries O
required O
by O
the O
provided O
code O
had O
been O
supplanted O
by O
newer O
versions, O
which O
lacked O
backwards O
com- O
patibility. O
Since O
the O
versions O
used O
in O
2017 O
are O
no O
longer O
readily O
available, O
we O
had O
to O
adapt O
the O
code O
to O
the O
new O
versions. O
Further O
modiﬁcations O
were O
necessary O
to O
account O
for O
the O
different O
format O
of O
the O
test O
data O
this O
year. O
Even O
with O
these O
modiﬁ- O
cations, O
the O
code O
failed O
to O
run O
properly O
on O
several O
languages, O
resulting O
in O
0% B-MetricValue
accuracy. B-MetricName
2.3 O
D O
IREC O
TL+ O
(UA-03) O
We O
perform O
string O
transduction O
with O
a O
modiﬁed O
version3of O
D O
IREC O
TL+, O
a O
tool O
originally O
designed O
for O
grapheme-to-phoneme O
conversion O
(Jiampoja- O
marn O
et O
al., O
2008). O
D O
IREC O
TL+ O
is O
a O
feature- O
rich, O
discriminative O
character O
string O
transducer O
that O
searches O
for O
a O
model-optimal O
sequence O
of O
character O
transformation O
rules O
for O
its O
input. O
The O
core O
of O
the O
engine O
is O
a O
dynamic O
programming O
al- O
gorithm O
capable O
of O
transducing O
many O
consecutive O
characters O
in O
a O
single O
operation. O
Using O
a O
structured O
version O
of O
the O
MIRA O
algorithm O
(McDonald O
et O
al., O
2005), O
training O
attempts O
to O
assign O
weights O
to O
each O
feature O
so O
that O
its O
linear O
model O
separates O
the O
gold- O
standard O
derivation O
from O
all O
others O
in O
its O
search O
space. O
We O
perform O
source-target O
pair O
alignment O
with O
a O
modiﬁed O
version4of O
the O
M2M O
aligner O
(Ji- O
ampojamarn O
et O
al., O
2007), O
which O
applies O
the O
EM O
to O
2https://gitlab.cl.uzh.ch/makarov/sigmorphon2017 O
3https://github.com/GarrettNicolai/DTL O
4https://github.com/GarrettNicolai/m2mM2M-aligner O
maximize O
the O
conditional O
likelihood O
of O
its O
aligned O
source O
and O
target O
pairs. O
We O
apply O
the O
tag O
splitting O
and O
particle O
handling O
techniques O
described O
in O
our O
2017 O
system O
paper O
Nicolai O
et O
al. O
(2017). O
In O
particular, O
we O
split O
the O
tags O
into O
subtags, O
and O
append O
them O
at O
both O
the O
be- O
ginning O
and O
end O
of O
the O
lemma. O
We O
decided O
not O
apply O
any O
subtag O
reordering O
techniques O
this O
year, O
due O
to O
the O
large O
number O
of O
languages. O
We O
tune O
hyper-parameters O
for O
each O
language O
us- O
ing O
grid O
search. O
Table O
1 O
speciﬁes O
the O
tuning O
ranges O
for O
both O
the O
aligner O
and O
the O
transducer. O
The O
list O
of O
the O
actual O
hyper-parameter O
settings O
for O
each O
lan- O
guage O
is O
available O
on O
request. O
2.4 O
AC-RNN B-MethodName
(UA-04) O
AC-RNN B-MethodName
is O
our O
novel O
implementation O
of O
the O
encoder-decoder O
RNN O
model, O
which O
is O
special- O
ized O
to O
the O
sequence-labelling O
task, O
and O
trains O
with O
an O
Actor-Critic O
reinforcement-learning O
objective O
(Najaﬁ O
et O
al., O
2018a). O
The O
implementation O
is O
fur- O
ther O
modiﬁed O
to O
incorporate O
soft-general O
attention O
mechanism, O
and O
adapted O
to O
the O
task O
of O
morpholog- O
ical O
reinﬂection.5In O
an O
initial O
experiment, O
we O
val- O
idated O
AC-RNN B-MethodName
using O
the O
high-resource O
French O
dataset O
from O
the O
2017 O
shared O
task, O
obtaining O
the O
test O
accuracy B-MetricName
of O
89.7%, B-MetricValue
compared O
to O
89.5% B-MetricValue
of O
the O
best-performing O
2017 O
ensemble O
system. O
2.5 O
Standard O
Combination O
(UA-05) O
In O
our O
development O
experiments, O
we O
observed O
that O
no O
system O
described O
in O
this O
section O
strictly O
domi- O
nates O
the O
others O
in O
terms O
of O
accuracy O
on O
every O
lan- O
guage; O
rather, O
different O
systems O
perform O
well O
on O
different O
languages. O
Furthermore, O
we O
often O
found O
instances O
where O
incorrect O
predictions O
were O
made O
by O
the O
top-performing O
system O
for O
the O
language O
in O
question, O
but O
the O
correct O
output O
was O
produced O
by O
other O
systems. O
These O
observations O
motivated O
our O
attempt O
to O
combine O
the O
strengths O
of O
the O
four O
sys- O
tems. O
5https://gitlab.com/SaeedNajaﬁ/ac-morph117Our O
standard O
combination O
approach O
is O
based O
on O
weighted O
voting O
. O
The O
top O
prediction O
from O
each O
of O
the O
four O
individual O
systems6is O
assigned O
a O
score O
equal O
to O
the O
system’s O
accuracy B-MetricName
on O
the O
development O
set O
for O
that O
language. O
The O
prediction O
with O
the O
high- O
est O
total O
score O
is O
returned. O
This O
system O
favors O
predictions O
from O
the O
top- O
performing O
system O
on O
a O
given O
language, O
while O
al- O
lowing O
errors O
to O
be O
corrected O
when O
other O
systems O
agree O
on O
a O
different O
prediction. O
If O
one O
system O
achieves O
an O
accuracy B-MetricName
greater O
than O
the O
sum O
of O
the O
accuracies B-MetricName
of O
all O
other O
systems, O
it O
dominates O
the O
voting, O
and O
the O
output O
of O
the O
combination O
is O
iden- O
tical O
to O
the O
output O
of O
that O
system. O
This O
scenario O
occurred O
for O
only O
seven O
languages. O
3 O
Non-standard O
Systems O
Large O
monolingual O
raw O
text O
corpora, O
which O
are O
freely O
available O
for O
a O
wide O
variety O
of O
languages, O
offer O
the O
possibility O
of O
improving O
the O
accuracy B-MetricName
of O
transduction O
models O
trained O
on O
small O
amounts O
of O
source-target O
pairs. O
Many O
of O
the O
target O
forms O
are O
observed O
in O
raw O
text O
corpora. O
In O
addition, O
character-level O
language O
models O
derived O
from O
monolingual O
corpora O
can O
reduce O
the O
number O
of O
output O
forms O
that O
violate O
the O
phonotactic O
con- O
straints O
of O
a O
language. O
Target O
language O
modelling O
is O
particularly O
important O
in O
low-data O
scenarios, O
where O
the O
limited O
transduction O
models O
often O
pro- O
duce O
many O
ill-formed O
output O
candidates. O
In O
this O
section, O
we O
describe O
the O
sources O
of O
the O
text O
cor- O
pora, O
and O
two O
novel O
methods O
that O
attempt O
to O
lever- O
age O
the O
additional O
information. O
3.1 O
Additional O
Data O
The O
monolingual O
corpora O
come O
from O
one O
of O
two O
sources. O
The O
UniMorph B-DatasetName
project I-DatasetName
(Kirov O
et O
al., O
2018) O
contains O
corpora O
for O
46 O
out O
of O
103 O
lan- O
guages.7For O
42 O
languages O
that O
are O
not O
represented O
in O
Unimorph, B-DatasetName
we O
instead O
use O
the O
target O
side O
of O
the O
high-resource O
training O
data O
in O
this O
shared O
task. O
For O
the O
15 O
remaining O
languages O
that O
lack O
either O
of O
these O
resources, O
we O
simply O
back O
off O
to O
the O
standard O
version O
of O
each O
system. O
Note O
that O
we O
use O
only O
the O
target-side O
forms O
of O
the O
high-resource O
training O
data O
(if O
applicable), O
so O
that O
there O
is O
no O
overlap O
between O
the O
training O
and O
testing O
sets. O
The O
principal O
use O
of O
the O
additional O
data O
is O
to O
6We O
had O
no O
access O
to O
additional O
top- O
npredictions O
from O
the O
B B-MethodName
ASELINE I-MethodName
and O
HAEM B-MethodName
systems. O
7https://unimorph.github.ioconstruct O
a O
list O
of O
all O
word O
types, O
with O
counts, O
into O
atarget O
word O
list O
. O
The O
idea O
is O
to O
bias O
the O
sys- O
tem O
predictions O
towards O
forms O
that O
are O
actually O
ob- O
served O
in O
a O
monolingual O
corpus. O
In O
this O
shared O
task, O
our O
word O
list O
sizes O
vary O
between O
115 O
for O
Ara- O
bic O
and O
22,371 O
for O
Slovene. O
The O
second O
use O
of O
the O
unannotated O
corpora O
is O
to O
derive O
a O
target O
character-level O
n-gram O
language O
model. O
For O
this O
purpose, O
we O
employ O
the O
CMU O
lan- O
guage O
modeling O
toolkit.8 O
3.2 O
DTLM B-MethodName
(UA-06) O
Nicolai O
et O
al. O
(2018) O
present O
DTLM, B-MethodName
a O
new O
system O
that O
combines O
discriminative O
transduction O
with O
character O
and O
word O
language O
models O
derived O
from O
large O
unannotated O
corpora. O
DTLM B-MethodName
is O
an O
exten- O
sion O
of O
D O
IREC O
TL+ O
(Section O
2.3), O
whose O
target O
language O
modeling O
is O
limited O
to O
a O
set O
of O
binary O
n- O
gram O
features, O
which O
are O
based O
exclusively O
on O
the O
target O
sequences O
from O
the O
parallel O
training O
data. O
DTLM B-MethodName
avoids O
the O
error O
propagation O
problem O
that O
is O
inherent O
in O
pipeline O
approaches O
by O
incorporat- O
ing O
the O
language-model O
features O
directly O
into O
the O
transducer. O
In O
addition, O
DTLM B-MethodName
bolsters O
the O
quality O
of O
trans- O
duction O
by O
employing O
a O
novel O
alignment O
method, O
which O
is O
referred O
to O
as O
precision O
alignment. O
The O
idea O
is O
to O
allow O
null O
substrings O
on O
the O
source O
side O
during O
the O
alignment O
of O
the O
training O
data, O
and O
then O
apply O
a O
separate O
aggregation O
algorithm O
to O
merge O
them O
with O
adjoining O
non-empty O
substrings. O
This O
alignment O
method O
results O
in O
substantially O
higher O
transduction O
accuracy.9 O
3.3 O
AC-RNN B-MethodName
with O
Word O
Lists O
(UA-07) O
We O
also O
indirectly O
leverage O
the O
target O
word O
lists O
(ignoring O
the O
counts) O
in O
the O
AC-RNN B-MethodName
model O
(Sec- O
tion O
2.4). O
The O
neural O
network O
is O
trained O
with O
each O
of O
these O
external O
words O
as O
both O
input O
and O
output. O
We O
pre-train O
AC-RNN B-MethodName
with O
this O
copying O
proce- O
dure O
for O
50 B-HyperparameterValue
epochs. B-HyperparameterName
(Bergmanis O
et O
al. O
(2017) O
use O
a O
similar O
technique O
with O
randomly-generated O
se- O
quences.) O
We O
then O
ﬁne-tune O
the O
model O
on O
the O
ac- O
tual O
low-resource O
dataset. O
This O
approach O
is O
helpful O
in O
a O
several O
different O
ways: O
it O
biases O
the O
network O
towards O
copying O
input O
characters O
in O
the O
output, O
guides O
the O
attention O
parameters O
towards O
learning O
a O
monotonic O
alignment, O
and O
improves O
the O
randomly O
8http://www.speech.cs.cmu.edu/SLM/toolkit.html O
9DTLM B-MethodName
was O
also O
succesfully O
used O
in O
the O
NEWS O
2018 O
shared O
task O
on O
transliteration O
(Najaﬁ O
et O
al., O
2018b). O
initialized O
character O
embeddings O
by O
pre-training O
them O
on O
external O
data. O
We O
also O
experimented O
with O
two O
different O
ideas O
to O
re-rank O
predictions O
of O
AC-RNN. B-MethodName
The O
ﬁrst O
idea O
was O
to O
train O
a O
separate O
RNN-based O
language O
model O
to O
re-score O
predictions. O
The O
second O
idea O
was O
to O
learn O
a O
reverse O
model O
that O
would O
gener- O
ate O
the O
input O
lemma O
from O
the O
inﬂected O
form O
and O
tag, O
for O
the O
purpose O
of O
re-scoring O
the O
n-best O
lists O
of O
AC-RNN. B-MethodName
Unfortunately, O
neither O
of O
these O
ap- O
proaches O
outperformed O
the O
copying O
procedure O
out- O
lined O
in O
the O
previous O
paragraph. O
3.4 O
Non-standard O
Combination O
(UA-08) O
We O
take O
advantage O
of O
the O
ability O
of O
both O
DTLM B-MethodName
and O
AC-RNN B-MethodName
to O
produce O
n-best O
lists O
of O
predic- O
tions O
by O
combining O
the O
lists O
via O
a O
linear O
combina- O
tion O
of O
their O
conﬁdence O
scores. O
The O
scores O
from O
each O
model O
are O
normalized, O
and O
the O
linear O
coefﬁ- O
cients O
are O
tuned O
separately O
for O
each O
language O
on O
the O
provided O
development O
sets. O
The O
top O
scoring O
output O
for O
each O
input O
instance O
is O
returned. O
4 O
Results O
Table O
2 O
shows O
the O
average B-MetricName
accuracy I-MetricName
over O
103 O
lan- O
guages O
for O
our O
eight O
submitted O
systems O
in O
the O
low- O
resource O
setting. O
The O
ranking O
of O
the O
systems O
is O
the O
same O
for O
both O
the O
development O
and O
test O
sets.10 O
The O
best O
performing O
individual O
standard O
system O
is O
D B-MethodName
IREC I-MethodName
TL+, I-MethodName
followed O
by O
HAEM, B-MethodName
B B-MethodName
ASELINE I-MethodName
, O
and O
AC-RNN. B-MethodName
We O
conclude O
that O
100 O
training O
in- O
stances O
are O
insufﬁcient O
for O
the O
soft-attention O
based O
neural O
models O
like O
AC-RNN. B-MethodName
Moreover, O
we O
were O
not O
able O
to O
replicate O
the O
superior O
results O
of O
HAEM B-MethodName
reported O
in O
the O
2017 O
shared O
task, O
which O
we O
at- O
tribute O
to O
the O
reasons O
outlined O
in O
Section O
2.2. O
Our O
10Detailed O
results O
on O
all O
languages O
are O
available O
on O
request. O
weighted-voting O
combination O
of O
all O
four O
systems O
substantially O
improves O
over O
each O
individual O
sys- O
tem. O
In O
the O
development O
experiments, O
we O
ob- O
served O
that O
all O
individual O
systems, O
including O
AC- B-MethodName
RNN, I-MethodName
contributed O
to O
the O
accuracy O
of O
the O
combina- O
tion O
system. O
Among O
the O
non-standard O
systems, O
the O
DTLM B-MethodName
model O
easily O
outperforms O
D B-MethodName
IREC I-MethodName
TL+. I-MethodName
The O
copy O
pre-training O
approach O
on O
the O
target O
word O
lists O
al- O
most O
doubles O
the O
accuracy B-MetricName
of O
AC-RNN, B-MethodName
but O
it O
is O
not O
sufﬁcient O
to O
even O
reach O
the O
B B-MethodName
ASELINE I-MethodName
. O
Nev- O
ertheless, O
the O
linear O
combination O
of O
the O
two O
non- O
standard O
systems O
is O
clearly O
the O
best O
of O
our O
submis- O
sions, O
obtaining O
the O
highest O
accuracy B-MetricName
on O
34 O
lan- O
guages O
in O
the O
shared O
task. O
In O
order O
to O
shed O
light O
on O
the O
effect O
of O
additional O
data O
on O
the O
DTLM B-MethodName
results, O
we O
ran O
experiments O
on O
46 O
languages O
that O
have O
both O
the O
Unimorph B-DatasetName
and O
high-resource O
data O
(Table O
3). O
It O
is O
clear O
that O
incor- O
porating O
a O
target O
language O
model O
from O
either O
data O
source O
improves O
the O
overall O
accuracy. O
The O
results O
also O
suggest O
that O
the O
Unimorph B-DatasetName
corpora O
are O
better O
for O
the O
purpose O
of O
deriving O
the O
language O
models O
than O
the O
high-resource O
training O
data. O
The O
addition O
of O
the O
target O
word O
lists O
from O
Unimorph B-DatasetName
further O
im- O
proves O
the O
results. O
However, O
the O
word O
lists O
from O
the O
high-resource O
data O
are O
detrimental. O
Since O
there O
is O
no O
overlap O
between O
the O
training O
and O
develop- O
ment O
data, O
there O
are O
no O
useful O
targets O
in O
the O
word O
lists O
to O
help O
guide O
the O
model O
outputs. O
5 O
Conclusion O
We O
described O
the O
details O
of O
the O
systems O
that O
we O
tested O
on O
103 O
languages O
in O
the O
low-resource O
set- O
ting O
of O
the O
shared O
task. O
In O
particular, O
we O
exper- O
imented O
with O
combining O
diverse O
systems, O
apply- O
ing O
reinforcement O
learning O
to O
neural O
models, O
and O
leveraging O
target O
corpora O
for O
reinﬂection. B-TaskName
Our O
results O
suggest O
that O
these O
techniques O
lead O
to O
im- O
provements O
in O
accuracy B-MetricName
with O
respect O
to O
the O
base O
systems. O
We O
hope O
that O
this O
report O
will O
serve O
as O
a O
useful O
reference O
for O
future O
experiments O
involving O
the O
datasets O
from O
this O
shared O
task.119Acknowledgements O
We O
thank O
Garrett O
Nicolai O
for O
the O
assistance O
with O
DTLM. B-MethodName
We O
thank O
the O
shared O
task O
organizers O
for O
preparing O
a O
large O
number O
of O
datasets, O
and O
smoothly O
executing O
the O
evaluation O
process. O
This O
research O
was O
supported O
by O
the O
Natural O
Sciences O
and O
Engineering O
Research O
Council O
of O
Canada, O
Al- O
berta O
Innovates, O
and O
Alberta O
Advanced O
Education. O