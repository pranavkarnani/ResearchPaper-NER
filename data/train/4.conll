Proceedings O
of O
the O
2022 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics: O
Human O
Language O
Technologies O
, O
pages O
5763 O
=- O
5776 O
July O
10-15, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
Exploiting O
Inductive O
Bias O
in O
Transformers O
for O
Unsupervised O
Disentanglement O
of O
Syntax O
and O
Semantics O
with O
VAEs B-MethodName
Ghazi O
Felhi, O
Joseph O
Le O
Roux O
LIPN O
Université O
Sorbonne O
Paris O
Nord O
=- O
CNRS O
UMR O
7030 O
F-93430, O
Villetaneuse, O
France O
{felhi, O
leroux}@lipn.frDjamé O
Seddah O
INRIA O
Paris O
Paris, O
France O
djame.seddah@inria.fr O
Abstract O
We O
propose O
a O
generative O
model O
for O
text B-TaskName
genera- I-TaskName
tion, I-TaskName
which O
exhibits O
disentangled O
latent O
repre- O
sentations O
of O
syntax O
and O
semantics. O
Contrary O
to O
previous O
work, O
this O
model O
does O
not O
need O
syntac- O
tic O
information O
such O
as O
constituency O
parses, O
or O
semantic O
information O
such O
as O
paraphrase O
pairs. O
Our O
model O
relies O
solely O
on O
the O
inductive O
bias O
found O
in O
attention-based O
architectures O
such O
as O
Transformers. O
In O
the O
attention O
of O
Transformers, O
keys O
handle O
information O
selection O
while O
values O
specify O
what O
information O
is O
conveyed. O
Our O
model, O
dubbed O
QKV B-MethodName
AE, I-MethodName
uses O
Attention O
in O
its O
decoder O
to O
read O
latent O
variables O
where O
one O
latent O
variable O
infers O
keys O
while O
another O
infers O
values. O
We O
run O
experiments O
on O
latent O
representations O
and O
experiments O
on O
syntax/semantics O
transfer O
which O
show O
that O
QKV B-MethodName
AE I-MethodName
displays O
clear O
signs O
of O
disentangled O
syntax O
and O
semantics. O
We O
also O
show O
that O
our O
model O
displays O
competitive O
syn- O
tax O
transfer O
capabilities O
when O
compared O
to O
su- O
pervised O
models O
and O
that O
comparable O
super- O
vised O
models O
need O
a O
fairly O
large O
amount O
of O
data O
(more O
than O
50K O
samples) O
to O
outperform O
it O
on O
both O
syntactic O
and O
semantic O
transfer. O
The O
code O
for O
our O
experiments O
is O
publicly O
available1. O
1 O
Introduction O
Disentanglement, O
a O
process O
aimed O
at O
obtaining O
neu- O
ral O
representations O
with O
identified O
meaning, O
is O
a O
crucial O
component O
of O
research O
on O
interpretability O
(Rudin O
et O
al., O
2022). O
A O
form O
of O
disentanglement O
that O
received O
a O
lot O
of O
interest O
from O
the O
NLP O
commu- O
nity O
is O
the O
separation O
between O
syntax O
and O
semantics O
in O
neural O
representations O
(Chen O
et O
al., O
"2019;" O
Bao O
et O
al., O
"2019;" O
Zhang O
et O
al., O
"2019;" O
Chen O
et O
al., O
"2020;" O
Huang O
and O
Chang, O
"2021;" O
Huang O
et O
al., O
2021). O
Pre- O
vious O
works O
perform O
disentanglement O
using O
para- O
phrase O
pairs O
as O
information O
for O
semantics, O
and/or O
constituency O
parses O
as O
information O
for O
syntax. O
The O
1github.com/ghazi-f/QKV O
AEdependence O
of O
models O
on O
labeled O
data O
is O
known O
to O
entail O
high O
cost O
(see O
Seddah O
et O
al., O
2020 O
on O
syntactic O
annotation), O
and O
to O
often O
require O
new O
labels O
to O
han- O
dle O
problems O
such O
as O
concept O
drift O
(Lu O
et O
al., O
2019) O
and O
domain O
adaptation O
(Farahani O
et O
al., O
2021). O
In O
light O
of O
the O
above, O
we O
propose O
an O
unsuper- O
vised O
model O
which O
directs O
syntax O
and O
semantics O
into O
different O
neural O
representations O
without O
se- O
mantic O
or O
syntactic O
information. O
In O
the O
Trans- O
former O
architecture O
(Vaswani O
et O
al., O
2017), O
the O
attention O
mechanism O
is O
built O
upon O
a O
query O
from O
a O
set O
Q, O
which O
pools O
values O
Vthrough O
keysK. O
For O
each O
query, O
values O
are O
selected O
according O
to O
their O
matching O
score O
computed O
by O
the O
similarity O
between O
their O
corresponding O
keys O
and O
the O
query. O
Building O
on O
an O
analogy O
between O
the O
(K, O
V O
)cou- O
ple O
and O
syntactic O
roles O
with O
their O
lexical O
realiza- O
tions O
(explicited O
in O
§4.2) O
we O
present O
QKV B-MethodName
AE2, I-MethodName
a O
Transformer-based O
Variational O
Autoencoder O
(V O
"AE;" O
Kingma O
and O
Welling, O
2014). O
To O
build O
our O
model, O
we O
modify O
a O
previous O
Transformer-based O
V O
AE, O
called O
the O
Attention- O
Driven O
V O
AE O
(ADV B-MethodName
"AE;" I-MethodName
Felhi O
et O
al., O
2021). O
Using O
Cross-Attention, O
our O
model O
encodes O
sentences O
into O
two O
latent O
variables: O
zsemto O
infer O
values O
for O
V, O
and O
zsynto O
assign O
keys O
in O
Kfor O
values O
in O
V. O
These O
keys O
and O
values O
are O
then O
used O
in O
the O
Attention O
mech- O
anism O
of O
a O
Transformer O
Decoder O
to O
generate O
sen- O
tences. O
We O
show O
that O
zsyntends O
to O
contain O
syn- O
tactic O
information, O
while O
zsemtends O
to O
represent O
semantic O
information. O
Additionally, O
comparisons O
with O
a O
supervised O
model O
show O
that O
it O
needs O
a O
con- O
siderable O
amount O
of O
data O
to O
outperform O
our O
model O
on O
syntactic O
and O
semantic O
transfer O
metrics. O
Our O
contributions O
can O
be O
summarized O
as O
follows: O
•We O
describe O
QKV B-MethodName
AE, I-MethodName
a O
model O
designed O
to O
dis- O
entangle O
syntactic O
information O
from O
semantic O
information O
by O
using O
separate O
latent O
variables O
for O
keys O
and O
values O
in O
Transformers O
Attention. O
2A O
contraction O
of O
the O
(Q, O
K, O
V O
)triplet O
with O
the O
V O
AE O
acronym.5763•We O
run O
experiments O
on O
a O
dataset O
for O
English O
which O
empirically O
show O
that O
the O
two O
types O
of O
latent O
variables O
have O
strong O
preferences O
re- O
spectively O
for O
syntax O
and O
semantic. O
•We O
also O
show O
that O
our O
model O
is O
capable O
of O
transferring O
syntactic O
and O
semantic O
informa- O
tion O
between O
sentences O
by O
using O
their O
respec- O
tive O
latent O
variables. O
Moreover, O
we O
show O
that O
our O
model’s O
syntax O
transfer O
capabilities O
are O
competitive O
with O
supervised O
models O
when O
they O
use O
their O
full O
training O
set O
(more O
than O
400k O
sentences), O
and O
that O
a O
supervised O
model O
needs O
a O
fairly O
large O
amount O
of O
labeled O
data O
(more O
than O
50k O
samples) O
to O
outperform O
it O
on O
both O
semantic O
and O
syntactic O
transfer. O
2 O
Related O
Work O
We O
broadly O
divide O
works O
on O
explainability O
in O
NLP O
into O
two O
research O
directions. O
The O
first O
seeks O
post O
hocexplanations O
for O
black-box O
models, O
and O
led O
to O
a O
rich O
literature O
of O
observations O
on O
the O
behavior O
of O
Neural O
Models O
in O
NLP O
(Tenney O
et O
al., O
"2019;" O
Jawa- O
har O
et O
al., O
"2019;" O
Hu O
et O
al., O
"2020;" O
Kodner O
and O
Gupta, O
"2020;" O
Marvin O
and O
Linzen, O
"2020;" O
Kulmizev O
et O
al., O
"2020;" O
Rogers O
et O
al., O
2020). O
Along O
with O
these O
ob- O
servations, O
this O
line O
of O
works O
also O
led O
to O
numerous O
advances O
in O
methodology O
concerning, O
for O
instance, O
the O
use O
of O
attention O
as O
an O
explanation O
(Jain O
and O
Wallace, O
"2019;" O
Wiegreffe O
and O
Pinter, O
2020), O
the O
validity O
of O
probing O
(Pimentel O
et O
al., O
2020), O
or O
con- O
trastive O
evaluation O
with O
minimal O
pairs O
(Vamvas O
and O
Sennrich, O
2021). O
The O
second O
research O
direction O
on O
explainability O
in O
NLP O
seeks O
to O
build O
models O
that O
are O
explainable O
by O
design. O
This O
led O
to O
models O
with O
explicit O
linguistically O
informed O
mechanisms O
such O
as O
the O
induction O
of O
grammars O
"(RNNG;" O
Dyer O
et O
al., O
2016, O
"URNNG;" O
Kim O
et O
al., O
2019) O
or O
constituency O
trees O
"(ON-LSTM;" O
Shen O
et O
al., O
2019, O
ONLSTM- O
"SYD;" O
Du O
et O
al., O
2020). O
Disentangled O
representation O
learning O
is O
a O
sub- O
field O
of O
this O
second O
research O
direction O
which O
aims O
at O
separating O
neural O
representations O
into O
neurons O
with O
known O
associated O
meanings. O
This O
separation O
was O
performed O
on O
various O
characteristics O
in O
text O
such O
as O
style O
(John O
et O
al., O
"2020;" O
Cheng O
et O
al., O
2020), O
sentiment O
and O
topic O
(Xu O
et O
al., O
2020), O
or O
word O
mor- O
phology O
(Behjati O
and O
Henderson, O
2021). O
In O
works O
on O
disentanglement, O
consequent O
efforts O
have O
been O
put O
in O
the O
separation O
between O
syntax O
and O
semantics, O
whether O
merely O
to O
obtain O
an O
interpretable O
special- O
ization O
in O
the O
embedding O
space O
(Chen O
et O
al., O
"2019;Bao" O
et O
al., O
"2019;" O
Ravfogel O
et O
al., O
"2020;" O
Huang O
et O
al., O
2021), O
or O
for O
controllable O
generation O
(Zhang O
et O
al., O
"2019;" O
Chen O
et O
al., O
"2020;" O
Huang O
and O
Chang, O
"2021;" O
Li O
et O
al., O
2021). O
However, O
all O
these O
works O
rely O
on O
syntactic O
information O
(constituency O
parses O
and O
PoS O
tags) O
or O
semantic O
information O
(paraphrase O
pairs). O
To O
the O
best O
of O
our O
knowledge, O
our O
work O
is O
the O
first O
to O
present O
a O
method O
that O
directs O
syntactic O
and O
se- O
mantic O
information O
into O
assigned O
embeddings O
in O
the O
challenging O
unsupervised O
setup. O
From O
a O
broader O
machine O
learning O
perspective, O
using O
knowledge O
of O
the O
underlying O
phenomena O
in O
our O
data, O
we O
design O
our O
model O
QKV B-MethodName
AE I-MethodName
with O
an O
in- O
ductive O
bias O
that O
induces O
understandable O
behavior O
in O
an O
unsupervised O
fashion. O
Among O
the O
existing O
line O
of O
applications O
of O
this O
principle O
(Rezende O
et O
al., O
"2016;" O
Hudson O
and O
Manning, O
"2018;" O
Locatello O
et O
al., O
"2020;" O
Tjandra O
et O
al., O
2021), O
ADV O
AE O
(Felhi O
et O
al., O
2021), O
the O
model O
on O
which O
QKV B-MethodName
AE I-MethodName
is O
based, O
is O
de- O
signed O
to O
separate O
information O
from O
the O
realizations O
of O
different O
syntactic O
roles O
without O
supervision O
on O
a O
dataset O
of O
regularly O
structured O
sentences. O
3 O
Background O
In O
this O
section, O
we O
go O
over O
the O
components O
of O
our O
model, O
namely O
V O
AEs, O
attention O
in O
Transformers, O
and O
ADV O
AE, O
the O
model O
on O
which O
QKV O
AE O
is O
based. O
3.1 O
VAEs O
as O
Language O
Models O
Given O
a O
set O
of O
observations O
w, O
V O
AEs O
are O
a O
class O
of O
deep O
learning O
models O
that O
train O
a O
generative O
model O
pθ(w) O
=/integraltext O
zp(z)pθ(w|z)dz, O
where O
p(z)is O
a O
prior O
distribution O
on O
latent O
variables O
zthat O
serve O
as O
a O
seed O
for O
generation, O
and O
pθ(w|z)is O
called O
the O
de- O
coder O
and O
generates O
an O
observation O
wfrom O
each O
latent O
variable O
value O
z. O
Since O
directly O
maximizing O
the O
likelihood O
pθ(w)to O
train O
a O
generative O
model O
is O
intractable, O
an O
approximate O
inference O
distribution O
qϕ(z|w), O
called O
the O
encoder, O
is O
used O
to O
formulate O
a O
lower-bound O
to O
the O
exact O
log-likelihood O
of O
the O
model, O
called O
the O
Evidence O
Lower-Bound O
(ELBo): O
logpθ(w)≥ O
E(z)∼qϕ(z|w)[logpθ(w|z)]− O
KL[qϕ(z|w)||p(z)] O
= O
ELBo( O
"w;z)(1)" O
Early O
works O
on O
V O
AEs O
as O
language O
models O
have O
shown O
that, O
contrary O
to O
non-generative O
sequence- O
to-sequence O
(Sutskever O
et O
al., O
2014) O
models, O
they O
learn O
a O
smooth O
latent O
space O
(Bowman O
et O
al., O
2016). O
In O
fact, O
this O
smoothness O
enables O
decoding O
an O
inter- O
polation O
of O
latent O
codes O
( O
i.e.a O
homotopy) O
coming5764from O
two O
sentences O
to O
yield O
a O
well-formed O
third O
sentence O
that O
clearly O
shares O
characteristics O
(syntac- O
tic, O
semantic. O
. O
. O
) O
with O
both O
source O
sentences. O
This O
interpolation O
will O
be O
used O
as O
a O
control O
baseline O
in O
our O
experiments. O
3.2 O
Attention O
in O
Transformers. O
The O
inductive O
bias O
responsible O
for O
the O
disentangle- O
ment O
capabilities O
of O
our O
model O
is O
based O
on O
the O
de- O
sign O
of O
Attention O
in O
Transformers O
(Vaswani O
et O
al., O
2017). O
In O
attention O
mechanisms, O
each O
element O
of O
a O
series O
of O
query O
vectors O
Q={q1, O
. O
. O
. O
, O
q O
|Q|}per- O
forms O
a O
soft O
selection O
of O
values O
V={v1, O
. O
. O
. O
, O
v O
|V|} O
whose O
compatibility O
with O
the O
query O
is O
given O
by O
their O
corresponding O
key O
vector O
in O
K={k1, O
. O
. O
. O
, O
k O
|V|} O
via O
dot O
product. O
For O
each O
qi∈Q, O
the O
series O
of O
dot O
products O
is O
normalized O
and O
used O
as O
weights O
for O
a O
convex O
interpolation O
of O
the O
values. O
Formally, O
the O
result O
is O
compactly O
written O
as: O
Attention( O
Q, O
K, O
V O
) O
= O
Softmax( O
QKT)V(2) O
Here, O
we O
stress O
that O
Kis O
only O
capable O
of O
con- O
trolling O
what O
information O
is O
selected O
from O
V, O
while O
Vis O
responsible O
for O
the O
value O
of O
this O
information. O
Using O
the O
above O
operators O
and O
the O
embedding O
level O
concatenation O
operator O
Cat, O
Multi-Head O
Attention O
(MHA O
) O
in O
Transformers O
is O
defined O
as O
follows: O
MHA( O
˜Q,˜K,˜V) O
= O
Cat( O
head O
1, O
...head O
H)WO O
s.t:head O
i= O
Attention( O
˜QWQ O
i,˜KWK O
i,˜V O
WV O
i) O
Where O
WO,WQ O
i,WK O
i, O
and O
WV O
iare O
trainable O
pa- O
rameter O
matrices. O
In O
turn, O
Self-Attention O
( O
SA) O
and O
Cross-Attention O
( O
CA) O
are O
defined, O
for O
sets O
of O
ele- O
ments O
called O
source O
Sand O
target O
T, O
as O
follows: O
SA(T) O
= O
MHA( O
T, O
T, O
T O
) O
CA(T, O
S) O
= O
MHA( O
T, O
S, O
S O
) O
The O
above O
SAmechanism O
is O
used O
to O
exchange O
information O
between O
elements O
of O
target O
T, O
while O
inCA, O
targets O
Tpull O
(or O
query O
for) O
information O
from O
each O
element O
of O
the O
source O
S. O
Transformer O
Encoders O
( O
Enc) O
are O
defined O
as O
the O
composition O
of O
layers O
each O
consisting O
of O
an O
attention O
followed O
by O
a O
Feed-Forward O
Network O
F:3 O
Enc(T) O
=˜TDenc,s.t.˜Td=/braceleftbiggTifd= O
0,else: O
F(SA( O
˜Td−1)) O
3We O
omit O
residual O
connections O
and O
layer O
normalizations O
after O
each O
SAorCAfor O
simplicity.Transformer O
Decoders O
( O
Dec) O
are O
defined O
with O
instances O
of O
SA,CAandF: O
Dec(T, O
S) O
=˜TDdec,s.t.: O
˜Td=/braceleftbiggTifd= O
0,else: O
F(CA(SA( O
˜Td−1), O
S)) O
where O
DencandDdecabove O
are O
respectively O
the O
number O
of O
layers O
of O
Enc O
andDec. O
For O
autoregres- O
sive O
decoding, O
Vaswani O
et O
al. O
-2017 O
define O
a O
ver- O
sion O
of O
Dec O
we O
will O
call O
Dec. O
In O
this O
version, O
the O
result O
of O
each O
QKT(Eq. O
2) O
in O
Self-Attention O
is O
masked O
so O
that O
each O
tiinTonly O
queries O
for O
infor- O
mation O
from O
tjwithj≤i. O
Even O
though O
Dec O
yields O
a O
sequence O
of O
length O
equal O
to O
that O
of O
target O
T, O
in O
the O
following O
sections O
we O
will O
consider O
its O
output O
to O
be O
only O
the O
last O
element O
of O
˜TDdecin O
order O
to O
express O
auto-regressive O
generation O
in O
a O
clear O
manner. O
3.3 O
ADVAE O
ADV O
AE O
is O
a O
Variational O
Autoencoder O
for O
unsuper- O
vised O
disentanglement O
of O
sentence O
representations. O
It O
mainly O
differs O
from O
previous O
LSTM-based O
(Bow- O
man O
et O
al., O
2016) O
and O
Transformer-based O
(Li O
et O
al., O
2020b) O
V O
AEs O
in O
that O
it O
uses O
Cross-Attention O
to O
en- O
code O
and O
decode O
latent O
variables, O
which O
is O
the O
cor- O
nerstone O
of O
our O
model. O
In O
ADV B-MethodName
AE, I-MethodName
Cross-Attention O
is O
used O
to: O
i)encode O
information O
from O
sentences O
into O
a O
fixed O
number O
of O
vectorial O
latent O
"variables;" O
ii)decode O
these O
vectorial O
latent O
variables O
by O
using O
them O
as O
sources O
for O
the O
target O
sentences O
generated O
by O
a O
Transformer O
Decoder. O
Formally, O
let O
us O
define O
Mµ,Mσ, O
and O
Mwto O
be O
linear O
layers O
that O
will O
respectively O
be O
used O
to O
obtain O
the O
latent O
variables’ O
means O
and O
standard O
deviations, O
and O
the O
generated O
words’ O
probabili- O
ties,Lthe O
number O
of O
vectorial O
latent O
variables O
z={z1, O
. O
. O
. O
, O
z O
L}, O
and O
finally O
E={e1, O
. O
. O
. O
, O
e O
L} O
andD={d1, O
. O
. O
. O
, O
d O
L}two O
sets O
of O
Ltrainable O
em- O
beddings. O
Embeddings O
eianddiserve O
as O
fixed O
identifiers O
for O
the O
latent O
variable O
zirespectively O
in O
the O
encoder O
and O
in O
the O
decoder. O
Given O
input O
token O
sequence O
w, O
the O
encoder O
qϕ(z|w) O
=/producttext O
lqϕ(zl|w)first O
yields O
parameters O
µl O
andσlto O
be O
used O
by O
the O
diagonal O
Gaussian O
distri- O
bution O
of O
each O
of O
the O
latent O
variables O
zlas O
follows4: O
4To O
simplify O
equations, O
we O
omit O
word O
embedding O
look-up O
tables O
and O
positional O
embeddings O
˜z= O
Dec( O
"e;" O
Enc( O
w)) O
∀ls.t.1≤l≤L: O
µl=Mµ(˜zl), O
σl= O
SoftPlus( O
Mσ(˜zl)) O
zl∼ O
"N(µl;σl)" O
-3 O
Cross-Attention O
is O
also O
used O
by O
the O
ADV B-MethodName
AE I-MethodName
decoder O
to O
dispatch O
information O
from O
the O
source O
latent O
variable O
samples O
to O
the O
target O
generated O
sequence. O
Accordingly, O
using O
a O
beginning-of- O
sentence O
token O
w0,pθ(w|z) O
=/producttext O
ipθ(wi|w<i, O
z) O
yields O
probabilities O
for O
the O
categorical O
distribution O
of O
the O
generated O
tokens O
wby O
decoding O
latent O
vari- O
ables O
zconcatenated O
with O
their O
embeddings O
d: O
y= O
Cat( O
"d;z)" O
∀is.t.1≤i≤ O
|w|: O
˜wi=Dec(w0, O
. O
. O
. O
, O
w B-MethodName
"i−1;" O
Enc( O
y)) O
wi∼Categorical(Softmax( O
Mw( O
˜wi))) O
4 O
QKVAE: O
Using O
separate O
latent O
variables O
for O
Keys O
and O
Values O
In O
this O
section, O
we O
describe O
the O
architecture O
of O
our O
model, O
the O
behavior O
it O
entails, O
and O
how O
we O
deal O
with O
the O
optimization O
challenges O
it O
poses. O
4.1 O
QKVAE B-MethodName
architecture O
The O
modification O
we O
bring O
to O
ADV B-MethodName
AE I-MethodName
is O
aimed O
at O
controlling O
how O
information O
is O
selected O
from O
the O
latent O
space O
with O
the O
value O
of O
a O
newly O
introduced O
latent O
variable. O
We O
call O
this O
latent O
variable O
zsyn, O
and O
refer O
to O
the O
latent O
variables O
already O
formulated O
in O
ADV B-MethodName
AE I-MethodName
as O
zsem={zsem O
1, O
. O
. O
. O
, O
zsem O
L}.zsynis O
obtained O
with O
the O
same O
process O
as O
each O
zsem O
l(Eq. O
3),i.e.by O
adding O
an O
additional O
identifier O
embed- O
dinges, O
and O
matrices O
MµsandMσsto O
obtain O
its O
mean O
and O
standard-deviation O
parameters. O
For O
the O
QKV O
AE O
Decoder, O
we O
modify O
the O
Trans- O
former O
Decoder O
Dec O
intoQKVDec O
so O
as O
to O
use O
Multi-Head O
Attention O
with O
separate O
inputs O
for O
keys O
and O
values O
instead O
of O
Cross-Attention O
:QKVDec( O
"T;SK;SV)" O
=˜TDQKV,s.t. O
: O
˜Td=/braceleftbiggTifd= O
0,else: O
F(MHA(SA( O
˜Td−1), O
SK, O
SV) O
where O
DQKVis O
the O
number O
of O
layers. O
Similar O
to O
Dec, O
we O
define O
QKVDec O
to O
be O
the O
auto-regressive O
version O
of O
QKVDec O
. O
The O
QKV O
AE O
decoder O
yields O
probabilities O
for O
the O
generated O
tokens O
by O
using O
this O
operator O
on O
values O
given O
by O
zsemconcatenated O
with O
embeddings O
d, O
and O
keys O
given O
by O
a O
linear O
transfor- O
mation O
on O
zsyn: O
v= O
Cat( O
"d;zsem)," O
k=Ms(zsyn) O
∀is.t.1≤i≤ O
|w|: O
˜wi=QKVDec( O
w0, O
. O
. O
. O
, O
w O
"i−1;k;v)" O
wi∼Categorical(Softmax( O
Mw( O
˜wi))) O
where O
Msis O
a O
linear O
layer.5While O
ADV B-MethodName
AE I-MethodName
already O
uses O
Cross-Attention O
to O
encode O
and O
decode O
latent O
variables, O
our O
model O
uses O
separate O
variables O
to O
obtain O
keys O
and O
values O
for O
Multi-Head O
Attention O
in O
its O
decoder. O
4.2 O
QKVAE B-MethodName
Behavior O
In O
the O
Multi-Head O
Attention O
of O
our O
decoder, O
zsyn O
controls O
keys, O
and O
zsemcontrols O
values. O
In O
other O
words, O
the O
value O
of O
each O
zsem O
lis O
called O
to O
be O
passed O
to O
the O
target O
sequence O
according O
to O
its O
key O
which O
is O
given O
by O
the O
variable O
zsyn. O
Therefore, O
given O
a O
query, O
zsyndecides O
which O
content O
vector O
zsem O
lpar- O
ticipates O
most O
to O
the O
value O
of O
the O
generated O
token O
at O
each O
generation O
step. O
To O
better O
get O
a O
gist O
of O
the O
kind O
of O
behavior O
intended O
by O
this O
construction, O
we O
assume O
in O
Table O
1 O
for O
explanatory O
purposes, O
that O
our O
decoder O
has O
one O
layer O
and O
one O
attention O
head, O
that O
the O
value O
of O
each O
klin O
key O
matrices O
k1and O
k2corresponds O
to O
syntactic O
roles, O
and O
that O
each O
vl O
informs O
on O
the O
realization O
of O
the O
corresponding O
syn- O
tactic O
role. O
Table O
1 O
displays O
the O
resulting O
sentence O
when O
each O
of O
k1andk2are O
coupled O
with O
v. O
5The O
output O
of O
Msis O
reshaped O
to O
obtain O
a O
matrix O
of O
keys.5766In O
the O
examples O
in O
Table O
1, O
the O
generator O
uses O
a O
query O
at O
each O
generation O
step O
to O
pick O
a O
word O
in O
a O
manner O
that O
would O
comply O
with O
English O
syntax. O
Therefore, O
the O
key O
of O
each O
value O
should O
inform O
on O
its O
role O
in O
the O
target O
structure, O
which O
justifies O
syntactic O
roles O
as O
an O
adequate O
meaning O
for O
keys. O
Although O
our O
model O
may O
stray O
from O
this O
possi- O
bility O
and O
formulate O
non-interpretable O
values O
and O
keys, O
keys O
will O
still O
inform O
on O
the O
roles O
of O
values O
in O
the O
target O
structure, O
and O
therefore O
influence O
the O
way O
values O
are O
injected O
into O
the O
target O
sequence. O
And O
given O
the O
fact O
that O
our O
model O
uses O
multiple O
layers O
and O
attention O
heads O
and O
the O
continuous O
nature O
of O
keys O
in O
Attention O
(as O
opposed O
to O
discrete O
syntac- O
tic O
role O
labels), O
our O
model O
performs O
a O
multi-step O
and O
continuous O
version O
of O
the O
behavior O
described O
in O
Table O
1 O
Injecting O
values O
into O
the O
structure O
of O
a O
sentence O
requires O
the O
decoder O
to O
model O
this O
structure. O
Previ- O
ous O
works O
have O
shown O
that O
this O
is O
well O
within O
the O
capabilities O
of O
Transformers. O
Specifically, O
Hewitt O
and O
Manning O
-2019 O
showed O
that O
Transformers O
em- O
bed O
syntactic O
trees O
in O
their O
inner O
representations, O
Clark O
et O
al. O
-2019 O
showed O
that O
numerous O
atten- O
tion O
heads O
attend O
to O
specific O
syntactic O
roles, O
and O
we O
(Felhi O
et O
al., O
2021) O
showed O
that O
Transformer-based O
V O
AEs O
can O
capture O
the O
realizations O
of O
syntactic O
roles O
in O
latent O
variables O
obtained O
with O
Cross-Attention. O
4.3 O
Balancing O
the O
Learning O
of O
zsemandzsyn O
Similar O
to O
ADV B-MethodName
AE, I-MethodName
we O
use O
a O
standard O
Normal O
distribution O
as O
a O
prior O
p(z) O
=p(zsem)p(zsyn)and O
train O
QKV B-MethodName
AE I-MethodName
with O
the O
β-V O
AE O
objective O
(Higgins O
et O
al., O
2017) O
which O
is O
simply O
ELBo O
(Eq. O
1) O
with O
a O
weight O
βon B-HyperparameterName
its O
Kullback-Leibler O
( O
KL) O
term. O
Hig- O
gins O
et O
al. O
-2017 O
show O
that O
a O
higher O
βleads O
to O
better O
unsupervised O
disentanglement. O
However, O
the O
KLterm O
is O
responsible O
for O
a O
phenomenon O
called O
posterior O
collapse O
where O
the O
latent O
variables O
be- O
come O
uninformative O
and O
are O
not O
used O
by O
the O
de- O
coder O
(Bowman O
et O
al., O
2016). O
Therefore, O
higher O
val- O
ues O
for O
βcause B-HyperparameterName
poorer O
reconstruction O
performance O
(Chen O
et O
al., O
2018). O
To O
avoid O
posterior O
collapse, O
we O
follow O
Li O
et O
al. O
(2020a): O
i)We O
pretrain O
our O
model O
as O
an O
autoencoder O
by O
setting O
βto B-HyperparameterName
"0;" B-HyperparameterValue
ii)We O
linearly O
increase O
βto O
its O
final O
value O
( O
"KLannealing;" O
Bow- O
man O
et O
al., O
2016) O
and O
we O
threshold O
each O
dimension O
of O
the O
KLterm O
with O
a O
factor O
λ(Free-Bits B-HyperparameterName
"strategy;" B-HyperparameterName
Kingma O
et O
al., O
2016). O
In O
preliminary O
experiments O
with O
our O
model, O
we O
observed O
that O
it O
tends O
to O
encode O
sentences O
usingonlyzsem. O
As O
we O
use O
conditionally O
independent O
posteriors6q(zsyn|w)andq(zsem|w)for O
our O
latent O
variables, O
their O
KLterms O
(Eq. O
1) O
can O
be O
written O
seperately, O
and O
they O
can O
therefore O
be O
weighted O
sep- O
arately O
with O
different O
values O
of O
β. B-HyperparameterName
Using O
a O
lower O
β B-HyperparameterName
forzsynas O
was O
done O
by O
(Chen O
et O
al., O
2020)7did O
not O
prove O
effective O
in O
making O
it O
informative O
for O
the O
model. O
Alternatively, O
linearly O
annealing O
βforzsem B-HyperparameterName
before O
zsyndid O
solve O
the O
issue. O
This O
intervention O
on O
the O
learning O
process O
was O
inspired O
by O
the O
work O
of O
Li O
et O
al. O
(2020c) O
which O
shows O
that O
latent O
variables O
used O
at O
different O
parts O
of O
a O
generative O
model O
should O
be O
learned O
at O
different O
paces. O
5 O
Experiments O
5.1 O
Setup O
Data O
To O
compare O
our O
model O
to O
its O
supervised O
counterparts, O
we O
train O
it O
with O
data O
from O
the O
En- O
glish O
machine-generated O
paraphrase O
pairs O
dataset O
ParaNMT O
(Wieting O
and O
Gimpel, O
2018). O
More O
specifically, O
we O
use O
the O
493K O
samples O
used O
by O
Chen O
et O
al. O
(2020)8to O
train O
their O
model O
VGV O
AE. O
Since O
our O
model O
is O
unsupervised, O
we O
only O
use O
the O
reference O
sentences O
(half O
the O
training O
set) O
to O
train O
our O
model. O
Using O
the O
development O
and O
test O
sets O
of O
ParaNMT, O
Chen O
et O
al. O
-2020 O
also O
provide O
a O
curated O
set O
of O
triplets O
formed O
by O
a O
target O
sentence O
( O
target O
), O
a O
semantic O
source O
( O
sem_src O
),and O
a O
syntactic O
source O
(syn_src O
). O
The O
semantic O
source O
is O
a O
paraphrase O
of O
the O
target O
sentence, O
while O
the O
syntactic O
source O
is O
selected O
by O
finding O
a O
sentence O
that O
is O
syntactically O
close O
to O
the O
target O
( O
i.e.edit O
distance O
between O
the O
sequence O
of O
PoS O
Tags O
of O
both O
sentences O
is O
low9) O
and O
semantically O
different O
from O
the O
paraphrase O
(has O
low O
BLEU O
score O
with O
it). O
Contrary O
to O
paraphrases O
in O
the O
training O
set O
of O
ParaNMT, O
paraphrases O
from O
this O
set O
were O
manually O
curated. O
These O
triplets O
are O
divided O
into O
a O
development O
set O
of O
500 O
samples O
and O
a O
test O
set O
of O
800 O
samples. O
We O
display O
results O
on O
the O
test O
set O
in O
the O
main O
body O
of O
the O
paper. O
The O
results O
on O
the O
development O
set, O
which O
lead O
to O
the O
same O
conclusions, O
are O
reported O
in O
Appendix O
A. O
Training O
details O
& O
hyper-parameters O
Encoders O
and O
Decoders O
in O
QKV B-MethodName
AE I-MethodName
are O
initialized O
with O
pa- O
6These O
posteriors O
are O
ADV B-MethodName
AE I-MethodName
encoders O
(Eq. O
3). O
7Although O
not O
explicitly O
mentioned O
in O
the O
paper, O
this O
is O
performed O
in O
their O
companion O
source O
code. O
8https://drive.google.com/open?id=1HHDlUT_- O
WpedL6zNYpcN94cLwed_yyrP O
9We O
follow O
Chen O
et O
al. O
-2020 O
by O
using O
this O
evaluation O
data, O
although O
edit O
distance O
between O
PoS O
tags O
might O
not O
be O
a O
good O
proxy O
for O
syntactic O
similarity.5767rameters O
from O
BART O
(Lewis O
et O
al., O
2020). O
After O
manual O
trial O
and O
error O
on O
the O
development O
set, O
we O
set O
the O
sizes O
of O
zsynandzsemto O
768, O
and O
Lto O
4 O
Further O
Hyper-parameters O
are O
in O
Appendix O
B. O
We O
train O
5 O
instances O
of O
our O
model O
and O
report O
the O
average O
scores O
throughout O
all O
experiments. O
Baselines O
We O
compare O
our O
system O
to O
4 O
previ- O
ously O
published O
models, O
where O
2 O
are O
supervised O
and O
2 O
are O
unsupervised: O
i) O
VGVAE B-MethodName
(Chen O
et O
al., O
2020): O
a O
V O
AE-based O
paraphrase O
generation O
model O
with O
an O
LSTM O
architecture. O
This O
model O
is O
trained O
using O
paraphrase O
pairs O
and O
PoS O
Tags O
to O
separate O
syntax O
and O
semantics O
into O
two O
latent O
variables. O
This O
separation O
is O
used O
to O
separately O
specify O
semantics O
and O
syntax O
to O
the O
decoder O
in O
order O
to O
produce O
para- O
"phrases;" O
ii) O
SynPG B-MethodName
(Huang O
and O
Chang, O
2021): O
A O
paraphrase O
generation O
Seq2Seq O
model O
based O
on O
a O
Transformer O
architecture O
which O
also O
separately O
encodes O
syntax O
and O
semantics O
for O
the O
same O
pur- O
pose O
as O
VGV B-MethodName
AE. I-MethodName
This O
model O
is, O
however, O
trained O
using O
only O
source O
sentences O
with O
their O
syntactic O
parses, O
without O
"paraphrases;" O
iii) O
Optimus O
(Li O
et O
al., O
2020b): O
A O
large-scale O
V O
AE O
based O
on O
a O
fusion O
be- O
tween O
BERT O
(Devlin O
et O
al., O
2019) O
and O
GPT-2 O
(Rad- O
ford O
et O
al., O
2019) O
with O
competitive O
performance O
on O
various O
NLP O
"benchmarks;" O
iv) O
ADVAE: B-MethodName
This O
model O
is O
QKV B-MethodName
AE I-MethodName
without O
its O
syntactic O
variable. O
The O
size O
of O
its O
latent O
variable O
is O
set O
to O
1536 O
to O
equal O
the O
total O
size O
of O
latent O
variables O
in O
QKV B-MethodName
AE. I-MethodName
Official O
open-source O
instances10of O
the O
4 O
mod- O
els O
above O
are O
available, O
which O
ensures O
accurate O
comparisons. O
The O
off-the-shelf O
instances O
of O
VG- B-MethodName
V I-MethodName
AE I-MethodName
and O
SynPG B-MethodName
are O
trained O
on O
ParaNMT B-DatasetName
with O
GloVe11(Pennington O
et O
al., O
2014) O
embeddings. O
We O
fine-tune O
a O
pre-trained O
Optimus O
on O
our O
training O
set O
following O
instructions O
from O
the O
authors. O
Similar O
to O
our O
model, O
we O
initialize O
ADV B-MethodName
AE I-MethodName
with O
param- O
eters O
from O
BART(Lewis O
et O
al., O
2020) O
and O
train O
5 O
instances O
of O
it O
on O
ParaNMT B-DatasetName
with O
L= O
4 O
5.2 O
Syntax O
and O
Semantics O
Separation O
in O
the O
Embedding O
Space O
We O
first O
test O
whether O
zsynandzsemrespectively O
specialize O
in O
syntax O
and O
semantics. O
A O
syntactic O
(resp. O
semantic) O
embedding O
should O
place O
syntacti- O
cally O
(resp. O
semantically) O
similar O
sentences O
close O
10VGV O
AE: O
github.com/mingdachen/syntactic-template- O
"generation/;" O
SynPG: O
"github.com/uclanlp/synpg;" O
Op- O
timus: O
"github.com/ChunyuanLI/Optimus;" O
ADV B-MethodName
AE: I-MethodName
github.com/ghazi-f/ADV B-MethodName
AE I-MethodName
11Gains O
could O
be O
observed O
with O
better O
embeddings O
for O
su- O
pervised O
models, O
but O
we O
stick O
to O
the O
original O
implementations.zsem↑zsyn↓ O
to O
each O
other O
in O
the O
embedding O
space. O
Using O
the O
( O
target, O
sem_src, O
syn_src O
) O
triplets, O
we O
calculate O
for O
each O
embedding O
the O
probability O
that O
target O
is O
closer O
to O
sem_src O
than O
it O
is O
to O
syn_src O
in O
the O
embedding O
space. O
For O
simplicity, O
we O
refer O
to O
the O
syntactic O
and O
semantic O
embeddings O
of O
all O
models O
aszsynandzsem. O
For O
Gaussian O
latent O
variables, O
we O
use O
the O
mean O
parameter O
as O
a O
representation O
(respec- O
tively O
the O
mean O
direction O
parameter O
from O
the O
von O
Mises-Fisher O
distribution O
of O
the O
semantic O
variable O
of O
VGV B-MethodName
AE). I-MethodName
We O
use O
an O
L2 O
distance O
for O
Gaussian O
variables O
and O
a O
cosine O
distance O
for O
the O
others. O
Since O
Optimus O
and O
ADV B-MethodName
AE I-MethodName
do O
not O
have O
separate O
embed- O
dings O
for O
syntax O
and O
semantics O
i)We O
take O
the O
whole O
embedding O
for O
"Optimus;" O
ii)For O
ADV B-MethodName
AE, I-MethodName
we O
mea- O
sure O
the O
above O
probability O
on O
the O
development O
set O
for O
each O
latent O
variable O
zl(Eq. O
3). O
Then, O
we O
choose O
the O
latent O
variable O
that O
places O
target O
sentences O
clos- O
est O
to O
their O
sem_src O
(resp. O
syn_src O
) O
as O
a O
semantic O
(resp. O
syntactic) O
variable. O
The O
results O
are O
presented O
in O
Table O
2 O
Table O
2 O
clearly O
shows O
for O
QKV B-MethodName
AE, I-MethodName
SynPG, B-MethodName
and O
VGV B-MethodName
AE I-MethodName
that O
the O
syntactic O
(resp. O
semantic) O
vari- O
ables O
lean O
towards O
positioning O
sentences O
in O
the O
em- O
bedding O
space O
according O
to O
their O
syntax O
(resp. O
se- O
mantics). O
Surprisingly, O
the O
syntactic O
variable O
of O
our O
model O
specializes O
in O
syntax O
( O
i.e.has O
low O
score) O
as O
much O
as O
that O
of O
SynPG. B-MethodName
The O
generalist O
latent O
variable O
of O
Optimus O
seems O
to O
position O
sentences O
in O
the O
latent O
space O
according O
to O
their O
semantics. O
Accordingly, O
we O
place O
its O
score O
in O
the O
zsemcol- O
umn. O
Interestingly, O
the O
variables O
in O
ADV B-MethodName
AE I-MethodName
have O
very O
close O
scores O
and O
score O
well O
below O
50, O
which O
shows O
that O
the O
entire O
ADV B-MethodName
AE I-MethodName
embedding O
leans O
more O
towards O
syntax. O
This O
means O
that, O
without O
the O
key/value O
distinction O
in O
the O
Attention-based O
de- O
coder, O
the O
variables O
specialize O
more O
in O
structure O
than O
in O
content. O
5.3 O
Syntactic O
and O
Semantic O
Transfer O
Similar O
to O
(Chen O
et O
al., O
2020), O
we O
aim O
to O
produce O
sentences O
that O
take O
semantic O
content O
from O
sem_src O
sentences O
and O
syntax O
from O
syn_src O
sentences. O
For O
each O
of O
SynPG, B-MethodName
VGV B-MethodName
AE, I-MethodName
and O
QKV B-MethodName
AE I-MethodName
we O
simply O
use O
the O
syntactic O
embedding O
of O
syn_src O
, O
and O
the O
semantic O
embedding O
of O
sem_src O
as O
inputs O
to O
the O
de- O
coder O
to O
produce O
new O
sentences. O
Using O
the O
results O
of O
the O
specialization O
test O
in O
the O
previous O
experiment, O
we O
do O
the O
same O
for O
ADV B-MethodName
AE I-MethodName
by O
taking O
the O
2 O
latent O
variables O
that O
lean O
most O
to O
semantics O
(resp. O
syntax) O
as O
semantic O
(resp. O
syntactic) O
variables. O
The O
out- O
put O
sentences O
are O
then O
scored O
in O
terms O
of O
syntactic O
and O
semantic O
similarity O
with O
sem_src O
,syn_src O
and O
target O
. O
Control O
and O
reference O
baselines O
Beside O
model O
outputs, O
we O
also O
use O
our O
syntactic O
and O
semantic O
comparison O
metrics, O
explicited O
below, O
to O
compare O
syn_src O
andsem_src O
sentences O
to O
one O
another O
and O
totarget O
sentences. O
Additionally, O
using O
Optimus, O
we O
embed O
sem_src O
andsyn_src O
, O
take O
the O
dimension- O
wise O
average O
of O
both O
embeddings, O
and O
decode O
it. O
As O
V O
AEs O
are O
known O
to O
produce O
quality O
sentence O
in- O
terpolations O
(Bowman O
et O
al., O
"2016;" O
Li O
et O
al., O
2020b),the O
scores O
for O
this O
sentence O
help O
contrast O
a O
naïve O
fusion O
of O
features O
in O
the O
embedding O
space O
with O
a O
composition O
of O
well O
identified O
disentangled O
fea- O
tures. O
Transfer O
metrics O
We O
measure O
the O
syntactic O
and O
semantic O
transfer O
from O
source O
sentences O
to O
output O
sentences. O
i) O
Semantics: O
For O
semantics, O
previous O
works O
(Chen O
et O
al., O
"2020;" O
Huang O
and O
Chang, O
2021) O
rely O
on O
lexical O
overlap O
measures O
such O
as O
BLEU B-MetricName
(Papineni O
et O
al., O
2001), O
ROUGE B-MetricName
(Lin, O
2004), O
and O
Meteor B-MetricName
(Denkowski O
and O
Lavie, O
2014). O
As O
will O
be O
shown O
in O
our O
results, O
the O
lexical O
overlap O
signal O
does O
not O
capture O
semantic O
transfer O
between O
sen- O
tences O
when O
this O
transfer O
is O
too O
weak O
to O
produce O
paraphrases. O
Therefore, O
we O
use O
Meteor B-MetricName
( I-MetricName
M) I-MetricName
in O
con- O
junction O
with O
ParaBART O
(Huang O
et O
al., O
2021) O
a O
model O
where O
BART O
(Lewis O
et O
al., O
2020) O
is O
fine- O
tuned O
using O
syntactic O
information O
to O
produce O
neural O
representations O
that O
represent O
maximally O
semantics O
and O
minimally O
syntax. O
We O
measure O
the O
cosine O
sim- O
ilarity O
between O
sentences O
according O
to O
ParaBART O
embeddings O
( O
PB).ii) O
Syntax: O
We O
use O
the O
script O
of O
(Chen O
et O
al., O
2020) O
to O
produce O
a O
syntactic B-MetricName
tree I-MetricName
edit I-MetricName
distance I-MetricName
(STED) I-MetricName
between O
the O
constituency O
trees O
of O
sentences, O
as O
was O
done O
to O
assess O
VGV B-MethodName
AE. I-MethodName
Ad- O
ditionally, O
following O
the O
evaluation O
procedure O
de- O
signed O
by O
Huang O
and O
Chang O
-2021 O
for O
SynPG, B-MethodName
we O
measure O
the O
Template B-MetricName
Matching I-MetricName
Accuracy I-MetricName
between O
sentences, O
where O
the O
template O
is O
the O
constituency O
tree O
truncated O
at O
the O
second O
level O
(TMA2). B-MetricName
TMA2 B-MetricName
is O
the O
percentage O
of O
sentence O
pairs O
where O
such O
tem- O
plates O
match O
exactly. O
We O
extend O
this O
measure O
by O
also O
providing O
it O
at O
the O
third O
level O
(TMA3). B-MetricName
Results O
are O
presented O
in O
Tables O
3 O
and O
4 O
In O
both O
Tables, O
the O
comparison O
scores O
between O
sentences O
and O
syn_src O
that O
are O
not O
significantly12different O
from O
the O
same O
12We O
consider O
differences O
to O
be O
significant O
if O
their O
associ- O
atedt-test O
yields O
a O
p-value<0.01.5769scores O
produced O
with O
regard O
to O
sem_src O
are O
marked O
with†. O
Sanity O
checks O
with O
metrics O
and O
baselines O
We O
notice O
in O
Table O
4 O
that O
using O
Meteor O
as O
a O
semantic O
similarity O
measure O
results O
in O
various O
inconsisten- O
cies. O
For O
instance, O
paraphrases O
target O
have O
a O
higher O
Meteor B-MetricName
score I-MetricName
with O
the O
syntactic O
sources O
than O
with O
interpolations O
from O
Optimus O
. O
It O
can O
also O
be O
seen O
that O
the O
Meteor B-MetricName
score I-MetricName
between O
outputs O
from O
VG- B-MethodName
V I-MethodName
AE I-MethodName
and O
both O
syntactic O
and O
semantic O
sources O
are O
rather O
close13. O
In O
contrast, O
ParaBART B-MetricName
score I-MetricName
be- O
haves O
as O
expected O
across O
comparisons O
in O
Table O
4 O
Consequently, O
we O
retain O
ParaBART B-MetricName
score O
as O
a O
se- O
mantic O
similarity O
measure. O
In O
the O
following, O
we O
use O
the O
scores O
between O
sem_src O
,syn_src O
, O
and O
tar- O
get(first O
two O
rows O
in O
Tables O
4 O
and O
3) O
as O
reference O
scores O
for O
unrelated O
sentences, O
paraphrase O
pairs, O
and O
syntactically O
similar O
sentences. O
Comparing O
the O
supervised O
baselines O
VGV B-MethodName
AE I-MethodName
and O
SynPG B-MethodName
greatly O
differ O
in O
scores. O
It O
can O
be O
seen O
that O
SynPG B-MethodName
copies O
a O
lot O
of O
lexical O
items O
from O
its O
se- O
mantic O
input O
(high O
Meteor B-MetricName
score) I-MetricName
which O
allows O
for O
higher O
semantic O
similarity O
scores. O
However, O
Table O
3 O
shows O
that O
SynPG B-MethodName
transfers O
syntax O
from O
syn_src O
at O
a O
high O
level O
(high O
TMA2, O
but O
low O
TMA3). O
In O
contrast, O
VGV B-MetricName
AE I-MetricName
transfers O
syntax O
and O
semantics O
in O
a O
balanced O
way O
and O
achieves O
the O
best O
syntax O
trans- O
fer O
scores O
overall O
(lowest O
STED B-MetricName
with O
syn_src O
and O
target O
). O
Analysing O
the O
scores O
of O
QKVAE B-MethodName
The O
seman- O
tic O
similarity O
scores O
PBof O
QKV B-MethodName
AE I-MethodName
outputs O
with O
target O
andsem_src O
are O
close O
to O
those O
of O
Optimus O
outputs. O
Although O
these O
scores O
are O
low O
compared O
to O
supervised O
models, O
they O
are O
notably O
higher O
than O
semantic B-MetricName
similarity I-MetricName
scores I-MetricName
between O
unrelated O
sen- O
tences O
( O
e.g. O
syn_src O
andsem_src O
). O
However, O
in O
contrast O
to O
Optimus, O
QKV B-MethodName
AE I-MethodName
outputs O
display O
low O
PB B-MetricName
scores O
with O
syn_src O
, O
which O
show O
that O
they O
draw O
very O
little O
semantic O
information O
from O
the O
syntactic O
sources. O
Concerning O
syntactic O
transfer O
in O
Table O
3, O
QKV B-MethodName
AE I-MethodName
outputs O
share O
syntactic O
information O
with O
syn_src O
on O
all O
levels O
(low O
STED, B-MetricName
and O
high O
TMA2 B-MetricName
and O
TMA3). B-MetricName
Our O
model O
is O
even O
competitive O
with O
SynPG B-MethodName
on O
TMA2, B-MetricName
and O
better O
on O
TMA3 B-MetricName
and O
STED. B-MetricName
As O
expected, O
the O
scores O
comparing O
QKV B-MethodName
AE I-MethodName
outputs O
tosem_src O
show O
that O
they O
share O
very O
little O
syntac- O
tic O
information. O
On O
the O
other O
hand, O
ADV B-MethodName
AE I-MethodName
shows O
poor O
transfer O
performance O
on O
syntax O
and O
semantics, O
13This O
was O
not O
observed O
by O
Chen O
et O
al. O
(2020), O
as O
they O
only O
compared O
outputs O
from O
VGV B-MethodName
AE I-MethodName
to O
the O
target O
paraphrases.with O
only O
slight O
differences O
between O
scores O
w.r.t O
syn_src O
and O
scores O
w.r.t O
sem_src O
. O
5.4 O
Comparing O
our O
Model O
to O
a O
Supervised O
Model O
with O
Less O
Data O
Since O
VGV B-MethodName
AE I-MethodName
displays O
balanced O
syntactic O
and O
se- O
mantic O
transfer O
capabilities, O
we O
use O
it O
for O
this O
ex- O
periment O
where O
we O
train O
it O
on O
subsets O
of O
sizes O
in O
{10K,25K,50K,100K}from O
its O
original O
train- O
ing O
data. O
Our O
goal O
is O
to O
find O
out O
how O
much O
labeled O
data O
is O
needed O
for O
VGV B-MethodName
AE I-MethodName
to O
outperform O
our O
unsu- O
pervised O
model O
on O
both O
transfer O
metrics. O
20 O
25 O
30 O
35 O
40 O
PB O
between O
outputs O
and O
sem_src67891011STED B-MetricName
between O
outputs O
and O
syn_src10K O
25K O
50K O
100KVGVAE B-MethodName
QKVAE B-MethodName
Figure O
01:00 O
Plotting O
STED B-MetricName
w.r.t O
syn_ref O
and O
the O
PB B-MetricName
co- I-MetricName
sine I-MetricName
similarity I-MetricName
w.r.t O
sem_ref O
for O
VGV B-MethodName
AE I-MethodName
with O
different O
amounts O
of O
labeled O
data O
and O
for O
QKV B-MethodName
AE. I-MethodName
Points O
are O
scaled O
proportionally O
to O
the O
amount O
of O
training O
data. O
The O
vertical O
and O
horizontal O
diameters O
of O
each O
ellipse O
are O
equal O
to O
the O
standard O
deviation O
of O
the O
associated O
data O
points O
and O
axes. O
In O
Figure O
1, O
we O
plot O
for O
QKV B-MethodName
AE I-MethodName
and O
instances O
of O
VGV B-MethodName
AE I-MethodName
the O
STED B-MetricName
of O
their O
outputs O
w.r.t O
syn_src O
and O
thePBof B-MetricName
these O
outputs O
w.r.t O
sem_src O
. O
All O
values O
are O
averages O
over O
5 O
runs, O
with O
standard O
deviations O
plotted O
as O
ellipses. O
Figure O
1 O
shows O
that O
to O
outper- O
form O
QKV B-MethodName
AE I-MethodName
on O
syntactic O
and O
semantic O
transfer, O
VGV B-MethodName
AE I-MethodName
needs O
more O
than O
50K O
labeled O
samples. O
6 O
Discussion O
and O
conclusion O
In O
Table O
5, O
we O
display O
example O
outputs O
of O
SynPG, B-MethodName
VGV B-MethodName
AE, I-MethodName
and O
QKV B-MethodName
AE I-MethodName
along O
with O
their O
syntactic O
sources, O
semantic O
sources, O
and O
targets. O
We O
gen- O
erally O
observed O
that O
the O
outputs O
of O
QKV B-MethodName
AE I-MethodName
range O
from O
paraphrases O
(line O
6) O
to O
broadly O
related O
sen- O
tences O
(line O
3). O
As O
was O
shown O
by O
our O
quantitative O
results, O
outputs O
from O
V O
AE-based O
models O
(VGV B-MethodName
AE I-MethodName
and O
QKV B-MethodName
AE) I-MethodName
share O
relatively O
few O
lexical O
items O
with O
the O
semantic O
input. O
This O
can O
be O
seen O
in O
the O
qual- O
itative O
examples O
where O
they O
often O
swap O
words O
in O
the O
semantic O
source O
with O
closely O
related O
words O
( O
e.g. O
armored O
to O
military O
in O
line O
1, O
or O
lunch O
to O
snacks O
in O
line O
2). O
We O
attribute O
this O
quality O
to O
the O
smoothness O
of O
the O
latent O
space O
of O
V O
AEs O
which O
places O
coherent O
alternative O
lexical O
choices O
in O
the O
same O
vicinity. O
The O
examples O
above O
also O
show O
that O
our O
model O
is O
capable O
of O
capturing O
and O
transferring O
various O
syntactic O
characteristics O
such O
as O
the O
passive O
form O
(line O
1), O
the O
presence O
of O
subject-verb O
inversion O
(lines O
3, O
4, O
and O
7), O
or O
inter- O
jections O
(lines O
4 O
and O
6). O
We O
presented O
QKV B-MethodName
AE, I-MethodName
an O
unsupervised O
model O
which O
disentangles O
syntax O
from O
semantics O
without O
syntactic O
or O
semantic O
information. O
Our O
experiments O
show O
that O
its O
latent O
variables O
effectively O
position O
sentences O
in O
the O
latent O
space O
according O
to O
these O
attributes. O
Additionally, O
we O
show O
that O
QKV B-MethodName
AE I-MethodName
displays O
clear O
signs O
of O
disentanglement O
in O
trans- O
fer O
experiments. O
Although O
the O
semantic O
transfer O
is O
moderate, O
syntactic O
transfer O
with O
QKV B-MethodName
AE I-MethodName
is O
com- O
petitive O
with O
SynPG, B-MethodName
one O
of O
its O
supervised O
counter- O
parts. O
Finally, O
we O
show O
that O
VGV B-MethodName
AE, I-MethodName
a O
supervised O
model, O
needs O
more O
than O
50K O
samples O
to O
outperform O
QKV B-MethodName
AE I-MethodName
on O
both O
syntactic O
and O
semantic O
transfer. O
We O
plan O
to O
extend O
this O
work O
in O
three O
directions: O
i) O
Finding O
ways O
to O
bias O
representations O
of O
each O
zsem O
l O
towards O
understandable O
"concepts;" O
ii)Applying O
QK- B-MethodName
V I-MethodName
AE I-MethodName
to O
non-textual O
data O
since O
it O
is O
data O
agnostic O
(e.g.to O
rearrange O
elements O
of O
a O
visual O
"landscape.);" O
iii)Investigating O
the O
behavior O
of O
QKV B-MethodName
AE I-MethodName
on O
other O
languages.Acknowledgments O
This O
work O
is O
supported O
by O
the O
PARSITI O
project O
grant O
(ANR-16-CE33-0021) O
given O
by O
the O
French O
National O
Research O
Agency O
(ANR), O
the O
Laboratoire O
d’excellence O
“Empirical O
Foundations O
of O
Linguis- O
tics” O
(ANR-10-LABX-0083), O
as O
well O
as O
the O
ON- O
TORULE O
project. O
It O
was O
also O
granted O
access O
to O
the O
HPC O
resources O
of O
IDRIS O
under O
the O
allocation O
20XX-AD011012112 O
made O
by O
GENCI. O
Results O
on O
the O
development O
set O
We O
hereby O
display O
the O
scores O
on O
the O
development O
set. O
The O
encoder O
scores O
concerning O
the O
specializa- O
tion O
of O
latent O
variables O
are O
in O
Table O
6, O
while O
the O
transfer O
scores O
are O
in O
Table O
7 O
for O
semantics, O
and O
Table O
8 O
for O
syntax. O
The O
values O
on O
the O
development O
set O
concerning O
the O
comparison O
of O
QKV O
AE O
with O
VGV O
AE O
trained O
on O
various O
amounts O
of O
data O
is O
in O
Figure O
2 O
B O
Hyper-parameters O
Hyper-parameter O
values O
Theβweight B-MetricName
on O
the O
KLdivergence O
is O
set O
to O
0.6 B-MetricValue
for O
zcand O
to O
0.3 B-MetricValue
for O
zs, O
and O
the O
λthreshold B-MetricName
for O
the O
Free-Bits O
strategy O
is O
set O
to O
0.05. O
KLannealing O
is O
performed O
between O
steps O
3K O
and O
6K O
for O
zsem, O
and O
between O
steps O
7K O
and O
20K O
for O
zsyn. O
The O
model O
is O
trained O
using O
Adafac- O
tor O
(Shazeer O
and O
Stern, O
2018), O
a O
memory-efficient O
version O
of O
Adam O
(Kingma O
and O
Ba, O
2015). O
Using O
a O
batch O
size O
of O
64, O
we O
train O
for O
40 O
epochs, O
which O
takes O
about O
30 O
hours O
on O
a O
single O
Nvidia O
GEForce O
RTX O
2080 O
GPU. O
We O
use O
4 O
layers O
for O
both O
Trans- O
former O
encoders O
and O
decoders. O
The O
encoders O
(resp. O
decoders) O
are O
initialized O
with O
parameters O
from O
the O
4 O
first O
layers O
(resp. O
4 O
last O
layers) O
of O
BART O
encoders O
(resp. O
decoders). O
In O
total, O
our O
model O
uses O
236M O
parameters. O
Manual O
Hyper-parameter O
search O
Given O
that O
the O
architecture O
for O
Transformer O
layers O
is O
fixed O
by O
BART, O
we O
mainly O
explored O
3 O
parameters: O
number B-HyperparameterName
of I-HyperparameterName
latent I-HyperparameterName
variables I-HyperparameterName
L, I-HyperparameterName
number B-HyperparameterName
of I-HyperparameterName
Transformer I-HyperparameterName
lay- I-HyperparameterName
ers, I-HyperparameterName
values O
for O
β. B-HyperparameterName
Our O
first O
experiments O
have O
shown O
that O
setting O
Lto O
8 O
or O
16 O
does O
not O
yield O
good O
results, O
which O
is O
probably O
due O
to O
the O
fact O
that O
a O
high O
Lraises O
the O
search O
space O
for O
possible O
arrangements O
of O
values O
with O
keys, O
and O
consequently O
makes O
con- O
vergence O
harder. O
Concerning O
the O
number O
of O
layers, O
we O
observed O
that O
results O
with O
the O
full O
BART O
model O
(6 O
layers) O
have O
high O
variance O
over O
different O
runs. O
Reducing O
the O
number O
of O
layers O
to O
4 O
solved O
this O
is- O
sue. O
In O
regards O
to O
β, B-HyperparameterName
we O
observed O
that O
it O
must O
be O
0.6or B-HyperparameterValue
less O
for O
the O
model O
to O
produce O
adequate O
recon- O
structions O
and O
that O
it O
is O
beneficial O
to O
set O
it O
slightly O
lower O
for O
zsynthan O
for O
zsemso O
as O
to O
absorb O
more O
syntactic O
information O
with O
zsyn.5776 O
