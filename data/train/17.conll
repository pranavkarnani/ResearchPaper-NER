-DOCSTART- O
Controlling  O
Translation  O
Formality  O
Using  O
Pre-trained  O
Multilingual  O
Language  O
Models  O
Elijah  O
Rippeth⇤and  O
Sweta  O
Agrawal*and  O
Marine  O
Carpuat  O
Department  O
of  O
Computer  O
Science  O
University  O
of  O
Maryland  O
{erip,sweagraw,marine}@cs.umd.edu  O
Abstract  O
This  O
paper  O
describes  O
the  O
University  O
of  O
Mary-  O
land’s  O
submission  O
to  O
the  O
Special  O
Task  O
on  O
For-  O
mality  O
Control  O
for  O
Spoken  B-TaskName
Language  I-TaskName
Transla-  I-TaskName
tion  I-TaskName
at  O
IWSLT  O
,  O
which  O
evaluates  O
translation  O
from  O
English  O
into  O
6  O
languages  O
with  O
diverse  O
grammatical  O
formality  O
markers.  O
We  O
investigate  O
to  O
what  O
extent  O
this  O
problem  O
can  O
be  O
addressed  O
with  O
a  O
single  O
multilingual  O
model  O
,  O
simultane-  O
ously  O
controlling  O
its  O
output  O
for  O
target  O
language  O
and  O
formality.  O
Results  O
show  O
that  O
this  O
strategy  O
can  O
approach  O
the  O
translation  O
quality  O
and  O
formal-  O
ity  O
control  O
achieved  O
by  O
dedicated  O
translation  O
models.  O
However,  O
the  O
nature  O
of  O
the  O
underlying  O
pre-trained  O
language  O
model  O
and  O
of  O
the  O
ﬁnetun-  O
ing  O
samples  O
greatly  O
impact  O
results.  O
1  O
Introduction  O
While  O
machine  O
translation  O
(  O
MT)  O
research  O
has  O
pri-  O
marily  O
focused  O
on  O
preserving  O
meaning  O
across  O
lan-  O
guages,  O
linguists  O
and  O
lay-users  O
alike  O
have  O
long  O
known  O
that  O
pragmatic-preserving  O
communication  O
is  O
an  O
important  O
aspect  O
of  O
the  O
problem  O
(  O
Hovy  O
,1987  O
).  O
To  O
address  O
one  O
dimension  O
of  O
this,  O
several  O
works  O
have  O
attempted  O
to  O
control  O
aspects  O
of  O
formality  O
in  O
MT  O
(Sennrich  O
et  O
al.  O
,2016  O
;Feely  O
et  O
al.  O
,2019  O
;  O
Schioppa  O
et  O
al.  O
,2021  O
).  O
Indeed,  O
this  O
research  O
area  O
was  O
formalized  O
as  O
formality-sensitive  O
machine  O
translation  O
(  O
FSMT  O
)  O
by  O
Niu  O
et  O
al.  O
(2017  O
),  O
where  O
the  O
translation  O
is  O
not  O
only  O
a  O
function  O
of  O
the  O
source  O
segment  O
but  O
also  O
the  O
desired  O
target  O
formality.  O
The  O
lack  O
of  O
gold  O
translation  O
with  O
alternate  O
formality  O
for  O
supervised  O
training  O
and  O
evaluation  O
has  O
lead  O
re-  O
searchers  O
to  O
rely  O
on  O
manual  O
evaluation  O
and  O
syn-  O
thetic  O
supervision  O
in  O
past  O
work  O
(  O
Niu  O
and  O
Carpuat  O
,  O
2020  O
).  O
Additionally,  O
these  O
works  O
broadly  O
adapt  O
to  O
formal  O
and  O
informal  O
registers  O
as  O
opposed  O
to  O
speciﬁ-  O
cally  O
controlling  O
grammatical  O
formality.  O
The  O
Special  O
Task  O
on  O
Formality  O
Control  O
on  O
Spo-  O
ken  O
Language  O
Translation  O
provides  O
a  O
new  O
bench-  O
mark  O
by  O
contributing  O
high-quality  O
training  O
datasets  O
⇤equal  O
Contrastive  O
formal  O
and  O
informal  O
translations  O
into  O
German.  O
Grammatical  O
formality  O
markers  O
are  O
bolded  O
and  O
aligned  O
via  O
indices.  O
for  O
diverse  O
languages  O
(  O
N˘adejde  O
et  O
al.  O
,2022  O
).  O
In  O
this  O
task,  O
a  O
source  O
segment  O
in  O
English  O
is  O
paired  O
with  O
two  O
references  O
which  O
are  O
minimally  O
contrastive  O
in  O
gram-  O
matical  O
formality,  O
one  O
for  O
each  O
formality  O
level  O
Training  O
and  O
test  O
sam-  O
ples  O
are  O
provided  O
in  O
the  O
domains  O
of  O
“telephony  O
data”  O
and  O
“topical  O
chat”  O
(  O
Gopalakrishnan  O
et  O
al.  O
,2019  O
)  O
for  O
four  O
language  O
pairs  O
(English-{German  O
(  O
DE),  O
Span-  O
ish  O
(ES),  O
Hindi  O
(  O
HI),  O
Japanese(  O
JA)})  O
and  O
a  O
test  O
dataset  O
for  O
two  O
additional  O
“zero-shot”  O
(ZS)  O
lan-  O
guage  O
pairs  O
(  O
EN-{Russian  O
(  O
RU),  O
Italian  O
(  O
IT)}).  O
Markers  O
of  O
grammatical  O
formality  O
vary  O
across  O
these  O
languages.  O
Personal  O
pronouns  O
and  O
verb  O
agreement  O
mark  O
formality  O
in  O
many  O
Indo-European  O
languages  O
(e.g.,  O
DE,HI,IT,RU,ES),  O
while  O
in  O
JA,  O
Korean  O
(KO)  O
and  O
other  O
languages,  O
distinctions  O
can  O
be  O
more  O
extensive  O
(e.g.,  O
using  O
morphological  O
markers)  O
to  O
express  O
polite,  O
respectful,  O
and  O
humble  O
speech.  O
In  O
this  O
work,  O
we  O
investigate  O
how  O
to  O
control  O
gram-  O
matical  O
formality  O
in  O
MTfor  O
many  O
languages  O
with  O
minimal  O
resources.  O
Speciﬁcally,  O
we  O
ask  O
whether  O
a  O
single  O
multilingual  O
model  O
can  O
be  O
ﬁnetuned  O
to  O
trans-  O
late  O
in  O
the  O
appropriate  O
formality  O
for  O
any  O
of  O
the  O
task  O
languages.  O
We  O
introduce  O
additive  O
vector  O
interven-  O
tions  O
to  O
encode  O
style  O
on  O
top  O
of  O
m  O
T5-large  O
(  O
Xue  O
et  O
al.  O
,2021  O
)  O
and  O
m  O
BART  O
-large  O
(  O
Liu  O
et  O
al.  O
,2020  O
),  O
and  O
investigate  O
the  O
impact  O
of  O
ﬁnetuning  O
on  O
varying  O
types  O
of  O
gold  O
and  O
synthetic  O
samples  O
to  O
minimize  O
reliance  O
on  O
manual  O
annotation.3272  O
Method  O
Given  O
an  O
input  O
sequence  O
x,  O
we  O
design  O
a  O
single  O
model  O
that  O
produces  O
an  O
output  O
y⇤=  O
arg  O
max  O
p(y|x,  O
l,  O
f  O
;✓LM,✓F)  O
for  O
any  O
language  O
land  O
formality  O
level  O
fconsidered  O
in  O
this  O
task.  O
The  O
bulk  O
of  O
its  O
parameters  O
✓LMare  O
initialized  O
with  O
a  O
pre-trained  O
multilingual  O
language  O
model.  O
A  O
small  O
number  O
of  O
additional  O
parameters  O
✓Fenable  O
formality  O
control.  O
All  O
parameters  O
are  O
ﬁnetuned  O
for  O
formality-controlled  O
translation.  O
2.1  O
Multilingual  O
Language  O
Models  O
We  O
experiment  O
with  O
two  O
underlying  O
multilingual  O
models:  O
1)  O
mT5-large1—  B-MethodName
a  O
multilingual  O
variant  O
of  O
T5that  O
is  O
pre-trained  O
on  O
the  O
Common  B-DatasetName
Crawl-based  I-DatasetName
dataset  O
covering  O
101languages  O
and  O
2)  O
mBART  B-MethodName
-  I-MethodName
large2—  I-MethodName
a  O
Transformer  O
encoder-decoder  O
which  O
supports  O
multilingual  O
machine  O
translation  O
for  O
50  O
languages.  O
While  O
m  O
BART  B-MethodName
-large  I-MethodName
is  O
pre-trained  O
with  O
parallel  O
and  O
monolingual  O
supervision,  O
m  O
T5-  O
large  O
uses  O
only  O
monolingual  O
dataset  O
during  O
the  O
pre-training  O
phase.  O
Following  O
standard  O
practice,  O
mT5controls  O
the  O
output  O
language,  O
l,  O
via  O
prompts  O
(“Translate  O
to  O
German”),  O
and  O
m  O
BART  O
replaces  O
the  O
beginning  O
of  O
sequence  O
token  O
in  O
the  O
decoder  O
with  O
target  O
language  O
tags  O
(  O
<2xx>).  O
2.2  O
Additive  B-TaskName
Formality  I-TaskName
Control  I-TaskName
While  O
large-scale  O
pre-trained  O
language  O
models  O
have  O
shown  O
tremendous  O
success  O
in  O
multiple  O
mono-  O
lingual  O
and  O
multilingual  O
controlled  O
generation  O
(Zhang  O
et  O
al.  O
,2022  O
)  O
and  O
style  O
transfer  O
tasks,  O
their  O
application  O
to  O
controlled  O
cross-lingual  O
text  O
gener-  O
ation  O
have  O
been  O
limited.  O
Few-shot  O
style-transfer  O
approaches  O
(  O
Garcia  O
et  O
al.  O
,2021  O
;Riley  O
et  O
al.  O
,2021  O
;  O
Krishna  O
et  O
al.  O
,2022  O
)  O
hold  O
the  O
promise  O
of  O
minimal  O
supervision  O
but  O
perform  O
poorly  O
on  O
low-resource  O
settings  O
and  O
their  O
outputs  O
lack  O
diversity.  O
A  O
popular  O
way  O
of  O
introducing  O
control  O
when  O
generating  O
text  O
with  O
a  O
particular  O
style  O
attribute  O
istagging  O
,  O
where  O
the  O
desired  O
control  O
tags  O
(e.g.,  O
<2formal  O
>)  O
are  O
appended  O
to  O
the  O
source  O
or  O
the  O
tar-  O
get  O
sequence.  O
However,  O
as  O
discussed  O
in  O
Schioppa  O
et  O
al.  O
(2021  O
),  O
this  O
approach  O
has  O
several  O
limitations,  O
including  O
but  O
not  O
limited  O
to  O
the  O
necessity  O
of  O
includ-  O
ing  O
the  O
control  O
tokens  O
in  O
the  O
vocabulary  O
at  O
the  O
start  O
124  O
layers  O
with  O
1024  O
sized  O
embeddings,  O
2816  O
FFN  O
embed-  O
ding  O
dimension,  O
and  O
16  O
heads  O
for  O
both  O
encoder  O
and  O
decoder.  O
212  O
layers  O
with  O
1024  O
sized  O
embeddings,  O
4096  O
FFN  O
embed-  O
ding  O
dimension,  O
and  O
16  O
heads  O
for  O
both  O
encoder  O
and  O
decoder.  O
Figure  O
1:  O
Controlling  O
the  O
output  O
formality  O
of  O
a  O
multilin-  O
gual  O
language  O
model  O
with  O
additive  O
interventions.  O
of  O
the  O
training,  O
which  O
restricts  O
the  O
enhancement  O
of  O
pre-trained  O
models  O
with  O
controllability.  O
We  O
introduce  O
formality  O
control  O
by  O
adapting  O
the  O
vector-valued  O
interventions  O
proposed  O
by  O
Schioppa  O
et  O
al.  O
(2021  O
)  O
for  O
machine  O
translation  O
(  O
MT),  O
as  O
il-  O
lustrated  O
in  O
Figure  O
1.  O
Formally,  O
given  O
source  O
text  O
x,  O
a  O
formality  O
level  O
f,  O
an  O
encoder  O
Eand  O
decoder  O
D,  O
parameterized  O
by  O
✓LM,  O
and  O
a  O
style  O
embedding  O
layer  O
(  O
Emb)  O
parameterized  O
by  O
✓Fwith  O
the  O
same  O
output  O
dimension  O
as  O
E,  O
we  O
have  O
Z=E(x),V  O
=Emb(f)  O
y=D(Z+V)  O
Our  O
formality  O
levels  O
can  O
take  O
values  O
corresponding  O
to  O
formal,  O
informal,  O
and  O
“neutral”  O
translations,  O
the  O
last  O
of  O
which  O
is  O
used  O
to  O
generate  O
“generic”  O
transla-  O
tions  O
in  O
which  O
there  O
is  O
no  O
difference  O
in  O
the  O
gram-  O
matical  O
formality  O
of  O
the  O
translation  O
of  O
the  O
source  O
if  O
translated  O
formally  O
or  O
informally.  O
Unlike  O
Schioppa  O
et  O
al.  O
(2021  O
)  O
who  O
use  O
a  O
zero-vector  O
as  O
their  O
neutral  O
vector,  O
we  O
learn  O
a  O
separate  O
vector.  O
2.3  O
Finetuning  O
Finetuning  O
each  O
multilingual  O
model  O
requires  O
triplets  O
of  O
the  O
form  O
(x,  O
y,  O
f  O
)for  O
each  O
available  O
tar-  O
get  O
language,  O
l,  O
where  O
x,yandfare  O
the  O
source  O
text,  O
the  O
reference  O
translation  O
and  O
the  O
formality  O
label  O
cor-  O
responding  O
to  O
the  O
reference  O
translation  O
respectively.  O
The  O
loss  O
function  O
is  O
then  O
given  O
by:  O
L=X  O
(x,y,l,f  O
)logp(y|x,  O
l,  O
f  O
;✓LM,✓F)(1)  O
Given  O
paired  O
contrastive  O
training  O
samples  O
of  O
the  O
form  O
(X,  O
Y  O
f,Yif),  O
as  O
provided  O
by  O
the  O
shared  O
task,  O
the  O
loss  O
decomposes  O
into  O
balanced  O
formal  O
and  O
in-  O
formal  O
components,  O
but  O
does  O
not  O
explicitly  O
exploit328Language  O
Shared  O
Task  O
Data  O
Statistics:  O
We  O
use  O
“13a”  O
tokenization  O
for  O
all  O
languages  O
except  O
Japanese  O
for  O
which  O
we  O
use  O
“ja-mecab”  O
implemented  O
in  O
the  O
sacrebleu  O
library.  O
the  O
fact  O
that  O
YiandYfalign  O
to  O
the  O
same  O
input:  O
L=X  O
(x,yf,l)logp(yf|x,  O
l,  O
f  O
;✓LM,✓F)+  O
X  O
(x,yif,l)logp(yif|x,  O
l,  O
if  O
;✓LM,✓F)(2)  O
2.4  O
Synthetic  O
Supervision  O
Since  O
paired  O
contrastive  O
samples  O
are  O
expensive  O
to  O
obtain,  O
we  O
explore  O
the  O
use  O
of  O
synthetic  O
training  O
sam-  O
ples  O
to  O
replace  O
or  O
complement  O
them.  O
This  O
can  O
be  O
done  O
either  O
by  O
automatically  O
annotating  O
naturally  O
occurring  O
bitext  O
for  O
formality,  O
which  O
yields  O
formal  O
and  O
informal  O
samples,  O
and  O
additionally  O
by  O
rewrit-  O
ing  O
the  O
translation  O
to  O
alter  O
its  O
formality  O
to  O
obtain  O
paired  O
contrastive  O
samples.  O
The  O
second  O
approach  O
was  O
used  O
by  O
Niu  O
and  O
Carpuat  O
(2020  O
)  O
to  O
control  O
the  O
register  O
of  O
MToutput.  O
However,  O
since  O
this  O
shared  O
task  O
targets  O
grammatical  O
formality  O
and  O
excludes  O
other  O
markers  O
of  O
formal  O
vs.  O
informal  O
registers,  O
we  O
focus  O
on  O
the  O
ﬁrst  O
approach,  O
thus  O
prioritizing  O
control  O
on  O
the  O
nature  O
of  O
the  O
formality  O
markers  O
in  O
the  O
out-  O
put  O
over  O
the  O
tighter  O
supervision  O
provided  O
by  O
paired  O
contrastive  O
samples.  O
Given  O
a  O
translation  O
example  O
(x,  O
y),  O
we  O
predict  O
a  O
silver-standard  O
formality  O
label  O
(  O
f)  O
for  O
the  O
target  O
y  O
using  O
two  O
distinct  O
approaches:  O
•Rules  O
(  O
ES,DE,IT,RU):  O
We  O
label  O
formality  O
using  O
heuristics  O
based  O
on  O
keyword  O
search,  O
de-  O
pendency  O
parses,  O
and  O
morphological  O
features.  O
We  O
use  O
spaCy  O
(  O
Honnibal  O
et  O
al.  O
,2020  O
)  O
to  O
(non-  O
exhaustively)  O
retrieve  O
documents  O
that  O
imply  O
a  O
necessarily  O
formal,  O
necessarily  O
informal,  O
or  O
am-  O
biguously  O
formal  O
label.  O
In  O
the  O
case  O
of  O
an  O
ambigu-  O
ously  O
formal  O
label,  O
we  O
treat  O
it  O
as  O
unambiguously  O
formal  O
(for  O
examples,  O
see  O
B).  O
The  O
complete  O
set  O
of  O
rules  O
for  O
each  O
of  O
the  O
languages  O
are  O
included  O
in  O
the  O
Appendix  O
12.  O
While  O
simple  O
to  O
im-  O
plement,  O
these  O
heuristics  O
privilege  O
precision  B-MetricName
over  O
recall,  B-MetricName
and  O
risk  O
biasing  O
the  O
synthetic  O
data  O
to  O
the  O
few  O
grammatical  O
aspects  O
they  O
encode.•Classiﬁers  O
(  O
HI,JA,IT,RU):  O
We  O
train  O
a  O
binary  O
formal  O
vs.  O
informal  O
classiﬁer  O
on  O
the  O
shared  O
task  O
data  O
(  O
HI,JA)  O
and  O
on  O
the  O
synthetic  O
data  O
(  O
IT,  O
RU).  O
Unlike  O
rules,  O
they  O
can  O
also  O
be  O
transferred  O
in  O
a  O
zero-shot  O
fashion  O
to  O
new  O
languages,  O
and  O
might  O
be  O
less  O
biased  O
toward  O
precision  O
when  O
well-  O
calibrated.  O
3  O
Evaluation  O
Settings  O
Data  O
The  O
shared  O
task  O
provides  O
English  O
source  O
segments  O
paired  O
with  O
two  O
contrastive  O
reference  O
translations,  O
one  O
for  O
each  O
formality  O
level  O
(informal  O
and  O
formal)  O
for  O
four  O
language  O
pairs:  O
EN-{DE,ES,  O
JA,HI}  O
in  O
the  O
supervised  O
setting  O
and  O
two  O
language  O
pairs:  O
EN-{RU,IT}  O
in  O
the  O
zero-shot  O
setting.  O
The  O
sizes  O
and  O
properties  O
of  O
the  O
datasets  O
for  O
the  O
super-  O
vised  O
language  O
pairs  O
are  O
listed  O
in  O
Table  O
2.  O
Formal  O
texts  O
tend  O
to  O
be  O
longer  O
and  O
more  O
diverse  O
than  O
infor-  O
mal  O
texts  O
for  O
JAcompared  O
to  O
other  O
language  O
pairs.  O
The  O
percentage  O
of  O
neutral  O
samples  O
(same  O
formal  O
and  O
informal  O
outputs)  O
vary  O
from  O
2%(inJA)  O
to17%  O
(inHI).  O
In  O
the  O
zero-shot  O
setting,  O
600test  O
samples  O
are  O
released  O
for  O
the  O
two  O
language  O
pairs  O
(RU,  O
IT).  O
During  O
development,  O
the  O
last  O
50paired  O
con-  O
trastive  O
examples  O
from  O
each  O
domain  O
are  O
set  O
aside  O
as  O
a  O
validation  O
set  O
for  O
each  O
of  O
the  O
supervised  O
lan-  O
guages  O
(  O
TASK  O
DEV  O
)  O
and  O
use  O
the  O
remaining  O
samples  O
for  O
training  O
(T  O
ASK  O
TRAIN  O
).  O
Metrics  O
We  O
evaluate  O
the  O
translation  O
quality  O
of  O
the  O
detruecased  O
detokenized  O
outputs  O
from  O
each  O
systems  O
using  O
BLEU  B-MetricName
(Papineni  O
et  O
al.  O
,2002  O
)  O
and  O
COMET  B-MetricName
(Rei  O
et  O
al.  O
,2020  O
).  O
We  O
use  O
the  O
13Atokenizer  O
to  O
re-  O
port  O
SACRE  O
BLEU3scores  O
for  O
all  O
languages,  O
except  O
Japanese,  O
for  O
which  O
we  O
use  O
the  O
JA-MECAB  O
.W  O
e  O
also  O
report  O
the  O
ofﬁcial  O
formality  O
accuracy  O
(ACC.).  O
Given  O
a  O
set  O
of  O
hypotheses  O
H,  O
sets  O
of  O
corresponding  O
phrase-annotated  O
formal  O
references  O
Fand  O
informal  O
Data  O
sources  O
from  O
which  O
unlabeled  O
formality  O
parallel  O
examples  O
are  O
sampled  O
for  O
EN-X  O
for  O
training  O
the  O
Synthetic  O
Finetuned  O
and  O
the  O
Bilingual  O
baselines.  O
references  O
IF,  O
and  O
a  O
function  O
 yielding  O
phrase-  O
level  O
contrastive  O
terms  O
from  O
a  O
reference,  O
the  O
task-  O
speciﬁc  O
evaluation  O
metric  O
is  O
deﬁned  O
as  O
follows:  O
match  O
f=X  O
j  O
[ (Fj)2Hj^ (IFj)/2Hj]  O
match  O
i=X  O
j  O
[ (Fj)/2Hj^ (IFj)2Hj]  O
accj=match  O
j  O
match  O
f+match  O
i,j  O
2{f,i}  O
We  O
note  O
that  O
the  O
task  O
accuracy  O
is  O
a  O
func-  O
tion  O
of  O
the  O
number  O
of  O
matches  O
in  O
the  O
hypothe-  O
ses,  O
not  O
the  O
number  O
of  O
expected  O
phrases,  O
i.e.  O
match  O
f+match  O
ifkHkand  O
discuss  O
the  O
im-  O
plications  O
in  O
the  O
Appendix  O
(Section  O
C).  O
4  O
Experimental  O
Conditions  O
We  O
compare  O
multilingual  O
models,  O
where  O
a  O
single  O
model  O
is  O
used  O
to  O
generate  O
formal  O
and  O
informal  O
translations  O
for  O
all  O
languages  O
with  O
bilingual  O
models  O
trained  O
for  O
each  O
language  O
pair,  O
as  O
detailed  O
below.  O
4.1  O
Multilingual  O
Models  O
Data  O
We  O
consider  O
three  O
ﬁnetuning  O
settings:  O
•Gold  O
ﬁnetuned  O
:  O
the  O
model  O
is  O
ﬁnetuned  O
only  O
on  O
paired  O
contrastive  O
shared  O
task  O
data  O
(400  O
to  O
1000  O
samples  O
per  O
language  O
pair).  O
•Synthetic  O
ﬁnetuned  O
:  O
the  O
model  O
is  O
ﬁnetuned  O
on  O
synthetic  O
silver-labelled  O
triplets  O
(up  O
to  O
7500  O
sam-  O
ples  O
per  O
formality  O
level  O
and  O
language  O
as  O
described  O
below).  O
•Two-pass  O
ﬁnetuned  O
:  O
the  O
Synthetic  O
ﬁnetuned  O
model  O
is  O
further  O
ﬁnetuned  O
on  O
a  O
mixture  O
of  O
gold  O
data  O
and  O
1000  O
examples  O
re-sampled  O
from  O
the  O
syn-  O
thetic  O
training  O
set  O
for  O
unseen  O
languages,  O
which  O
we  O
use  O
to  O
avoid  O
catastrophic  O
forgetting  O
from  O
the  O
silver  O
ﬁnetuning  O
stage.Synthetic  O
samples  O
are  O
drawn  O
from  O
multiple  O
data  O
sources  O
(  O
3),  O
sampling  O
at  O
most  O
7500  O
examples  O
for  O
each  O
language  O
and  O
formality  O
level.4The  O
formality  O
labels  O
are  O
predicted  O
as  O
described  O
in  O
2.4.  O
Rule-based  O
predictors  O
directly  O
give  O
a  O
label.  O
With  O
classiﬁers,  O
we  O
assign  O
the  O
formal  O
label  O
if  O
P(formal  O
|y) 0.85and  O
informal  O
if  O
P(formal  O
|y)0.15.  O
We  O
additionally  O
compare  O
with  O
the  O
translations  O
generated  O
from  O
the  O
base  O
m  O
BART  O
-large  O
model  O
with  O
no  O
ﬁnetuning,  O
referred  O
to  O
as  O
the  O
“  O
formality  O
agnostic  B-MethodName
mBART  I-MethodName
-large  O
”.  O
Training  O
settings  O
We  O
ﬁnetune  B-MethodName
m  I-MethodName
T5-large  I-MethodName
and  B-MethodName
mBART  I-MethodName
-large  O
with  B-HyperparameterName
a  I-HyperparameterName
batch  I-HyperparameterName
size  B-HyperparameterValue
of  I-HyperparameterValue
2and8respec-  I-HyperparameterValue
tively  O
for  O
10and3epochs  O
respectively.  O
We  O
mask  O
the  O
formality  O
labels  O
used  O
to  O
generate  O
vector-valued  O
interventions  O
with  O
a  O
probability  O
of  O
0.2.  O
The  O
m  O
T5-  O
large  O
model  O
—  O
“  O
synthetic  O
ﬁnetuned  O
m  O
T5-large  O
”—  O
is  O
trained  O
for  O
an  O
additional  O
5epochs,  O
with  O
a  O
batch  O
size  O
of  O
2on  O
a  O
mixture  O
of  O
task  O
data  O
for  O
seen  O
lan-  O
guages  O
and  O
a  O
subset  O
of  O
the  O
sampled  O
synthetic  O
data  O
for  O
unseen  O
languages.  O
Again,  O
we  O
mask  O
the  O
formal-  O
ity  O
tag  O
with  O
probability  O
0.2except  O
in  O
the  O
case  O
of  O
un-  O
seen  O
languages  O
where  O
the  O
formality  O
tag  O
is  O
masked  O
with  O
probability  O
1.0,  O
resulting  O
in  O
the  O
“  O
two-pass  O
ﬁne-  O
tuned  O
m  O
T5-large  O
”  O
model.  O
Formality  O
Classiﬁers  O
Following  O
Briakou  O
et  O
al.  O
(2021  O
),  O
we  O
ﬁnetune  O
XLM-R  O
on  O
binary  O
classiﬁca-  O
tion  O
between  O
formal  O
and  O
informal  O
classes,  O
using  O
the  O
shared  O
task  O
datasets  O
for  O
each  O
of  O
the  O
supervised  O
language  O
pairs  O
(  O
DE,ES,JA,HI)  O
and  O
synthetic  O
datasets  O
for  O
zero-shot  O
language  O
pairs  O
(  O
RU,IT).  O
We  O
treat  O
the  O
“neutral”  O
samples  O
as  O
both  O
“formal”  O
and  O
“informal”  O
when  O
training  O
the  O
classiﬁers.  O
We  O
use  O
the  O
Adam  O
optimizer,  O
a  O
batch  O
size  O
of  O
32,  O
and  O
a  O
learning  O
rate  O
of  O
5⇥10 3to  O
ﬁnetune  O
for  O
3epochs.  O
We  O
report  O
4We  O
do  O
not  O
experiment  O
with  O
varying  O
the  O
sizes  O
of  O
the  O
syn-  O
thetic  O
dataset  O
due  O
to  O
the  O
time  O
constraints  O
and  O
leave  O
it  O
to  O
the  O
future  O
Results  O
on  O
the  O
TASK  O
DEV  O
split  O
when  O
training  O
Additive  O
m  O
T5-large  O
with  O
and  O
without  O
contrastive  O
examples:  O
Sample  O
diversity  O
from  O
Unpaired  O
triplets  O
improve  O
BLEU  O
and  O
Accuracy  O
over  O
paired  O
contrastive  O
samples.  O
Results  O
on  O
the  O
TASK  O
DEV  O
split:  O
TER  O
between  O
generated  O
formal  O
and  O
informal  O
sentences.  O
the  O
accuracy  O
of  O
the  O
learned  O
classiﬁers  O
trained  O
on  O
the  O
T  O
ASK  O
TRAIN  O
dataset  O
in  O
Appendix  O
Table  O
14.  O
4.2  O
Bilingual  O
Models  O
We  O
consider  O
two  O
types  O
of  O
bilingual  O
models:  O
1.Formality  O
Agnostic:  O
These  O
models  O
were  O
re-  O
leased  O
by  O
the  O
shared  O
task  O
organizers.  O
Each  O
model  O
is  O
bilingual  O
and  O
trained  O
on  O
a  O
sample  O
of  O
20  O
million  O
lines  O
from  O
the  O
Paracrawl  O
Corpus  O
(V9)  O
using  O
the  O
Sockeye  O
NMT  O
toolkit.  O
Models  O
use  O
big  O
transformers  O
with  O
20encoder  O
layers,  O
2de-  O
coder  O
layers,  O
SSRU’s  O
in  O
place  O
of  O
decoder  O
self-  O
attention,  O
and  O
large  O
batch  O
training.  O
2.Formality  O
Speciﬁc  O
(Gold):  O
We  O
ﬁnetune  O
the  O
models  O
in  O
[1]  O
to  O
generate  O
a  O
formal  O
model  O
and  O
an  O
informal  O
model  O
for  O
each  O
language  O
pair  O
(except  O
the  O
zero-shot  O
language  O
pairs).  O
The  O
effective  O
capacity  O
of  O
the  O
bilingual,  O
formality  O
speciﬁc  O
models  O
is  O
3.14B  O
parameters.Each  O
model  O
has314M  O
parameters,  O
resulting  O
in  O
(314  O
⇥2⇥4)  O
=  O
2.5B  O
parameters  O
for  O
the  O
four  O
supervised  O
languages  O
(DE,ES,HI,JA)  O
and  O
two  O
pre-trained  O
models  O
(314  O
⇥2)  O
=  O
628  O
M  O
parameters  O
for  O
the  O
unseen  O
lan-  O
guages  O
(  O
RU,IT).This  O
is  O
signiﬁcantly  O
larger  O
than  O
the  O
capacities  O
of  O
our  O
single  O
multilingual  O
models  O
(Additive  O
m  O
T5-large:  O
1.25B,  O
Additive  O
m  O
BART  O
-  O
large:  O
610M).  O
5  O
System  O
Development  O
Results  O
During  O
system  O
development,  O
we  O
explore  O
the  O
im-  O
pact  O
of  O
different  O
types  O
of  O
training  O
samples  O
and  O
ﬁne-  O
tuning  O
strategies  O
on  O
translation  O
quality  O
and  O
formal-  O
ity  O
accuracy  O
on  O
T  O
ASK  O
DEV  O
.Contrastive  O
Samples  O
We  O
estimate  O
the  O
beneﬁts  O
of  O
ﬁne-tuning  O
on  O
informal  O
vs.  O
formal  O
translations  O
of  O
the  O
same  O
inputs  O
for  O
this  O
task.  O
We  O
train  O
two  O
variants  O
of  O
the  O
gold  O
finetuned  O
mT5-large  O
model  O
using  O
50%  O
of  O
the  O
paired  O
contrastive  O
samples  O
and  O
100%  O
of  O
the  O
unpaired  O
triplets  O
(i.e.,  O
selecting  O
one  O
for-  O
mality  O
level  O
per  O
unique  O
source  O
sentence)  O
from  O
the  O
TASK  O
TRAIN  O
samples  O
Results  O
show  O
that  O
sample  O
diversity  O
resulting  O
from  O
unpaired  O
triplets  O
leads  O
to  O
better  O
translation  O
quality  O
as  O
measured  O
by  O
BLEU  O
(Average  O
Gain:  O
Formal  O
+3.2.  O
Informal  O
+5.38),  O
without  O
compromising  O
on  O
the  O
formality  O
accuracy.  O
Training  O
with  O
paired  O
samples  O
result  O
in  O
lower  O
TER  O
between  O
formal  O
and  O
informal  O
output  O
compared  O
to  O
unpaired  O
triplets  O
(Table  O
5),  O
suggesting  O
that  O
the  O
outputs  O
generated  O
by  O
the  O
model  O
trained  O
on  O
paired  O
samples  O
are  O
more  O
contrastive.  O
This  O
further  O
motivates  O
our  O
two-pass  O
finetuned  O
model  O
which  O
uses  O
gold  O
contrastive  O
samples  O
on  O
the  O
ﬁnal  O
stage  O
of  O
ﬁnetuning  O
to  O
bias  O
the  O
model  O
towards  O
gen-  O
erating  O
contrastive  O
MT  O
outputs.  O
While  O
TASK  O
DEV  O
is  O
too  O
small  O
to  O
make  O
deﬁnitive  O
claims,  O
we  O
report  O
our  O
system  O
development  O
results  O
in  O
Tables  O
6and7.  O
We  O
observe  O
that  O
ﬁnetuning  O
on  O
gold  O
contrastive  O
examples  O
(  O
gold-finetuned  O
)  O
improves  O
the  O
translation  O
quality  O
and  O
accuracy  O
of  O
the  O
translation  O
models  O
(  O
formality-agnostic  O
),  O
highlighting  O
the  O
importance  O
of  O
limited  O
but  O
high-  O
quality  O
in-domain  O
supervision  O
on  O
the  O
resulting  O
models.  O
Further,  O
each  O
of  O
the  O
mT5-large  O
mod-  O
els  O
improves  O
in  O
translation  O
quality  O
with  O
additional  O
data  O
and  O
training.  O
While  O
the  O
results  O
are  O
dramatic  O
due  O
to  O
size  O
of  O
both  O
TASK  O
TRAIN  O
andTASK  O
DEV  O
,  O
the  O
trends  O
validate  O
the  O
approach  O
to  O
augment  O
both  O
mBART  O
-large  O
and  O
the  O
m  O
T5-large  O
with  O
additive  O
interventions  O
to  O
control  O
formality.  O
6  O
Ofﬁcial  O
Results  O
Submissions  O
We  O
submit  O
ﬁve  O
variants  O
of  O
multi-  O
lingual  O
models  O
(numbered  O
[1-5]  O
in  O
Results  O
on  O
the  O
T  O
ASK  O
DEV  O
split  O
in  O
the  O
informal  O
supervised  O
setting.  O
A  O
CC.:informal  O
accuracy.  O
and  O
compare  O
them  O
to  O
the  O
bilingual  O
models  O
built  O
on  O
top  O
of  O
the  O
organizers’  O
baselines.  O
We  O
ﬁrst  O
discuss  O
results  O
on  O
the  O
ofﬁcial  O
test  O
split  O
for  O
the  O
supervised  O
setting  O
To  O
better  O
understand  O
the  O
de-  O
gree  O
of  O
overall  O
control  O
afforded,  O
we  O
also  O
report  O
the  O
average  O
scores  O
of  O
the  O
formal  O
and  O
informal  O
settings  O
in  O
turning  O
to  O
the  O
zero-shot  O
setting  O
in  O
Table  O
11.  O
Multilingual  O
Approach  O
The  O
best  O
multilingual  O
models  O
(  O
[1]  O
&[4])  O
consistently  O
outperform  O
the  O
bilingual  O
formality-agnostic  O
baselines,  O
improving  O
both  O
translation  O
quality  O
(Worst-case  O
gain  O
in  O
Average  O
BLEU  O
:  O
Formal  O
(+1.67),  O
Informal:  O
(  O
+3.7))  O
and  O
formality  O
accuracy  O
(Worst-case  O
gain  O
in  O
Average  O
ACC.:  O
Formal  O
(+40.38),  O
Informal:  O
(  O
+31.6)).  O
They  O
approach  O
the  O
quality  O
of  O
formal  O
and  O
informal  O
bilingual  O
systems,  O
but  O
the  O
gap  O
in  O
translation  O
quality  O
and  O
formality  O
accuracy  O
varies  O
across  O
languages.  O
While  O
for  O
DE  O
andES,  O
there  O
is  O
a  O
large  O
difference  O
in  O
translation  O
quality  O
(approx.  O
10  O
BLEU  O
points)  O
between  O
the  O
multilingual  O
models  O
and  O
the  O
bilingual  O
baselines,the  O
multilingual  O
models  O
consistently  O
get  O
higher  O
formality  O
accuracy  O
across  O
language  O
pairs  O
and  O
style  O
directions  O
and  O
also  O
perform  O
comparably  O
with  O
the  O
bilingual  O
models  O
in  O
matching  O
the  O
translation  O
quality  O
forHIand  O
JA.  O
We  O
attribute  O
these  O
differences  O
to  O
the  O
amount  O
of  O
training  O
data  O
used  O
across  O
the  O
language  O
pairs  O
(  O
HI:0.7M  O
to  O
DE20M).  O
This  O
is  O
an  O
encouraging  O
result,  O
since  O
the  O
bilingual  O
approach  O
uses  O
a  O
much  O
larger  O
language-speciﬁc  O
parameter  O
budget  O
and  O
bitext  O
for  O
training  O
than  O
the  O
all  O
purpose  O
multilingual  O
models,  O
which  O
can  O
beneﬁt  O
from  O
transfer  O
learning  O
across  O
languages.  O
mBART  O
vs.  O
m  O
T5  O
The  O
gold  O
finetuned  O
mBART-large  O
model  O
achieves  O
the  O
best  O
overall  O
translation  O
quality  O
among  O
the  O
multilingual  O
variants  O
as  O
expected  O
given  O
that  O
m  O
BART  O
-large  O
is  O
pre-trained  O
on  O
parallel  O
text.  O
Its  O
translation  O
quality  O
is  O
higher  O
than  O
that  O
of  O
m  O
T5-large  O
models  O
according  O
to  O
BLEU  O
andCOMET  O
for  O
all  O
languages  O
except  O
HI(infor-  O
mal),  O
which  O
could  O
be  O
attributed  O
to  O
the  O
nature  O
and  O
amount  O
of  O
pre-training  O
data  O
used  O
for  O
HI.  O
Its  O
formal-  O
ity  O
accuracy  O
is  O
in  O
the  O
90’s  O
and  O
within  O
8:  O
Results  O
on  O
the  O
ofﬁcial  O
test  O
split  O
in  O
the  O
formal  O
supervised  O
setting.  O
Best  O
scores  O
from  O
multilingual  O
and  O
bilingual  O
systems  O
are  O
bolded  O
.  O
Our  O
ofﬁcial  O
submissions  O
to  O
the  O
shared  O
task  O
are  O
numbered  O
9:  O
Results  O
on  O
the  O
ofﬁcial  O
test  O
split  O
in  O
the  O
informal  O
supervised  O
setting.  O
Best  O
scores  O
from  O
multilingual  O
and  O
bilingual  O
systems  O
are  O
bolded  O
.  O
Our  O
ofﬁcial  O
submissions  O
to  O
the  O
shared  O
task  O
are  O
numbered  O
[1-4]  O
.  O
points  O
to  O
the  O
highest  O
score  O
for  O
all  O
languages  O
except  O
Japanese  O
(  O
78.2%)  O
in  O
the  O
formal  O
direction.  O
In  O
the  O
informal  O
direction,  O
the  O
gap  O
between  O
m  O
BART  O
-large  O
and  O
the  O
best  O
system  O
on  O
formality  O
accuracy  O
is  O
larger  O
across  O
the  O
board  O
(Average  O
Acc.:  O
+19.3),  O
suggest-  O
ing  O
that  O
ﬁnetuning  O
on  O
gold  O
data  O
cannot  O
completely  O
recover  O
an  O
informal  O
translation  O
despite  O
generally  O
strong  O
performance  O
in  O
formal  O
translations.  O
Finetuning  O
strategies  O
Results  O
show  O
that  O
the  O
com-  O
bination  O
of  O
synthetic  O
and  O
gold  O
data  O
is  O
crucial  O
to  O
help  O
the  O
m  O
T5-large-based  O
model  O
learn  O
to  O
trans-  O
late  O
and  O
mark  O
formality  O
appropriately.  O
Finetun-  O
ing  O
only  O
on  O
the  O
gold  O
data  O
leads  O
to  O
overﬁtting:  O
the  O
model  O
achieves  O
high  O
formality  O
accuracy  O
scores,  O
but  O
poor  O
translation  O
quality  O
(  O
BLEU  O
<10).  O
Manual  O
inspection  O
of  O
m  O
T5-large-based  O
system  O
outputs  O
sug-  O
gests  O
that  O
translations  O
often  O
include  O
tokens  O
in  O
the  O
wrong  O
language  O
Finetun-  O
ing  O
on  O
synthetic  O
data  O
improves  O
translation  O
qual-ity  O
substantially  O
compared  O
to  O
gold  O
data  O
only  O
(Av-  O
erage  O
gain  O
in  O
BLEU  O
:  O
Formal  O
(  O
+15.8),  O
Informal  O
(+14.6)).  O
Two-pass  O
ﬁnetuning  O
improves  O
formality  O
control  O
(Average  O
gain  O
in  O
ACC.:  O
Formal  O
(  O
+5.43),  O
In-  O
formal  O
(  O
+27.85)),  O
with  O
additional  O
translation  O
qual-  O
ity  O
improvement  O
across  O
the  O
board  O
over  O
synthetic-  O
ﬁnetuned  O
model  O
(Average  O
gain  O
in  O
BLEU  O
:  O
Formal  O
(+10.27),  O
Informal  O
(  O
+11.03);COMET  O
:  O
Formal  O
(+0.247),  O
Informal  O
(  O
+0.252)).  O
While  O
we  O
primarily  O
focused  O
on  O
the  O
impact  O
of  O
synthetic  O
supervision  O
on  O
mT5-large,  O
we  O
believe  O
a  O
similar  O
investigation  O
using  O
mBART  O
-large  O
would  O
yield  O
interesting  O
results  O
and  O
leave  O
this  O
as  O
future  O
work.  O
Performance  O
across  O
languages  O
While  O
the  O
higher  O
resource  O
language  O
pairs  O
(  O
DE,ES)  O
achieve  O
better  O
translation  O
quality  O
(in  O
BLEU  O
andCOMET  O
)  O
over  O
the  O
relatively  O
lower  O
resource  O
languages  O
(  O
HI,JA),  O
the  O
formality  O
accuracy  O
is  O
more  O
comparable  O
across  O
the  O
language  O
pairs  O
for  O
the  O
multilingual  O
10:  O
Averaged  O
formal  O
and  O
informal  O
results  O
on  O
the  O
ofﬁcial  O
test  O
split  O
in  O
the  O
supervised  O
setting.  O
Best  O
scores  O
from  O
multilingual  O
and  O
bilingual  O
systems  O
are  O
bolded  O
.  O
Our  O
ofﬁcial  O
submissions  O
to  O
the  O
shared  O
task  O
are  O
numbered  O
[1-4]  O
.  O
MODELTo  O
Formal  O
To  O
Informal  O
Results  O
on  O
the  O
ofﬁcial  O
test  O
split  O
for  O
the  O
zero-shot  O
setting.  O
Our  O
ofﬁcial  O
submissions  O
to  O
the  O
shared  O
task  O
are  O
numbered  O
[1-5]  O
.  O
(standard  O
deviation:  O
m  O
T5-large  O
(4),  O
m  O
BART  O
-large  O
(10)).  O
We  O
can  O
observe  O
that  O
the  O
task  O
accuracy  O
is  O
low-  O
est  O
(<90%)  O
when  O
translating  O
to  O
formal  O
Japanese.  O
By  O
inspection,  O
we  O
observe  O
three  O
broad  O
classes  O
of  O
er-  O
rors:  O
1)  O
lexical  O
choice,  O
2)  O
cross-script  O
matching,  O
3)  O
ambiguity  O
in  O
politeness  O
levels  O
(  O
Feely  O
et  O
al.  O
,2019  O
).  O
Lexical  O
choice  O
is  O
invariant  O
in  O
machine  O
translation  O
and  O
is  O
occasionally  O
a  O
valid  O
error  O
in  O
the  O
case  O
of  O
mis-  O
translation,  O
so  O
we  O
focus  O
on  O
the  O
latter  O
two  O
error  O
cases.  O
Japanese  O
has  O
three  O
writing  O
systems  O
and  O
false  O
pos-  O
itives  O
in  O
formality  O
evaluation  O
can  O
occur  O
when  O
sur-  O
face  O
forms  O
do  O
not  O
match  O
as  O
in  O
the  O
case  O
of  O
s√⌅  O
which  O
can  O
also  O
be  O
written  O
as  O
⌦B⌫M⌅  O
(gloss:  O
‘interesting’).  O
Finally,  O
there  O
are  O
cases  O
in  O
which  O
the  O
system  O
and  O
reference  O
formality  O
mismatch  O
but  O
can  O
both  O
be  O
interpreted  O
as  O
formal  O
(e.g.,  O
" >⇡vs.  O
"✏;  O
gloss:  O
‘work’  O
(polite)  O
vs.  O
‘work’  O
(formal)).  O
Zero-Shot  O
We  O
observe  O
limited  O
zero-shot  O
trans-  O
fer  O
of  O
grammatical  O
formality  O
to  O
unseen  O
lan-  O
guages  O
(Table  O
11).  O
For  O
both  O
mBART-large  O
and  O
mT5-large  O
models,  O
the  O
EN-ITperformance  O
is  O
biased  O
towards  O
informal  O
translations,  O
while  O
EN-  O
RUis  O
biased  O
in  O
the  O
formal  O
direction.  O
In  O
the  O
case  O
of  O
EN-IT,  O
both  O
m  O
BART  O
-large  O
and  O
m  O
T5-large  O
almost  O
always  O
interpret  O
the  O
English  O
second  O
person  O
pronoun  O
as  O
second  O
person  O
plural  O
when  O
translating  O
to  O
formal,exploiting  O
the  O
ambiguity  O
of  O
English  O
on  O
the  O
source  O
side.  O
By  O
contrast,  O
when  O
generating  O
informal  O
transla-  O
tions,  O
pronouns  O
are  O
typically  O
preserved  O
as  O
singular.  O
In  O
comparison,  O
with  O
m  O
T5-large-based  O
translations  O
intoRU,  O
we  O
see  O
almost  O
unanimous  O
preference  O
to-  O
ward  O
the  O
formal,  O
likely  O
due  O
to  O
sampling  O
bias  O
when  O
curating  O
the  O
synthetic  O
training  O
set.  O
We  O
also  O
observe  O
that  O
m  O
BART  O
-large  O
prefers  O
to  O
translate  O
in  O
a  O
formal  O
manner  O
irrespective  O
of  O
desired  O
target.  O
In  O
addition,  O
when  O
m  O
BART  O
-large  O
fails  O
to  O
account  O
for  O
the  O
tar-  O
get  O
formality,  O
it  O
often  O
generates  O
paraphrases  O
of  O
the  O
formal  O
target.  O
These  O
strong  O
preferences  O
might  O
be  O
symptoms  O
of  O
systematic  O
differences  O
in  O
formality  O
across  O
languages  O
in  O
the  O
training  O
data  O
of  O
these  O
mod-  O
els.  O
Finally,  O
the  O
use  O
of  O
silver  O
standard  O
formality  O
labels  O
(“fully  O
supervised”  O
setting  O
(  O
FS))  O
does  O
not  O
improve  O
over  O
the  O
zero-shot  O
approach,  O
with  O
similar  O
observations  O
of  O
m  O
T5-large-based  O
translations  O
as  O
outlined  O
above.  O
We  O
observe  O
that  O
in  O
the  O
case  O
of  O
EN-  O
RU,  O
there  O
is  O
a  O
higher  O
incidence  O
of  O
code-switched  O
translations.  O
This  O
may  O
indicate  O
noise  O
introduced  O
in  O
the  O
automatic  O
labeling  O
process  O
and  O
requires  O
further  O
examination  O
in  O
future  O
work.3347  O
Related  O
Work  O
Most  O
MTapproaches  O
only  O
indirectly  O
capture  O
the  O
style  O
properties  O
of  O
the  O
target  O
text.  O
While  O
efforts  O
have  O
been  O
made  O
to  O
generate  O
better  O
outputs  O
in  O
their  O
pragmatic  O
context  O
via  O
controlling  O
formality  O
(  O
Sen-  O
nrich  O
et  O
al.  O
,2016  O
;Feely  O
et  O
al.  O
,2019  O
;Niu  O
and  O
Carpuat  O
,2020  O
;Schioppa  O
et  O
al.  O
,2021  O
),  O
complex-  O
ity  O
(Marchisio  O
et  O
al.  O
,2019  O
;Agrawal  O
and  O
Carpuat  O
,  O
2019  O
),  O
gender  O
(  O
Rabinovich  O
et  O
al.  O
,2017  O
),  O
these  O
stud-  O
ies  O
only  O
focus  O
a  O
single  O
language  O
pair.  O
Due  O
to  O
the  O
paucity  O
of  O
style  O
annotated  O
corpora,  O
zero-shot  O
style  O
transfer  O
within  O
and  O
across  O
languages  O
has  O
received  O
a  O
lot  O
of  O
attention.  O
However,  O
adapting  O
pre-trained  O
large-scale  O
language  O
models  O
during  O
inference  O
us-  O
ing  O
only  O
a  O
few  O
examples  O
(  O
Garcia  O
et  O
al.  O
,2021  O
;Riley  O
et  O
al.  O
,2021  O
;Krishna  O
et  O
al.  O
,2022  O
)  O
limits  O
their  O
trans-  O
fer  O
ability  O
and  O
the  O
diversity  O
of  O
their  O
outputs.  O
While  O
prior  O
works  O
use  O
pre-trained  O
language  O
models  O
like  O
BERT  O
,GPT  O
to  O
intialize  O
✓LMfor  O
improving  O
trans-  O
lation  O
quality  O
(  O
Guo  O
et  O
al.  O
,2020  O
;Zhu  O
et  O
al.  O
,2019  O
),  O
in  O
this  O
work,  O
we  O
focus  O
on  O
adapting  O
sequence-to-  O
sequence  O
multilingual  O
models  O
for  O
controlled  O
gener-  O
ation  O
of  O
a  O
desired  O
formality  O
and  O
study  O
style  O
transfer  O
in  O
multilingual  O
supervised  O
and  O
zero-shot  O
settings.  O
8  O
Conclusion  O
We  O
present  O
the  O
University  O
of  O
Maryland’s  O
submis-  O
sion  O
which  O
examines  O
the  O
performance  O
of  O
a  O
single  O
multilingual  O
model  O
allowing  O
control  O
of  O
both  O
tar-  O
get  O
language  O
and  O
formality.  O
Results  O
show  O
that  O
while  O
multilingual  O
FSMT  O
models  O
lag  O
behind  O
large,  O
bilingual,  O
formality-speciﬁc  O
models  O
in  O
terms  O
of  O
MT  O
quality,  O
they  O
show  O
stronger  O
formality  O
control  O
performance  O
across  O
all  O
the  O
language  O
pairs.  O
Fur-  O
thermore,  O
while  O
synthetic  O
unpaired  O
triplets  O
help  O
mT5-large  O
with  O
FSMT  O
performance  O
and  O
the  O
two-stage  O
ﬁnetuning  O
process  O
improves  O
MTquality  O
and  O
contrastive  O
task  O
performance,  O
mBART-large  O
still  O
outperforms  O
this  O
class  O
of  O
models,  O
likely  O
due  O
to  O
its  O
large  O
amount  O
of  O
pre-training  O
supervision.  O
In  O
future  O
work,  O
we  O
suggest  O
a  O
deeper  O
investiga-  O
tion  O
of  O
potentially  O
confounding  O
roles  O
in  O
the  O
study  O
ofFSMT  O
,  O
such  O
as  O
the  O
impact  O
of  O
formal  O
register  O
as  O
compared  O
to  O
grammatical  O
formality  O
in  O
training  O
data.  O
We  O
also  O
suggest  O
a  O
thorough  O
analysis  O
of  O
what  O
is  O
transferred  O
in  O
the  O
zero-shot  O
setting.  O
Finally,  O
we  O
recommend  O
an  O
audit  O
of  O
underlying  O
pre-training  O
and  O
ﬁnetuning  O
data  O
sources  O
for  O
pre-trained  O
multilingual  O
models,  O
which  O
we  O
believe  O
hinder  O
zero-shot  O
formal-  O
ity  O
transfer  O
for  O
EN-ITandEN-RUin  O
which  O
a  O
sin-  O
gle  O
formality  O
is  O
strongly  O
sentences  O
for  O
each  O
language  O
pair  O
from  O
existing  O
bitext.  O
P:  O
Person;  O
PP:  O
Personal  O
pronoun;  O
N:  O
Number;  O
x∈Mindicates  O
that  O
some  O
token  O
within  O
the  O
sentence  O
has  O
morphological  O
features  O
matching  O
xas  O
produced  O
by  O
spaCy.  O
B  O
Glosses  O
B.1  O
Necessarily  O
formal  O
Appropriate  O
pronouns  O
with  O
accompanying  O
conjugation  O
imply  O
the  O
sentence  O
is  O
grammatically  O
formal.  O
(1)  O
¿Cuándo  O
Whennació  O
bornusted?  O
you  O
(form.)?(Spanish)  O
‘When  O
were  O
you  O
(form.)  O
born?’  O
(2)  O
Woher  O
Where  O
fromkommen  O
comeSie?  O
you  O
(form.)?(German)  O
‘Where  O
are  O
you  O
(form.)  O
from?’  O
B.2  O
Necessarily  O
informal  O
Appropriate  O
pronouns  O
with  O
accompanying  O
conjugation  O
imply  O
the  O
sentence  O
is  O
grammatically  O
informal.  O
Note  O
that  O
Spanish  O
is  O
pro-drop,  O
which  O
relaxes  O
the  O
requirement  O
on  O
personal  O
pronouns.  O
(3)  O
¿Cuándo  O
Whennaciste  O
born(tú)?  O
you  O
(inf.)?(Spanish)  O
‘When  O
were  O
you  O
(inf.)  O
born?’  O
(4)  O
Woher  O
Where  O
fromkommst  O
comedu?  O
you  O
(inf.)?(German)  O
‘Where  O
are  O
you  O
(inf.)  O
from?’  O
B.3  O
Ambiguously  O
formal  O
Because  O
Spanish  O
is  O
pro-drop,  O
personal  O
pronouns  O
can  O
be  O
omitted  O
depending  O
on  O
context.  O
Since  O
formal  O
conjugations  O
are  O
shared  O
with  O
neutral  O
third  O
person  O
subjects,  O
this  O
leaves  O
ambiguity  O
when  O
the  O
pronoun  O
is  O
dropped.  O
For  O
sake  O
of  O
gloss,  O
we  O
use  O
∅to  O
indicate  O
a  O
dropped  O
pronoun.  O
(5)  O
¿Cuándo  O
Whennació  O
born∅?  O
{you  O
(form.),  O
he,  O
she,  O
it}?  O
‘When  O
{were  O
you  O
(form.),  O
was  O
{he,  O
she,  O
it}}  O
born?’  O
C  O
Official  O
Evaluation  O
We  O
report  O
the  O
number  O
of  O
examples  O
labeled  O
as  O
FORMAL  O
,INFORMAL  O
,NEUTRAL  O
,OTHER  O
by  O
the  O
formality  O
scorer  O
for  O
the  O
best  O
multilingual  O
models  O
(  O
[1,  O
4]  O
)  O
and  O
the  O
baseline  O
systems  O
for  O
each  O
language  O
pair  O
and  O
formality  O
direction.  O
As  O
described  O
in  O
3,  O
the  O
accuracy  O
is  O
computed  O
based  O
on  O
realized  O
matches,  O
which  O
excludes  O
examples  O
labelled  O
as  O
NEUTRAL  O
andOTHER  O
.  O
Figure  O
2  O
shows  O
that  O
the  O
number  O
of  O
these  O
excluded  O
NEUTRAL  O
samples  O
can  O
range  O
from  O
15%  O
to  O
43%.338D  O
Example  O
Outputs  O
Source:  O
Wow,  O
that’s  O
awesome!  O
Who  O
is  O
your  O
favorite  O
Baseball  O
team?  O
I  O
like  O
my  O
Az  O
team  O
lol  O
German  O
Formal  O
Hypothesis:  O
Wow,  O
das  O
ist  O
toll!  O
Wer  O
ist  O
Ihr  O
Lieblings-  O
Baseballteam?  O
Ich  O
mag  O
meine  O
Az-Team  O
lol.  O
German  O
Formal  O
Reference:  O
Wow,  O
das  O
ist  O
fantastisch!  O
Welches  O
ist  O
Ihr  O
Lieblingsbaseballteam?  O
Ich  O
stehe  O
auf  O
mein  O
AZ-Team  O
lol.  O
German  O
Informal  O
Hypothesis:  O
Wow,  O
das  O
ist  O
toll!  O
Wer  O
ist  O
dein  O
Lieblings  O
野球team?  O
Ich  O
mag  O
meine  O
Az  O
Team  O
lol.  O
German  O
Informal  O
Reference:  O
Wow,  O
das  O
ist  O
fantastisch!  O
Welches  O
ist  O
dein  O
Lieblingsbaseballteam?  O
Ich  O
stehe  O
auf  O
mein  O
AZ-Team  O
lol.  O
Table  O
13:  O
Contrastive  O
outputs  O
from  O
English-German.  O
Note  O
that  O
there  O
is  O
not  O
only  O
variety  O
in  O
lexical  O
choice  O
between  O
references  O
and  O
hypotheses,  O
but  O
also  O
between  O
hypotheses  O
of  O
varying  O
formality  O
(i.e.,  O
野球is  O
“baseball”  O
in  O
Japanese)  O
E  O
Accuracy  O
of  O
Formality  O
Classifiers  O
We  O
report  O
the  O
accuracy  O
of  O
the  O
learned  O
classifiers  O
on  O
the  O
T  O
ASK  O
TRAIN  O
dataset  O
in  O
Accuracy  O
of  O
trained  O
formality  O
classifiers  O
on  O
the  O
T  O
ASK  O
DEV  O
Class  O
Distribution  O
for  O
the  O
baseline,  O
m  O
BART  O
-large  O
and  O
m  O
T5-large  O
systems  O
for  O
all  O
the  O
supervised  O
language  O
pairs.340  O