Proceedings O
of O
the O
3rd O
Wordplay: O
When O
Language O
Meets O
Games O
Workshop O
(Wordplay O
2022) O
, O
pages O
44 O
=- O
58 O
July O
14, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
A O
Sequence B-MethodName
Modelling I-MethodName
Approach I-MethodName
to I-MethodName
Question I-MethodName
Answering I-MethodName
in I-MethodName
Text-Based I-MethodName
Games I-MethodName
Greg O
Furman1Edan O
Toledo1,3Jonathan O
Shock2,4,5Jan O
Buys1 O
1Department O
of O
Computer O
Science, O
University O
of O
Cape O
Town O
2Department O
of O
Mathematics O
and O
Applied O
Mathematics, O
University O
of O
Cape O
Town O
3InstaDeep4INRS, O
Montreal, O
Canada5NiTHeCS, O
South O
Africa O
frmgre001@myuct.ac.za, O
e.toledo@instadeep.com, O
jonathan.shock@uct.ac.za, O
jbuys@cs.uct.ac.za O
Abstract B-TaskName
Interactive I-TaskName
Question I-TaskName
Answering I-TaskName
(IQA) I-TaskName
requires O
an O
intelligent O
agent O
to O
interact O
with O
a O
dynamic O
environment O
in O
order O
to O
gather O
information O
nec- O
essary O
to O
answer O
a O
question. O
IQA B-TaskName
tasks O
have O
been O
proposed O
as O
means O
of O
training O
systems O
to O
develop O
language O
or O
visual O
comprehension O
abilities. O
To O
this O
end, O
the O
Question B-TaskName
Answering I-TaskName
with I-TaskName
Interactive I-TaskName
Text I-TaskName
(QAit) I-TaskName
task O
was O
created O
to O
produce O
and O
benchmark O
interactive O
agents O
capable O
of O
seeking O
information O
and O
answering O
questions O
in O
unseen O
environments. O
While O
prior O
work O
has O
exclusively O
focused O
on O
IQA B-TaskName
as O
a O
re- O
inforcement O
learning O
problem, O
such O
methods O
suffer O
from O
low O
sample O
efficiency O
and O
poor O
ac- O
curacy O
in O
zero-shot O
evaluation. O
In O
this O
paper, O
we O
propose O
the O
use O
of O
the O
recently O
proposed O
Decision B-MethodName
Transformer I-MethodName
architecture O
to O
provide O
improvements O
upon O
prior O
baselines. O
By O
utilis- O
ing O
a O
causally O
masked O
GPT-2 B-MethodName
Transformer O
for O
command O
generation O
and O
a O
BERT B-MethodName
model O
for O
question O
answer O
prediction, O
we O
show O
that O
the O
Decision B-MethodName
Transformer I-MethodName
achieves O
performance O
greater O
than O
or O
equal O
to O
current O
state-of-the-art O
RL O
baselines O
on O
the O
QAit B-TaskName
task O
in O
a O
sample O
ef- O
ficient O
manner. O
In O
addition, O
these O
results O
are O
achievable O
by O
training O
on O
sub-optimal O
random O
trajectories, O
therefore O
not O
requiring O
the O
use O
of O
online O
agents O
to O
gather O
data. O
1 O
Introduction O
Traditional O
methods O
for O
question B-TaskName
answering I-TaskName
(QA) I-TaskName
and O
machine B-TaskName
reading I-TaskName
comprehension I-TaskName
(MRC) I-TaskName
are O
primarily O
concerned O
with O
the O
retrieval O
of O
declara- O
tive O
knowledge O
, O
that O
is, O
explicitly O
stated O
or O
static O
descriptions O
of O
entities O
in O
text O
documents O
or O
within O
a O
knowledge O
base O
(KB) O
(Trischler O
et O
al., O
2017). O
These O
models O
tend O
to O
answer O
questions O
primarily O
through O
basic O
pattern O
matching O
skills, O
further O
dif- O
ferentiating O
their O
abilities O
from O
those O
of O
humans. O
Conversely, O
procedural O
knowledge O
is O
the O
sequence O
of O
actions O
required O
to O
perform O
a O
task O
(Georgeff O
and O
Lansky, O
1986). O
To O
this O
end, O
interactive B-TaskName
question I-TaskName
an- I-TaskName
swering I-TaskName
(IQA) I-TaskName
has O
been O
proposed O
as O
a O
frameworkfor O
teaching O
MRC O
systems O
to O
gather O
the O
informa- O
tion O
necessary O
for O
question B-TaskName
answering I-TaskName
(Yuan O
et O
al., O
2019). O
IQA B-TaskName
requires O
an O
agent O
to O
interact O
with O
some O
dy- O
namic O
environment O
in O
order O
to O
gather O
the O
required O
knowledge O
to O
answer O
a O
question O
(Gordon O
et O
al., O
2018). O
As O
such, O
the O
task O
is O
well-suited O
to O
be O
ap- O
proached O
as O
a O
reinforcement O
learning O
(RL) O
prob- O
lem. O
Yuan O
et O
al. O
-2019 O
proposed O
Question B-TaskName
An- I-TaskName
swering I-TaskName
using I-TaskName
interactive I-TaskName
text I-TaskName
(QAit) I-TaskName
as O
a O
means O
of O
testing O
the O
knowledge O
gathering O
capabilities O
of O
an O
agent O
required O
to O
answer O
a O
question O
about O
its O
envi- O
ronment. O
Here O
an O
agent O
interacts O
with O
a O
partially O
observable O
text-based O
environment, O
created O
using O
Microsoft O
TextWorld O
(Côté O
et O
al., O
2018), O
in O
order O
to O
gather O
information O
and O
answer O
questions O
about O
the O
attributes, O
location, O
and O
existence O
of O
objects. O
The O
QAit B-TaskName
task O
thus O
aims O
to O
benchmark O
generalisa- O
tion O
and O
provides O
an O
environment O
to O
train O
agents O
capable O
of O
gathering O
information O
and O
answering O
questions. O
Yuan O
et O
al.’s O
proposed O
baselines O
(using O
DQN O
(Mnih O
et O
al., O
2015), O
DDQN O
(Van O
Hasselt O
et O
al., O
2016), O
and O
Rainbow O
(Hessel O
et O
al., O
2018)) O
all O
suf- O
fered O
from O
low O
sample O
efficiency O
and O
relatively O
poor O
performance O
on O
all O
three O
question O
types O
(loca- O
tion, O
attribute, O
and O
existence). O
These O
shortcomings O
suggest O
that O
alternative O
architectures O
and O
method- O
ologies O
are O
required O
to O
improve O
performance O
within O
the O
QAit O
setting. O
Transformers O
(Vaswani O
et O
al., O
2017) O
have O
shown O
success O
in O
modelling O
a O
diverse O
range O
of O
high- O
dimensional O
problems O
(Brown O
et O
al., O
"2020;" O
Ramesh O
et O
al., O
"2021;" O
Devlin O
et O
al., O
2019). O
Additionally, O
existing O
language O
models O
such O
as O
BERT B-MethodName
(Bidirec- I-MethodName
tional I-MethodName
Encoder I-MethodName
Representations I-MethodName
from I-MethodName
Transform- I-MethodName
ers) I-MethodName
and O
GPT B-MethodName
(Generative B-MethodName
Pre-Trained) I-MethodName
(Radford O
et O
al., O
2019) O
have O
been O
utilised O
to O
reduce O
the O
size O
of O
datasets O
required O
for O
training O
downstream O
lan- O
guage O
tasks O
(Lee O
and O
Hsiang, O
"2019;" O
Mager O
et O
al., O
2020). O
These O
benefits O
coupled O
with O
the O
demon-44strated O
ability O
of O
Transformers O
to O
model O
long O
se- O
quences O
by O
utilising O
the O
self-attention O
mechanism O
makes O
this O
architecture O
ideal O
for O
IQA. O
Recent O
work O
(Chen O
et O
al., O
"2021;" O
Janner O
et O
al., O
2021) O
have O
shown O
the O
applicability O
of O
Transformers O
to O
sequential O
deci- O
sion O
making O
problems O
as O
an O
alternative O
solution O
to O
RL O
problems. O
These O
approaches O
frame O
RL O
trajec- O
tories O
as O
sequences O
of O
states, O
actions, O
and O
rewards O
modelled O
autoregressively O
by O
a O
Transformer. O
This O
sequence O
modelling O
approach O
is O
referred O
to O
as O
the O
Decision O
Transformer O
(DT) O
(Chen O
et O
al., O
2021). O
In O
this O
paper O
we O
apply O
the O
Decision B-MethodName
Transformer I-MethodName
to O
QAit, B-TaskName
replacing O
the O
online O
interaction O
and O
train- O
ing O
methodology O
of O
RL O
approaches O
with O
a O
Decision B-MethodName
Transformer I-MethodName
that O
utilises O
the O
GPT-2 B-MethodName
(Radford O
et O
al., O
2019) O
architecture, O
closely O
following O
the O
method- O
ology O
outlined O
by O
(Chen O
et O
al., O
2021). O
We O
propose O
an O
additional O
QA B-TaskName
module O
that O
is O
a O
fine-tuned O
BERT B-MethodName
model, O
with O
the O
aim O
of O
leveraging O
pre-trained O
lan- O
guage O
models O
to O
provide O
more O
accurate O
answers O
to O
questions. O
We O
show O
that O
by O
framing O
the O
QAit B-TaskName
task O
as O
a O
sequence O
modelling O
problem, O
a O
Decision B-MethodName
Transformer I-MethodName
matches O
or O
exceeds O
the O
performance O
of O
previous O
RL-based O
benchmarks O
when O
trained O
on O
random O
episodic O
rollouts, O
while O
using O
significantly O
less O
data. O
Our O
main O
contributions O
are O
as O
follows: O
1.We O
show O
that O
an O
offline O
reinforcement O
learn- O
ing O
method O
is O
able O
to O
match O
the O
performance O
of O
online O
value-based O
reinforcement O
learning O
baselines O
in O
the O
QAit B-TaskName
environment. O
2.We O
show O
that O
by O
framing O
IQA B-TaskName
as O
a O
sequence O
modelling O
problem, O
the O
performance O
of O
the O
QAit B-TaskName
baselines O
can O
be O
matched O
using O
signifi- O
cantly O
less O
training O
data. O
3.We O
show O
that O
the O
Decision B-MethodName
Transformer I-MethodName
archi- O
tecture O
is O
able O
to O
learn O
policies O
comparable O
to O
those O
of O
online O
reinforcement O
learning O
meth- O
ods O
from O
purely O
random O
data, O
illustrating O
the O
architecture’s O
ability O
to O
find O
structure O
in O
inher- O
ently O
noisy O
data. O
2 O
Background O
2.1 O
QAit B-TaskName
QAit B-TaskName
is O
implemented O
in O
TextWorld1(Côté O
et O
al., O
2018), O
an O
open-source O
simulator O
for O
training O
rein- O
forcement O
learning O
(RL) O
agents O
for O
decision O
mak- O
ing O
and O
language O
comprehension. O
QAit B-TaskName
text-based O
1https://www.microsoft.com/en- O
us/research/project/textworld/environments O
are O
generated O
procedurally O
via O
sam- O
pling O
from O
a O
distribution O
of O
world O
settings. O
There O
are O
two O
environment O
map O
types: O
A O
fixed O
map O
con- O
tains O
six O
rooms, O
whereas O
random O
maps O
sample O
their O
number O
of O
rooms O
from O
a O
uniform O
distribution O
U(2,12). O
QAit O
requires O
an O
agent O
to O
answer O
ques- O
tions O
about O
the O
location, O
existence O
and O
attributes O
of O
objects O
in O
an O
environment. O
An O
agent O
interacts O
with O
a O
QAit B-TaskName
environment O
using O
text O
commands O
that O
consist O
of O
an O
action, O
modifier, O
and O
object O
triplet, O
e.g., O
open O
A O
generated O
environ- O
ment O
consists O
of O
rooms O
each O
containing O
randomly O
assigned O
objects O
and O
location O
names. O
The O
agent O
moves O
around O
in O
the O
environment O
for O
a O
pre-defined O
number O
of O
time O
steps O
or O
until O
the O
predicted O
com- O
mand O
action O
is O
wait. O
TextWorld O
responds O
to O
agent O
commands O
with O
a O
state O
string O
containing O
informa- O
tion O
about O
the O
room O
the O
agent O
is O
in O
and O
the O
objects O
present. O
2.1.1 O
Question O
types O
An O
agent O
is O
required O
to O
answer O
one O
of O
three O
question O
types: O
•Location O
questions O
assess O
an O
agent’s O
ability O
to O
navigate O
the O
environment O
to O
find O
the O
location O
of O
an O
object. O
For O
example, O
Where O
could O
be O
answered O
with O
garden O
or O
toolbox. O
•Existence O
questions O
requires O
the O
agent O
to O
nav- O
igate O
and O
interact O
with O
the O
environment O
to O
gather O
knowledge O
and O
determine O
whether O
an O
object O
exists. O
Questions O
are O
phrased O
as O
is O
where O
X O
is O
an O
en- O
tity O
in O
the O
vocabulary, O
and O
answers O
are O
either O
yes O
"(""1"")" O
or O
no O
"(""0"")." O
•Attribute O
questions O
require O
that O
the O
agent O
in- O
teracts O
with O
an O
object O
to O
determine O
whether O
it O
has O
a O
particular O
characteristic O
or O
quality. O
For O
such O
question O
types, O
the O
level O
of O
interaction O
and O
movement O
required O
in O
observing O
a O
suffi- O
cient O
amount O
of O
information O
to O
answer O
a O
ques- O
tion O
greatly O
exceeds O
location O
and O
existence O
questions. O
Answers O
are O
also O
either O
yes O
or O
no. O
For O
example, O
Is O
requires O
an O
agent O
to O
find O
and O
interact O
with O
stove O
to O
answer O
the O
question O
correctly. O
Comprehension O
of O
both O
the O
question O
and O
the O
environment O
are O
required. O
Entities O
often O
have O
arbitrary O
names O
and O
at- O
tributes, O
making O
memorisation O
impossible O
2.1.2 O
Rewards O
Yuan O
et O
al. O
-2019 O
proposed O
two O
reward O
types: O
Sufficient B-MetricName
Information: I-MetricName
Sufficient B-MetricName
information I-MetricName
is O
a O
metric O
used O
to O
evaluate O
the O
amount O
of O
informa- O
tion O
gathered O
by O
the O
agent O
and O
whether O
or O
not O
the O
information O
was O
sufficient O
to O
answer O
the O
question O
(Yuan O
et O
al., O
2019). O
It O
is O
also O
used O
as O
part O
of O
the O
re- O
ward O
function. O
The O
sufficient B-MetricName
information I-MetricName
score I-MetricName
is O
calculated O
when O
the O
agent O
decides O
to O
stop O
the O
inter- O
action O
and O
answer O
the O
question. O
For O
each O
question O
type, O
the O
sufficient B-MetricName
information I-MetricName
score I-MetricName
is O
calculated O
as O
follows: O
•Location: O
A O
score O
of O
1 B-MetricValue
is O
given O
if, O
when O
the O
agent O
decides O
to O
stop O
the O
interaction, O
the O
entity O
mentioned O
in O
the O
question O
is O
present O
in O
the O
final O
observation. O
This O
indicates O
the O
agent O
has O
witnessed O
the O
information O
it O
needs O
to O
answer O
the O
question O
successfully. O
If O
the O
mentioned O
entity O
is O
not O
present O
in O
the O
final O
observation O
then O
a O
score O
of O
0 B-MetricValue
is O
given. O
•Existence: O
If O
the O
TRUE O
answer O
to O
the O
question O
isyesthen O
a O
score O
of O
1 B-MetricValue
is O
given O
if O
the O
entity O
mentioned O
in O
the O
question O
is O
present O
in O
the O
final O
observation. O
If O
the O
TRUE O
answer O
to O
the O
question O
is O
no, O
then O
a O
score O
between O
0 B-MetricValue
and I-MetricValue
1is I-MetricValue
given O
proportional O
to O
the O
amount O
of O
explo- O
ration O
coverage O
of O
the O
environment O
the O
agent O
has O
performed. O
Intuitively O
this O
can O
be O
seen O
as O
a O
confidence O
score O
=- O
if O
the O
agent O
witnesses O
the O
entity, O
it O
is O
100% O
confident O
of O
its O
"existence;" O
otherwise, O
until O
it O
explores O
the O
entire O
environ- O
ment, O
it O
cannot O
be O
completely O
confident. O
•Attribute: O
Attribute O
questions O
have O
a O
set O
of O
heuristics O
defined O
to O
verify O
each O
attribute O
and O
assign O
a O
score O
of O
sufficient O
information. O
Each O
attribute O
has O
specific O
commands O
that O
need O
to O
be O
executed O
for O
sufficient O
information O
to O
be O
gathered. O
This O
also O
depends O
on O
the O
agent O
be- O
ing O
in O
certain O
states O
for O
these O
outcomes O
to O
be O
observed O
correctly, O
e.g. O
an O
agent O
needs O
to O
be O
holding O
an O
object O
to O
try O
to O
eat O
the O
object. O
Exploration O
Reward: O
The O
agent O
is O
also O
given O
an O
exploration O
reward O
(Yuan O
et O
al., O
2018) O
whenever O
en- O
tering O
a O
previously O
unseen O
state O
in O
order O
to O
promote O
exploration O
of O
the O
environment. O
2.1.3 O
Evaluation O
(Yuan O
et O
al., O
2019) O
trained O
agents O
on O
multiple O
Num- O
ber O
of O
Games O
settings, O
i.e., O
number O
of O
unique O
envi- O
ronments O
that O
an O
agent O
interacts O
with O
during O
train-46ing. O
In O
this O
paper, O
we O
restrict O
our O
experiments O
to O
the O
500 O
games O
setting O
when O
generating O
offline O
training O
data O
for O
the O
Decision B-MethodName
Transformer. I-MethodName
We O
measure O
an O
agent’s O
performance O
through O
both O
sufficient B-MetricName
information I-MetricName
score I-MetricName
and O
question B-MetricName
an- I-MetricName
swering I-MetricName
accuracy. I-MetricName
Models O
are O
evaluated O
in O
a O
zero- O
shot O
evaluation O
on O
the O
QAit B-TaskName
test O
set O
in O
order O
to O
assess O
agents’ O
generalisation O
abilities. O
Each O
ques- O
tion O
type O
and O
map O
type O
have O
their O
own O
unique O
set O
of O
500 O
never-before-seen O
games, O
each O
containing O
a O
single O
question. O
2.2 O
Decision B-MethodName
Transformer I-MethodName
The O
Decision B-MethodName
Transformer I-MethodName
(Chen O
et O
al., O
2021) O
ar- O
chitecture O
approaches O
reinforcement O
learning O
prob- O
lems O
by O
autoregressively O
modelling O
a O
trajectory O
of O
actions/commands, O
states, O
and O
rewards. O
Com- O
mand O
triples O
(action, O
modifier, O
noun) O
are O
condi- O
tioned O
upon O
the O
total O
reward O
that O
can O
still O
be O
gath- O
ered O
from O
interacting O
with O
the O
environment. O
This O
is O
referred O
to O
as O
the O
returns-to-go O
(RTG) O
Rt=/summationtextT O
t′=trt′where O
Tis O
the O
trajectory O
length O
and O
rtis O
the O
reward O
at O
time O
step O
t. O
Thus O
the O
ini- O
tial O
return-to-go O
R1represents O
the O
total O
reward O
to O
be O
gained O
from O
a O
given O
episode. O
After O
every O
episodic O
play-through, O
the O
trajectory O
is O
represented O
as(R1, O
s1, O
a1, O
R2, O
s2, O
a2...RT, O
sT, O
aT), O
where O
Rt O
is O
the O
RTG, O
sis O
a O
state, O
and O
aan O
action/command. O
An O
example O
QAit B-TaskName
trajectory O
is O
shown O
in O
Figure O
2 O
The O
trajectory O
representation O
enables O
training O
a O
sequence O
model O
such O
as O
GPT-2 B-MethodName
(Radford O
et O
al., O
2019), O
as O
command O
prediction O
is O
based O
on O
gaining O
some O
future O
reward, O
rather O
than O
on O
how O
much O
re- O
ward O
has O
already O
been O
obtained. O
During O
testing O
the O
model O
is O
conditioned O
on O
total O
desired O
reward O
by O
set- O
tingR1and O
the O
starting O
state O
to O
generate O
command O
sequences O
autoregressively. O
If O
an O
agent O
obtains O
some O
reward O
while O
interacting O
with O
the O
world, O
this O
is O
deducted O
from O
its O
RTG O
in O
subsequent O
time O
steps. O
3 O
Approach O
Training O
a O
Decision B-MethodName
Transformer I-MethodName
requires O
offline O
training O
data O
for O
supervised O
learning. O
Online O
rein- O
forcement O
learning, O
in O
contrast, O
sees O
an O
agent O
con- O
tinually O
interacting O
with O
the O
environment O
to O
gather O
experience O
and O
update O
its O
policy O
based O
on O
observed O
rewards. O
3.1 O
Training O
data O
generation O
with O
random O
rollouts O
We O
generate O
offline O
training O
data O
using O
random O
rollouts O
for O
each O
map O
type O
and O
question O
type O
in O
the O
500 O
games O
setting. O
The O
rollouts O
are O
generated O
using O
a O
random O
agent O
which O
uniformly O
samples O
commands O
from O
all O
admissible O
commands O
for O
a O
particular O
time O
step. O
This O
restriction O
stems O
from O
the O
sparsity O
of O
the O
action O
space O
(approximately O
16543 O
possible O
commands O
compared O
to O
approximately O
8 O
admissible O
commands): O
sampling O
commands O
from O
the O
complete O
vocabulary O
results O
in O
mostly O
invalid O
commands. O
Thus, O
by O
only O
using O
admissible O
com- O
mands O
in O
data O
generation, O
we O
intend O
for O
the O
Deci- B-MethodName
sion I-MethodName
Transformer I-MethodName
to O
learn O
which O
command O
triplets O
are O
admissible O
(as O
this O
is O
unknown O
during O
testing). O
The O
sequence O
of O
commands O
and O
observed O
states O
are O
recorded O
along O
with O
the O
reward O
for O
each O
command. O
Training O
dataset O
statistics O
are O
given O
in O
Table O
1 O
3.2 O
Decision B-MethodName
Transformer I-MethodName
The O
maximum O
trajectory O
input O
length O
of O
the O
De- O
cision O
Transformer O
is O
set O
to O
K= O
50 O
time O
steps, O
which O
is O
the O
maximum O
length O
of O
a O
QAit B-TaskName
episode. O
This O
allows O
the O
DT B-MethodName
to O
access O
the O
entire O
trajectory O
for O
command O
generation O
and O
question B-TaskName
answering. I-TaskName
Token O
embeddings O
for O
states O
and O
command O
se- O
quences O
are O
obtained O
using O
a O
single O
embedding O
layer. O
At O
each O
time O
step O
the O
sequence O
of O
tokens O
representing O
the O
current O
state O
of O
the O
environment O
is O
encoded O
with O
a O
GRU O
(Cho O
et O
al., O
2014) O
with O
the O
fi- O
nal O
hidden O
state O
hnrepresenting O
the O
entire O
encoded O
environment O
state. O
We O
concatenate O
the O
question O
to O
the O
end O
of O
each O
state O
sequence, O
separated O
by O
a O
“< O
|>” O
delimiter O
token. O
Commands, O
which O
can O
consist O
of O
up O
to O
3 O
tokens, O
are O
similarly O
encoded O
with O
another O
GRU. O
Embeddings O
for O
returns-to-go O
Rtare O
also O
learnt O
and O
projected O
to O
the O
embedding O
dimension. O
Finally, O
a O
positional O
embedding O
representing O
the O
en- O
vironment O
time O
step O
tis O
concatenated O
to O
each O
input O
(returns, O
states O
& O
commands) O
after O
the O
embedding O
and O
GRU O
layers. O
The O
embedded O
and O
positionally O
encoded O
command, O
state, O
and O
return-to-go O
inputs O
are O
fed O
into O
the O
GPT B-MethodName
model. O
Figure O
1 O
shows O
the O
Decision B-MethodName
Transformer I-MethodName
architecture O
with O
example O
input. O
At O
each O
time O
step O
t, O
the O
Decision B-MethodName
Transformer I-MethodName
encoding O
xtis O
fed O
into O
four O
linear O
decoders. O
Three O
decoders O
predict O
the O
next O
command’s O
action, O
mod- O
ifier, O
and O
object O
components, O
while O
the O
fourth O
de- O
coder O
predicts O
the O
answer O
to O
the O
question O
(this O
is O
the O
same O
at O
each O
time O
step). O
Although O
we O
also O
use O
a O
separate O
QA O
module O
to O
predict O
the O
final O
answer, O
the O
answer O
decoder O
allows O
the O
Decision B-MethodName
Transformer I-MethodName
to O
learn O
some O
primitive O
level O
of O
question O
answer- O
ing, O
thereby O
allowing O
the O
QA B-HyperparameterName
loss I-HyperparameterName
to O
help O
guide O
command O
generation. O
Chen O
et O
al. O
-2021 O
found O
that O
predicting O
states O
and O
returns-to-go O
at O
each O
time O
step O
did O
not O
improve O
performance. O
This O
motivates O
exclusively O
predicting O
command O
triples, O
along O
with O
answers O
to O
questions. O
The O
model O
is O
trained O
to O
optimize O
the O
sum B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
cross-entropy I-HyperparameterName
losses I-HyperparameterName
of O
action, O
modifier, O
ob- O
ject, O
and O
answer O
prediction. O
For O
each O
question O
and O
map O
type O
configuration, O
a O
set O
of O
unique O
validation O
games O
were O
generated O
wherein O
an O
agent O
must O
inter- O
act O
with O
an O
environment O
to O
answer O
a O
question. O
Dur- O
ing O
training, O
the O
Decision B-MethodName
Transformer I-MethodName
is O
evaluated O
on O
the O
set O
of O
hold-out O
games O
every O
250 O
iterations O
to O
monitor O
sufficient O
information O
scores O
and O
to O
avoid O
overfitting. O
3.3 O
QA O
module O
The O
QA O
module O
consists O
of O
a O
pretrained O
BERT B-MethodName
encoder O
with O
a O
linear O
classification O
layer. O
The O
per-time O
step O
state O
sequences O
are O
joined O
into O
a O
sin-gle O
long O
sequence O
of O
tokens O
with O
the O
question O
ap- O
pended O
at O
the O
end. O
A O
[CLS] O
token O
is O
added O
to O
the O
beginning O
of O
the O
sequence, O
and O
a O
[SEP] O
token O
before O
and O
after O
the O
question. O
This O
concatenated O
sequence O
is O
tokenised O
by O
a O
Bert-Base-Uncased O
to- O
keniser O
(Devlin O
et O
al., O
2019) O
and O
padded O
or O
front- O
truncated O
(keeping O
the O
most O
recent O
part O
of O
the O
state O
sequence) O
to O
return O
a O
512 O
token O
sequence O
which O
is O
then O
fed O
to O
the O
BERT O
encoder. O
We O
subsequently O
pass O
BERT’s B-MethodName
output O
vector O
corresponding O
to O
the O
CLS O
token O
to O
a O
linear O
layer. O
For O
attribute O
and O
exis- O
tence O
questions O
this O
model O
performs O
binary O
classifi- O
cation O
(to O
predict O
“yes” O
or O
“no”), O
while O
for O
location O
questions O
it O
produces O
a O
softmax O
over O
the O
vocabu- O
lary. O
We O
use O
cross-entropy B-HyperparameterName
to O
calculate O
the O
loss O
between O
predicted O
and O
correct O
answers. O
Training O
and O
validation O
sets O
that O
consist O
entirely O
of O
valid O
trajectories O
are O
created O
for O
the O
QA O
module. O
We O
use O
the O
validation O
set O
(20% O
of O
the O
generated O
trajectories) O
to O
save O
the O
model O
with O
the O
highest O
validation O
QA B-MetricName
accuracy I-MetricName
(after O
30 B-HyperparameterValue
training O
epochs). B-HyperparameterName
In O
order O
to O
simulate O
more O
realistic O
QA O
training O
data, O
we O
feed O
the O
QA O
training O
examples O
back O
into O
the O
trained O
DT B-MethodName
and O
use O
it O
to O
predict O
where O
to O
cut O
off O
the O
generated O
sequence, O
as O
during O
testing O
there O
is O
no O
guarantee O
that O
the O
DT B-MethodName
would O
have O
explored O
up O
until O
the O
correct O
answer O
has O
been O
found. O
The O
sequence O
is O
cut O
off O
when O
the O
DT B-MethodName
predicts O
the O
stop O
action O
( O
wait) O
or O
the O
time O
step O
limit O
is O
exceeded. O
3.4 O
Decoding O
To O
generate O
the O
command O
sequence O
from O
the O
DT B-MethodName
during O
testing, O
instead O
of O
greedy O
decoding O
we O
sam- O
ple O
each O
next O
command O
from O
the O
probability O
distri- O
butions O
over O
the O
action, O
modifier, O
and O
object. O
This O
motivation O
is O
similar O
to O
that O
of O
stochastic O
decod- O
ing O
algorithms O
in O
natural O
language O
generation: O
The O
stochasticity O
minimises O
the O
risk O
of O
the O
Decision B-MethodName
Transformer I-MethodName
entering O
a O
loop O
in O
which O
the O
same O
command O
is O
generated O
repeatedly, O
and O
more O
closely O
mirrors O
natural O
language O
which O
avoids O
utterances O
with O
too O
high O
probability O
(Zarrieß O
et O
al., O
2021). O
In O
the O
case O
of O
answer O
prediction, O
we O
deterministically O
take O
the O
argmax O
of O
the O
output. O
3.5 O
Returns O
Tuning O
A O
shortcoming O
in O
the O
DT’s B-MethodName
methodology O
is O
the O
ex- O
pectation O
for O
the O
environment’s O
maximum O
achiev- O
able O
return O
to O
be O
known O
a O
priori O
. O
Using O
an O
inap- O
propriate O
value O
can O
greatly O
hamper O
performance, O
resulting O
in O
premature O
halting O
or O
needlessly O
exces- O
sive O
exploration. O
Thus, O
we O
tune O
the O
value O
of O
the O
initial O
returns-to-go O
R1as O
a O
hyperparameter. O
We O
create O
a O
validation O
set O
of O
50 O
games O
to O
evaluate O
the O
question O
answering O
performance O
of O
both O
the O
DT’s B-MethodName
answer O
prediction O
head O
and O
BERT B-MethodName
model. O
For O
each O
question O
and O
map O
type O
we O
consider O
R1either O
set O
as O
a O
fixed O
value O
or O
sampled O
from O
an O
exponential O
distri- O
bution. O
Following O
the O
methodology O
used O
by O
Yuan O
et O
al. O
-2019 O
for O
selecting O
the O
best O
model O
during O
training, O
we O
tune O
R1to O
maximize O
the O
sum O
of O
the O
sufficient O
information O
score O
and O
question O
answer- O
ing O
accuracy. O
See O
Appendix O
A O
for O
details. O
4 O
Results O
We O
evaluate O
the O
Decision B-MethodName
Transformer I-MethodName
both O
where O
its O
own O
answer O
prediction O
head O
is O
used O
for O
ques- O
tions O
answering O
and O
where O
this O
is O
done O
with O
the O
BERT B-MethodName
QA I-MethodName
model. O
Test O
set O
results O
on O
the O
500 O
games O
setting O
are O
given O
in O
Table O
2, O
together O
with O
with O
RLmodel O
results O
as O
reported O
by O
Yuan O
et O
al. O
(2019). O
We O
also O
report O
results O
of O
training O
the O
DT B-MethodName
on O
reduced O
datasets O
with O
only O
10,000 O
episodes, O
in O
order O
to O
fur- O
ther O
evaluate O
the O
sample O
efficiency O
of O
our O
approach O
(see O
section O
4.5). O
Training O
results O
are O
available O
in O
the O
Appendix O
in O
Tables O
7 O
and O
8 O
Table O
3 O
gives O
the O
BERT B-MethodName
QA I-MethodName
model’s O
validation O
accuracy. B-MetricName
We O
discuss O
the O
results O
for O
each O
question O
type. O
Overall, O
DT-BERT B-MethodName
outperforms O
the O
Decision B-MethodName
Transformer’s I-MethodName
answer O
prediction O
head O
in O
location O
and O
attribute O
type O
questions, O
while O
the O
DT B-MethodName
gives O
a O
higher O
accuracy B-MetricName
on O
existence O
questions. O
At O
a O
high O
level, O
these O
performance O
differences O
depend O
on O
the O
state O
and O
action O
space O
that O
the O
model O
was O
required O
to O
learn O
and O
navigate. O
Existence O
type O
and O
attribute O
type O
questions O
may O
depend O
on O
long-range O
depen- O
dencies. O
For O
example, O
existence O
type O
questions O
require O
the O
ability O
to O
know O
whether O
an O
object O
has O
been O
witnessed O
or O
not. O
When O
the O
answer O
is O
that O
an O
object O
doesn’t O
exist, O
the O
problem O
is O
more O
than O
just O
word O
matching O
within O
the O
last O
few O
states. O
We O
believe O
that O
this O
is O
why O
the O
Decision B-MethodName
Transformer I-MethodName
QA O
head O
outperforms O
the O
BERT B-MethodName
model O
on O
exis- O
tence O
and O
attribute O
questions O
since O
it O
has O
access O
to O
the O
entire O
state O
trajectory. O
On O
questions O
whose O
answers O
were O
more O
likely O
to O
be O
found O
within O
the O
last O
512 O
tokens, O
DT-BERT B-MethodName
achieves O
higher O
question O
answering O
accuracy O
than O
the O
Decision B-MethodName
Transformer. I-MethodName
4.1 O
Location O
Questions O
The O
DT’s B-MethodName
answer O
prediction O
head O
has O
a O
lower O
QA B-MetricName
accuracy I-MetricName
than O
previous O
RL O
approaches O
on O
both O
fixed O
and O
random O
maps. O
However O
its O
sufficient O
information O
scores, O
reflecting O
the O
DT’s B-MethodName
information O
gathering O
capacities, O
are O
higher. O
For O
location O
type O
questions, O
QA B-MetricName
accuracy I-MetricName
normally O
matches O
suffi- O
cient O
information O
results O
due O
to O
QA O
modules O
effec- O
tively O
performing O
word O
matching O
once O
an O
agent O
arrives O
in O
the O
correct O
state. O
We O
see O
this O
in O
the O
RL O
methods’ O
results O
as O
their O
sufficient O
informa- B-MetricName
tion I-MetricName
scores I-MetricName
are O
very O
close O
to O
their O
QA B-MetricName
accuracies. I-MetricName
This O
indicates O
that O
the O
DT’s B-MethodName
question O
answering O
prediction O
head O
is O
underfitting O
the O
training O
data. O
DT-BERT B-MethodName
outperforms O
the O
QA B-MetricName
accuracy I-MetricName
of O
the O
RL O
models O
on O
both O
fixed O
and O
random O
maps. O
On O
random O
maps, O
QA B-MetricName
accuracy I-MetricName
is O
slightly O
higher O
than O
sufficient B-MetricName
information, I-MetricName
suggesting O
that O
in O
a O
small O
number O
of O
cases O
the O
BERT B-MethodName
model O
may O
be O
able O
to O
deduce O
the O
answer O
from O
the O
context O
even O
when O
it O
does O
not O
explicitly O
appears O
in O
the O
trajectory. O
The O
performance O
gap O
between O
the O
BERT B-MethodName
QA I-MethodName
model O
and O
the O
DT B-MethodName
means O
that O
a O
question O
can O
still O
be O
correctly O
answered O
even O
if O
the O
DT B-MethodName
stops O
in O
an O
incorrect O
state. O
The O
high O
BERT B-MethodName
QA I-MethodName
accuracy O
for O
location O
questions O
can O
also O
be O
seen O
in O
Table O
3 O
We O
suggest O
two O
reasons O
for O
the O
BERT B-MethodName
QA I-MethodName
model O
answering O
location O
type O
questions O
more O
accurately O
than O
the O
Decision B-MethodName
Transformer’s I-MethodName
prediction O
head. O
First, O
it O
is O
easier O
for O
BERT B-MethodName
model O
to O
learns O
skills O
basic O
pattern O
matching O
skills O
such O
as O
identifying O
entity O
and O
location O
names O
from O
state O
strings. O
Sec- O
ond, O
exploration O
is O
not O
as O
highly O
encouraged O
with O
location O
questions O
as O
with O
existence O
and O
attribute O
types. O
Less O
exploration O
means O
fewer O
states O
visited, O
allowing O
the O
state O
context O
window O
to O
contain O
less O
noisy O
state O
strings O
than O
other O
question O
types. O
4.2 O
Existence O
Questions O
The O
DT B-MethodName
outperforms O
RL O
baselines O
on O
sufficient O
in- O
formation O
and O
QA O
accuracy O
in O
the O
random O
maps O
setting O
for O
existence O
questions. O
However O
on O
fixed O
maps O
it O
performs O
worse O
than O
the O
DQN. O
The O
BERT B-MethodName
QA I-MethodName
model O
underperforms O
the O
DT B-MethodName
answer O
predic- O
tion O
head O
here O
in O
both O
map O
types, O
suggesting O
that O
jointly O
optimising O
answer O
and O
command O
prediction O
leads O
to O
improved O
performance O
on O
existence O
typequestions. O
Reasoning O
about O
the O
existence O
of O
an O
object O
within O
a O
TextWorld O
environment O
requires O
knowl- O
edge O
about O
the O
entirety O
of O
the O
world. O
Therefore, O
ex- O
istential O
questions O
require O
an O
agent O
to O
fully O
explore O
an O
environment O
to O
answer O
whether O
or O
not O
an O
entity O
exists O
within O
it. O
The O
Decision B-MethodName
Transformer’s I-MethodName
self- O
attention O
mechanism O
makes O
performing O
long-term O
credit O
assignments O
possible. O
The O
answer O
prediction O
head O
of O
the O
DT B-MethodName
can O
thus O
draw O
upon O
information O
gathered O
in O
all O
previous O
states O
to O
inform O
question O
answering. O
As O
a O
result, O
the O
ability O
to O
model O
de- O
pendencies O
that O
stretch O
throughout O
all O
states O
en- O
countered O
allows O
the O
DT B-MethodName
to O
outperform O
the O
BERT B-MethodName
model, O
whose O
context O
window O
is O
constrained O
to O
512 O
tokens. O
4.3 O
Attribute O
Questions O
None O
of O
the O
models O
achieve O
results O
that O
are O
sub- O
stantially O
above O
50% O
on O
attribute O
questions, O
con- O
firming O
the O
challenge O
of O
this O
question O
type. O
The O
Decision B-MethodName
Transformer I-MethodName
did O
obtain O
higher O
sufficient B-MetricName
information I-MetricName
than O
all O
RL O
baselines. O
DT-BERT B-MethodName
ob- O
tains O
higher O
QA B-MetricName
accuracies I-MetricName
than O
the O
DT B-MethodName
answer O
prediction O
"head;" O
it O
obtains O
the O
highest O
QA B-MetricName
accu- I-MetricName
racy I-MetricName
among O
all O
the O
models O
on O
random O
maps, O
and O
performs O
slightly O
worse O
than O
the O
DQN O
on O
fixed O
maps. O
Despite O
the O
Decision B-MethodName
Transformer’s I-MethodName
ability O
to O
learn O
long-term O
dependencies O
via O
its O
attention O
mechanism, O
we O
posit O
that O
the O
contextualised O
embed- O
dings O
of O
BERT B-MethodName
are O
able O
to O
model O
a O
richer O
semantic O
representation O
of O
TextWorld’s O
state-strings O
than O
the O
embeddings O
learnt O
by O
the O
DT. B-MethodName
This O
better O
capturing O
of O
the O
semantic O
space O
enables O
BERT B-MethodName
to O
more O
fully O
utilise O
the O
context O
with O
which O
it O
was O
provided O
by O
using O
pre-existing O
understanding O
to O
help O
answer O
questions O
posed O
in O
natural O
language. O
4.4 O
Rewards O
and O
Performance O
Based O
on O
validation O
set O
performance, O
the O
optimal O
initial B-HyperparameterName
return-to-go I-HyperparameterName
for I-HyperparameterName
location I-HyperparameterName
type I-HyperparameterName
questions I-HyperparameterName
was O
determined O
to O
be O
2 B-HyperparameterValue
for O
both O
fixed O
and O
random O
map O
settings. O
This O
is O
lower O
than O
for O
existence O
and O
attribute O
types, O
indicating O
that O
exploration O
is O
not O
as O
highly O
encouraged. O
In O
location O
type O
questions, O
the O
entity O
definitively O
exists O
somewhere O
within O
the O
environment. O
This O
means O
that O
the O
action O
space O
re- O
quired O
to O
answer O
questions O
of O
locality O
is O
reduced O
to O
traversals O
and O
basic O
interactions B-HyperparameterName
with O
contain- O
ers. O
Therefore, O
less O
exploration O
is O
needed O
as O
the O
information O
to O
answer O
a O
question O
is O
more O
easily O
ac-50quired. O
Too O
high O
an O
initial O
reward O
would O
promote O
unnecessary O
actions O
with O
a O
high O
likelihood O
of O
lead- O
ing O
the O
agent O
astray O
from O
stopping O
in O
the O
correct O
state. O
Existence O
questions O
require O
far O
more O
exploration O
of O
an O
environment O
than O
location O
type O
questions. O
Higher O
starting O
rewards O
reflect O
this O
need O
for O
greater O
exploration O
and O
are O
associated O
with O
better O
QA O
and O
sufficient B-MetricName
information I-MetricName
scores, I-MetricName
as O
seen O
in O
Table O
5 O
in O
the O
Appendix. O
These O
higher O
values O
promote O
a O
more O
complete O
traversal O
of O
the O
world, O
allowing O
for O
gathering O
information O
required O
to O
answer O
the O
ques- O
tion. O
However, O
too O
high O
an O
initial O
reward O
means O
that O
entering O
a O
correct O
state O
and O
receiving O
a O
reward O
of O
1 O
may O
not O
affect O
the O
model’s O
decision O
making. O
If O
the O
DT O
has O
a O
current O
RTG B-HyperparameterName
of O
5 B-HyperparameterValue
and O
enters O
the O
correct O
state O
that O
rewards O
1.0, O
the O
RTG B-HyperparameterName
from O
then O
onwards O
is O
4.0. B-HyperparameterValue
The O
return-to-go B-HyperparameterName
of O
4 B-HyperparameterValue
does O
not O
suggest O
to O
the O
model O
that O
it O
has O
entered O
the O
correct O
state, O
meaning O
it O
carries O
on O
exploring O
and O
gathering O
information. O
Likewise, O
too O
small O
a O
reward O
could O
prematurely O
cause O
an O
agent O
to O
stop O
exploring O
due O
to O
gaining O
rewards O
for O
entering O
new O
states O
via O
the O
exploration O
bonus. O
Therefore, O
we O
observe O
that O
the O
best O
RTG B-HyperparameterName
values O
err O
on O
the O
larger O
side, O
which O
en- O
courages O
greater O
world O
exploration. O
Attribute O
type O
questions O
are O
considered O
the O
most O
sparsely O
rewarded O
of O
all O
three O
types O
(Yuan O
et O
al., O
2019). O
We O
therefore O
expected O
higher O
rewards O
to O
be O
associated O
with O
better O
accuracies. O
The O
results, O
however, O
paint O
a O
different O
picture. O
In O
a O
fixed O
map, O
where O
the O
state O
space O
is, O
on O
average, O
smaller O
than O
that O
of O
random O
maps, O
we O
see O
that O
a O
smaller O
reward O
yields O
the O
best O
score. O
This O
reduction O
is O
likely O
a O
re- O
sult O
of O
the O
reduced O
state O
and O
action O
space O
making O
too O
much O
exploration O
and O
interaction O
with O
the O
envi- O
ronment O
degrade O
performance. O
On O
the O
other O
hand, O
in O
a O
random O
map O
setting O
higher O
rewards O
yields O
bet- O
ter O
QA O
and O
sufficient B-MetricName
information I-MetricName
scores, I-MetricName
allowing O
us O
to O
conclude O
that O
higher O
rewards O
promote O
more O
exploration O
and O
thus O
allows O
the O
model O
to O
better O
answer O
the O
question. O
4.5 O
Sample O
Efficiency O
The O
RL O
agents O
in O
QAit O
were O
trained O
for O
more O
than O
200K B-HyperparameterValue
episodes. B-HyperparameterName
In O
comparison, O
most O
of O
our O
De- B-MethodName
cision I-MethodName
Transformers I-MethodName
were O
trained O
on O
around O
40K B-HyperparameterValue
episodes B-HyperparameterName
(Table O
1). O
The O
test O
set O
results O
therefore O
show O
that O
DT B-MethodName
is O
able O
to O
match O
or O
outperform O
the O
pre- O
vious O
RL O
methods O
when O
trained O
on O
approximately O
25% B-HyperparameterValue
of O
the O
number O
of O
episodes. B-HyperparameterName
Moreover, O
all O
train- O
ing O
data O
used O
for O
the O
DT B-MethodName
was O
generated O
via O
randomrollouts O
=- O
indicating O
that O
the O
Decision B-MethodName
Transformer I-MethodName
has O
the O
ability O
to O
learn O
optimal O
policies O
from O
subop- O
timal O
data. O
We O
also O
found O
that O
fine-tuning O
a O
BERT B-MethodName
model O
for O
QA O
on O
the O
random O
rollout O
data O
works O
well, O
as O
long O
as O
the O
DT B-MethodName
is O
used O
to O
determine O
where O
to O
cut O
off O
the O
trajectory. O
In O
order O
to O
further O
elucidate O
the O
DT’s B-MethodName
sample O
efficient O
learning O
capabilities, O
we O
generated O
new O
datasets O
for O
all O
question O
and O
map O
types O
that O
only O
contained O
10 B-HyperparameterValue
thousand I-HyperparameterValue
episodes. B-HyperparameterName
The O
validation O
results O
can O
be O
seen O
Table O
6 O
in O
the O
Appendix. O
These O
experiments O
indicate O
that O
the O
DT B-MethodName
trained O
on O
even O
fewer O
offline O
trajectories O
can O
achieve O
results O
on O
par O
with O
or O
better O
than O
both O
previous O
baselines O
as O
well O
as O
identical O
models O
trained O
on O
more O
data. O
Here O
we O
see O
fixed O
map O
sufficient O
information O
scores O
being O
improved O
for O
all O
question O
types O
and O
QA B-MetricName
accuracy I-MetricName
in- O
creasing O
for O
attribute O
and O
existence O
questions. O
How- O
ever, O
QA B-MetricName
accuracy I-MetricName
for O
location O
type O
questions O
is O
worse O
than O
previous O
baselines O
in O
both O
random O
and O
fixed O
maps O
(see O
Table O
2). O
While O
the O
results O
are O
not O
consistently O
better, O
they O
do O
further O
indicate O
the O
sample O
efficiency O
of O
the O
Decision B-MethodName
Transformer. I-MethodName
5 O
Conclusion O
We O
showed O
that O
interactive O
question O
answering O
can O
be O
framed O
as O
a O
sequence O
modelling O
prob- O
lem O
by O
training O
Transformers O
for O
action O
genera- O
tion O
and O
answer O
prediction O
using O
random O
roll-outs. O
Results O
show O
that O
the O
Decision B-MethodName
Transformer I-MethodName
ap- O
proach O
matches O
or O
outperforms O
current O
reinforce- O
ment O
learning O
approaches O
for O
QAit B-TaskName
on O
most O
ques- O
tion O
types O
and O
maps O
type O
configurations O
in O
the O
500 O
game O
setting. O
Additionally, O
the O
approach O
is O
more O
sample O
efficient O
than O
reinforcement O
learning O
ap- O
proaches, O
reducing O
the O
amount O
of O
training O
data O
required O
even O
though O
the O
data O
generated O
via O
ran- O
dom O
rollouts O
is O
suboptimal. O
Fine-tuning O
a O
BERT B-MethodName
model O
for O
question O
answering O
on O
the O
same O
gener- O
ated O
dataset O
improves O
performance O
over O
using O
the O
Decision B-MethodName
Transformer I-MethodName
directly O
for O
question O
answer- O
ing O
in O
two O
of O
the O
three O
question O
types. O
6 O
Acknowledgements O
This O
work O
is O
based O
on O
research O
supported O
in O
part O
by O
the O
National O
Research O
Foundation O
of O
South O
Africa O
(Grant O
Number: O
129850). O
Some O
computations O
were O
performed O
using O
facilities O
provided O
by O
the O
University O
of O
Cape O
Town’s O
ICTS O
High O
Performance O
Computing. O
