Proceedings O
of O
the O
2nd O
Workshop O
on O
Deriving O
Insights O
from O
User-Generated O
Text O
, O
pages O
1 O
- O
9 O
May O
27, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
Unsupervised B-TaskName
Abstractive I-TaskName
Dialogue I-TaskName
Summarization I-TaskName
with O
Word B-MethodName
Graphs I-MethodName
and I-MethodName
POV I-MethodName
Conversion I-MethodName
Seongmin O
Park, O
Jihwa O
Lee O
ActionPower, O
Seoul, O
Republic O
of O
Korea O
{seongmin.park, O
jihwa.lee}@actionopwer.kr O
Abstract O
We O
advance O
the O
state-of-the-art O
in O
unsuper- B-TaskName
vised I-TaskName
abstractive I-TaskName
dialogue I-TaskName
summarization I-TaskName
by O
utilizing O
multi-sentence B-MethodName
compression I-MethodName
graphs. I-MethodName
Starting O
from O
well-founded O
assumptions O
about O
word O
graphs, O
we O
present O
simple O
but O
reli- O
able O
path-reranking O
and O
topic O
segmentation O
schemes. O
Robustness O
of O
our O
method O
is O
demon- O
strated O
on O
datasets O
across O
multiple O
domains, O
including O
meetings, O
interviews, O
movie O
scripts, O
and O
day-to-day O
conversations. O
We O
also O
iden- O
tify O
possible O
avenues O
to O
augment O
our O
heuristic- O
based O
system O
with O
deep O
learning. O
We O
open- O
source O
our O
code1, O
to O
provide O
a O
strong, O
repro- O
ducible O
baseline O
for O
future O
research O
into O
unsu- B-TaskName
pervised I-TaskName
dialogue I-TaskName
summarization. I-TaskName
1 O
Introduction O
Compared O
to O
traditional O
text O
summarization, O
di- O
alogue O
summarization O
introduces O
a O
unique O
chal- O
lenge: O
conversion O
of O
ﬁrst- O
and O
second-person O
speech O
into O
third-person O
reported O
speech. O
Such O
dis- O
crepancy O
between O
the O
observed O
text O
and O
expected O
model O
output O
puts O
greater O
emphasis O
on O
abstrac- O
tive O
transduction O
than O
in O
traditional O
summarization O
tasks. O
The O
disorientation O
is O
further O
exacerbated O
by O
each O
of O
many O
diverse O
dialogue O
types O
calling O
for O
a O
differing O
form O
of O
transduction O
– O
short O
dialogues O
re- O
quire O
terse O
abstractions, O
while O
meeting O
transcripts O
require O
summaries O
by O
agenda. O
Thus, O
despite O
the O
steady O
emergence O
of O
dialogue O
summarization O
datasets, O
the O
ﬁeld O
of O
dialogue O
sum- O
marization O
is O
still O
bottlenecked O
by O
a O
scarcity O
of O
training O
data. O
To O
train O
a O
truly O
robust O
dialogue O
sum- O
marization O
model, O
one O
requires O
transcript-summary O
pairs O
not O
only O
across O
diverse O
dialogue O
domains O
, O
but O
also O
across O
multiple O
dialogue O
types O
as O
well. O
The O
lack O
of O
diverse O
annotated O
summarization O
data O
is O
especially O
pronounced O
in O
low-resourced O
languages. O
From O
such O
state O
of O
the O
literature, O
we O
identify O
a O
need O
for O
unsupervised B-TaskName
dialogue I-TaskName
summarization. I-TaskName
1https://github.com/seongminp/graph-dialogue-summary O
Figure O
1: O
Our O
summarization O
pipeline. O
Our O
method O
builds O
upon O
previous O
research O
on O
unsupervised O
summarization O
using O
word O
graphs. O
Starting O
from O
the O
simple O
assumption O
that O
a O
good O
summary O
sentence O
is O
at O
least O
as O
informative O
as O
any O
single O
input O
sentence O
, O
we O
develop O
novel O
schemes O
for O
path O
extraction O
from O
word O
graphs. O
Our O
contri- O
butions O
are O
as O
follows: O
1.We O
present O
a O
novel O
scheme O
for O
path O
rerank- O
ing O
in O
graph-based O
summarization. O
We O
show O
that, O
in O
practice, O
simple O
keyword O
counting O
performs O
better O
than O
complex O
baselines. O
For O
longer O
texts, O
we O
present O
an O
optional O
topic O
seg- O
mentation O
scheme. O
2.We O
introduce O
a O
point-of-view O
(POV) O
conver- O
sion O
module O
to O
convert O
semi-extractive O
sum- O
maries O
into O
fully O
abstractive O
summaries. O
The O
new O
module O
by O
itself O
improves O
all O
scores O
on O
baseline O
methods, O
as O
well O
as O
our O
own. O
3.Finally, O
We O
verify O
our O
model O
on O
datasets O
be- O
yond O
those O
traditionally O
used O
in O
literature O
to O
provide O
a O
strong O
baseline O
for O
future O
research. O
With O
just O
an O
off-the-shelf O
part-of-speech O
(POS) O
tagger O
and O
a O
list O
of O
stopwords, O
our O
model O
can O
be O
applied O
across O
different O
types O
of O
dialogue O
summa- O
rization. O
2 O
Background O
2.1 O
Multi-sentence O
compression O
graphs O
Pioneered O
by O
Filippova O
(2010), O
a O
Multi-Sentence B-MethodName
Compression I-MethodName
Graph I-MethodName
(MSCG) I-MethodName
is O
a O
graph O
whose1Figure O
2: O
Construction O
of O
word O
graph. O
Red O
nodes O
and O
edges O
denote O
the O
selected O
summary O
path. O
Node O
highlighted O
in O
purple O
("Poodles") O
is O
the O
only O
non-stopword O
node O
included O
in O
the O
k-core O
subgraph O
of O
the O
word O
graph. O
We O
use O
nodes O
from O
the O
k-core O
subgraph O
as O
keyword O
nodes. O
All O
original O
sentences O
from O
the O
unabridged O
input O
is O
present O
as O
a O
possible O
path O
from O
vbostoveos. O
Paths O
that O
contain O
more O
information O
than O
those O
original O
paths O
are O
extracted O
as O
summaries. O
nodes O
are O
words O
from O
the O
input O
text O
and O
edges O
are O
coocurrance O
statistics O
between O
adjacent O
words. O
During O
preprocessing, O
words O
“<bos>” O
(beginning- O
of-sentence) O
and O
“<eos>” O
(end-of-sentence) O
are O
prepended O
and O
appended, O
respectively, O
to O
every O
input O
sentence. O
Thus, O
all O
sentences O
from O
the O
in- O
put O
are O
represented O
in O
the O
graph O
as O
a O
single O
path O
from O
the O
<bos> O
node O
(vbos) O
to O
the O
<eos> O
node O
(veos). O
Overlapping O
words O
among O
sentences O
will O
create O
intersecting O
paths O
within O
MSCG, B-MethodName
creating O
new O
paths O
from O
vbostoveos, O
unseen O
in O
the O
original O
text. O
Capturing O
these O
possibly O
shorter O
but O
informa- O
tive O
paths O
is O
the O
key O
to O
performant O
summarization O
with O
MSCGs. O
Ganesan O
et O
al. O
(2010) O
introduce O
an O
abstractive O
sentence O
generation O
method O
from O
word O
graphs O
to O
produce O
opinion O
summaries. O
Tixier O
et O
al. O
(2016) O
show O
that O
nodes O
with O
maximal O
neighbors O
– O
a O
con- O
cept O
captured O
by O
graph O
degeneracy O
– O
likely O
belong O
to O
important O
keywords O
of O
the O
document. O
Shortest O
paths O
fromvbostoveosare O
scored O
according O
to O
how O
many O
keyword O
nodes O
they O
contain. O
Subsequently, O
a O
budget-maximization O
scheme O
is O
introduced O
to O
ﬁnd O
the O
set O
of O
paths O
that O
maximizes O
the O
score O
sum O
within O
designated O
word O
count O
(Tixier O
et O
al., O
2017). O
We O
also O
adopt O
graph O
degeneracy O
to O
identify O
key- O
word O
nodes O
in O
MSCG. B-MethodName
2.2 O
Unsupervised O
Abstractive O
Dialogue O
Summarization O
Aside O
from O
MSCGs, B-MethodName
unsupervised O
dialogue O
sum- O
marization O
usually O
employ O
end-to-end O
neural O
ar-chitectures. O
Zhang O
et O
al. O
(2021) O
and O
Zou O
et O
al. O
(2021) O
utilize O
text O
variational B-MethodName
autoencoders I-MethodName
(V I-MethodName
AEs) I-MethodName
(Kingma O
and O
Welling, O
2014; O
Bowman O
et O
al., O
2016) O
to O
decode O
conditional O
or O
denoised O
abridgements. O
Fu O
et O
al. O
(2021) O
reformulate O
summary O
generation O
into O
a O
self-supervised O
task O
by O
equipping O
auxiliary O
objectives O
to O
the O
training O
architecture. O
Among O
end- O
to-end O
frameworks O
we O
only O
include O
Fu O
et O
al. O
(2021) O
as O
our O
baseline, O
because O
the O
brittle O
nature O
of O
train- O
ing O
text O
V B-MethodName
AEs, I-MethodName
coupled O
with O
the O
lack O
of O
detail O
on O
data O
and O
parameters O
used O
to O
train O
the O
models, O
ren- O
der O
V B-MethodName
AE-based I-MethodName
methods O
beyond O
reproducible. O
3 O
Summarization O
strategy O
In O
following O
subsections O
we O
outline O
our O
proposed O
summarization O
process. O
3.1 O
Word O
graph O
construction O
First, O
we O
assemble O
a O
word O
graph O
Gfrom O
the O
in- O
put O
text. O
We O
use O
a O
modiﬁed O
version O
of O
Filippova O
(2010)’s O
algorithm O
for O
graph O
construction: O
•LetSW O
be O
a O
set O
of O
stopwords O
and O
T= O
s0,s1,...be O
a O
sequence O
of O
sentences O
in O
the O
input O
text. O
•Decompose O
all O
si∈Tinto O
a O
sequence O
of O
POS-tagged O
words. O
si= O
(“bos”,“meta O
”),(wi,0,pos O
i,0),..., O
(wi,n−1,pos O
i,n−1),(“eos”,“meta O
”)(1) O
•For O
every O
(wi,j,pos O
i,j)∈sisuch O
thatwi/∈ O
SW O
andsi∈T, O
add O
a O
node O
vinG. O
If O
a2nodev/primewith O
the O
same O
lowercase O
word O
wi,k O
and O
tagposi,ksuch O
thatj/negationslash=kexists, O
pair O
(wi,j,pos O
i,j)withv/primeinstead O
of O
creating O
a O
new O
node. O
If O
multiple O
such O
matches O
exist, O
select O
the O
node O
with O
maximal O
overlapping O
context O
(wi,j−1andwi,j+1). O
•Add O
stopword O
nodes O
– O
(wi,j,pos O
i,j)∈sisuch O
thatwi,j∈SW O
andsi∈T– O
toGwith O
the O
algorithm O
described O
above. O
•For O
allsi∈T, O
add O
a O
directed O
edge O
be- O
tween O
node O
pairs O
that O
correspond O
to O
subse- O
quent O
words. O
Edge O
weight O
wbetween O
nodes O
v1andv2is O
calculated O
as O
follows: O
w/prime=freq(v1) O
+freq(v2) O
(/summationtext O
si∈Tdiff(i,v1,v2))−1(2) O
w/prime/prime=freq(v1)∗freq(v2) O
(3) O
w=w/prime/w/prime/prime(4) O
freq(v)is O
the O
number O
of O
words O
from O
original O
text O
mapped O
to O
node O
v.diff(i,v1,v2)is O
the O
absolute O
difference O
in O
word O
positions O
of O
v1 O
andv2withinsi: O
diff(i,v1,v2) O
=|k−j| O
(5) O
, O
wherewijandwikare O
words O
in O
sithat O
cor- O
respond O
to O
nodes O
v1andv2, O
respectively. O
In O
edge O
weight O
calculation, O
w/primefavors O
edges O
with O
strong O
cooccurrence, O
while O
w/prime/prime−1favors O
edges O
with O
greater O
salience, O
as O
measured O
by O
word O
frequency. O
It O
follows O
from O
above O
that O
only O
a O
single O
<bos> O
node O
and O
a O
single O
<eos> O
node O
will O
exist O
once O
the O
graph O
is O
completed. O
3.2 O
Keyword O
extraction O
The O
resulting O
graph O
from O
the O
previous O
step O
is O
a O
com- O
position O
that O
captures O
syntactic O
importance. O
Tra- O
ditional O
approaches O
utilize O
centrality O
measures O
to O
identify O
important O
nodes O
within O
word O
graphs O
(Mi- O
halcea O
and O
Tarau, O
2004; O
Erkan O
and O
Radev, O
2004). O
In O
this O
work O
we O
use O
graph O
degeneracy O
to O
extract O
keyword O
nodes. O
In O
a O
k-degenerate B-MethodName
word I-MethodName
graph, I-MethodName
words O
that O
belong O
to O
k-core O
nodes O
of O
the O
graph O
are O
considered O
to O
be O
keywords. O
We O
collect O
KW O
, O
a O
set O
of O
nodes O
belonging O
to O
the O
k-core O
subgraph. O
Thek-core O
of O
a O
graph O
is O
the O
maximally O
degenerate O
subgraph, O
with O
minimum O
degree O
of O
at O
least O
k. O
Figure O
3: O
Topic O
segmentation O
on O
AMI O
meeting O
ID O
ES2005b. O
Green O
bars O
indicate O
sentence O
boundaries O
with O
highest O
topic O
distance. O
3.3 O
Path O
threshold O
calculation O
Once O
keyword O
nodes O
are O
identiﬁed, O
we O
score O
ev- O
ery O
path O
from O
vbostoveosthat O
corresponds O
to O
a O
sentence O
from O
the O
original O
text. O
Contrary O
to O
previ- O
ous O
research O
into O
word-graph O
based O
summarization, O
we O
use O
a O
simple O
keyword O
coverage O
score O
for O
every O
path: O
Score O
i=|Vi∩KW| O
|KW|(6) O
, O
whereViis O
the O
set O
of O
all O
nodes O
in O
path O
pi, O
a O
rep- O
resentation O
of O
sentence O
si∈T, O
within O
the O
word O
graph. O
We O
calculate O
the O
path O
threshold O
t, O
the O
mean O
score O
of O
all O
sentences O
in O
the O
original O
text. O
Later, O
when O
summaries O
are O
extracted O
from O
the O
word O
graph, O
candidates O
with O
path O
score O
less O
than O
tare O
discarded. O
We O
also O
experimented O
with O
setting O
tas O
the O
mini- O
mum O
or O
maximum O
of O
all O
original O
path O
scores, O
but O
such O
conﬁgurations O
yielded O
inferior O
summaries O
in- O
ﬂuenced O
by O
outlier O
path O
scores. O
Our O
path O
score O
function O
is O
reminiscent O
of O
the O
diversity O
reward O
function O
in O
Shang O
et O
al. O
(2018). O
However, O
we O
use O
the O
function O
as O
a O
measure O
of O
cov- O
erage O
instead O
of O
diversity O
. O
More O
importantly, O
we O
utilize O
the O
score O
as O
means O
to O
extract O
a O
threshold O
based O
on O
all O
input O
sentences, O
which O
is O
signiﬁcantly O
different O
from O
Shang O
et O
al. O
(2018)’s O
utilization O
of O
the O
function O
as O
a O
monotonically O
increasing O
scorer O
in O
submodularity O
maximization. O
3.4 O
Topic O
segmentation O
For O
long O
texts, O
we O
apply O
an O
optional O
topic O
segmen- O
tation O
step. O
Our O
summarization O
algorithm O
is O
sepa- O
rately O
applied O
to O
each O
segmented O
text. O
Similar O
to O
path O
ranking O
in O
the O
next O
section, O
topics O
are O
deter- O
mined O
according O
to O
keyword O
frequency. O
For O
every3Dataset O
Domain O
Test O
ﬁles O
Dialogue O
length O
(chars) O
All O
character-level O
and O
word-level O
statistics O
are O
averaged O
over O
the O
test O
set O
and O
rounded O
to O
the O
nearest O
whole O
number. O
sentence O
in O
the O
input, O
we O
construct O
a O
topic O
cover- O
agevectorc, O
a O
zero-initialized O
row-vector O
of O
length O
|KW|. O
Each O
column O
of O
the O
row O
vector O
is O
a O
binary O
representation O
signaling O
the O
presence O
of O
a O
single O
element O
inKW O
. O
Topic O
coverage O
vector O
of O
a O
path O
containing O
two O
keywords O
from O
KW O
, O
for O
instance, O
would O
contain O
two O
columns O
with O
1. O
Every O
transition O
between O
sentences O
is O
a O
potential O
topic O
boundary. O
Since O
each O
sentence O
(and O
corre- O
sponding O
path) O
has O
an O
associated O
topic O
coverage O
vector, O
we O
quantify O
the O
topic O
distance O
dof O
a O
sen- O
tence O
with O
the O
next O
as O
the O
negative O
cosine O
distance O
of O
their O
topic O
vectors: O
di,i+1=−ci·ci+1 O
/bardblci/bardbl/bardblci+1/bardbl(7) O
Ifpis O
a O
hyperparameter O
representing O
the O
total O
number O
of O
topics, O
one O
can O
segment O
the O
original O
text O
atp−1sentence O
boundaries O
with O
the O
greatest O
topic O
distance. O
Alternatively, O
sentence O
boundaries O
with O
topic O
distance O
greater O
than O
a O
designated O
threshold O
can O
be O
selected O
as O
topic O
boundaries. O
For O
simplicity, O
we O
proceed O
with O
the O
former O
segmentation O
setup O
(top-pboundary) O
when O
necessary. O
3.5 O
Summary O
path O
extraction O
We O
generate O
a O
summary O
per-speaker. O
Our O
construc- O
tion O
of O
the O
word O
graph O
allows O
fast O
extraction O
of O
sub-graphs O
containing O
only O
nodes O
pertaining O
to O
ut- O
terances O
from O
a O
single O
speaker. O
For O
each O
speaker O
subgraph, O
we O
generate O
summary O
sentences O
as O
fol- O
lows: O
1.We O
obtainkshortest O
paths O
from O
vbostoveosby O
applying O
the O
k-shortest O
paths O
algorithm O
(Yen, O
1971) O
to O
our O
word O
graph. O
2.Iterating O
from O
the O
shortest O
path, O
we O
collect O
any O
paths O
with O
keyword O
coverage O
score O
above O
the O
threshold O
calculated O
in O
3.3.3.For O
each O
path O
found, O
we O
track O
the O
set O
of O
en- O
countered O
keywords O
in O
KW O
. O
We O
stop O
our O
search O
if O
all O
keywords O
in O
KW O
were O
encoun- O
tered, O
or O
a O
pre-deﬁned O
number O
of O
iterations O
(the O
search O
depth) O
is O
reached. O
A O
good O
summary O
has O
to O
be O
both O
concise O
and O
informative. O
Intuitively, O
edge O
weights O
of O
the O
pro- O
posed O
word O
graph O
captures O
the O
former, O
while O
key- O
word O
thresholding O
prioritizes O
the O
latter. O
3.6 O
POV O
conversion O
Finally, O
we O
convert O
our O
collected O
semi-extractive O
summaries O
into O
abstractive O
reported O
speech O
using O
a O
rule-based O
POV O
conversion O
module. O
We O
describe O
sentences O
extracted O
from O
our O
word O
graph O
as O
semi- O
extractive O
rather O
than O
extractive O
, O
to O
recognize O
the O
distinction O
between O
previously O
unseen O
sentences O
created O
from O
pieces O
of O
text, O
and O
sentences O
taken O
verbatim O
from O
the O
original O
text. O
Similar O
to O
exist- O
ingextract-then-abstract O
summarization O
pipelines O
(Mao O
et O
al., O
2021; O
Liu O
et O
al., O
2021), O
our O
method O
hinges O
on O
the O
assumption O
that O
the O
extractive O
path- O
reranking O
step O
will O
optimize O
for O
summary O
content O
, O
while O
the O
succeeding O
abstractive O
POV-conversion O
step O
will O
do O
so O
for O
summary O
style O
. O
FewSum O
(Bražin- O
skas O
et O
al., O
2020) O
also O
applies O
POV O
conversion O
in O
a O
few-shot O
summarization O
setting. O
FewSum O
condi- O
tions O
the O
summary O
generator O
to O
produce O
sentences O
in O
targeted O
styles, O
which O
is O
achieved O
by O
nudging O
the O
decoder O
to O
generate O
pronouns O
appropriate O
for O
each O
designated O
tone. O
Popular O
literature O
has O
established O
that O
deﬁning O
an O
all-encompassing O
set O
of O
rules O
for O
indirect O
speech O
conversion O
is O
infeasible O
(Partee, O
1973; O
Li, O
2011). O
In O
fact, O
the O
English O
grammar O
is O
mostly O
descrip- O
tive O
rather O
than O
prescriptive O
– O
no O
set O
of O
ofﬁcial O
rules O
dictated O
by O
a O
single O
governing O
authority O
ex- O
ists. O
All O
reported O
scores O
are O
F-1 B-MetricName
measures. I-MetricName
Models O
with O
POV O
indicate O
post-proceessing O
with O
our O
suggested O
POV O
conversion O
module. O
PreSeg O
models O
utilize O
topic O
segmentations O
provided O
in O
Shang O
et O
al. O
(2018), O
and O
TopicSeg O
models O
intake O
unsegmented O
raw O
transcripts O
and O
perform O
the O
topic O
segmentation O
algorithm O
suggested O
in O
this O
paper. O
Results O
for O
RepSum O
are O
quoted O
from O
the O
original O
paper. O
techniques, O
such O
as O
end-to-end O
Transformer O
net- O
works O
(Lee O
et O
al., O
2020). O
In O
this O
study, O
we O
limit O
our O
scope O
to O
rule-based O
conversion O
because O
only O
the O
rule-based O
system O
among O
all O
tested O
methods O
in O
Lee O
et O
al. O
(2020) O
confers O
to O
the O
unsupervised O
nature O
of O
this O
paper. O
We O
encourage O
further O
research O
into O
integrating O
more O
advanced O
reported O
speech O
conver- O
sion O
techniques O
into O
the O
abstractive O
summarization O
pipeline. O
In O
this O
work, O
we O
apply O
four O
conversion O
rules: O
1.Change O
pronouns O
from O
ﬁrst O
person O
to O
third O
person. O
2.Change O
modal O
verbs O
can,may, O
and O
must O
to O
could O
,might O
, O
and O
had O
to O
, O
respectively. O
3.Convert O
questions O
into O
a O
pre-deﬁned O
template: O
<Speaker> O
asks O
<utterance> O
. O
4.Fix O
subject-verb O
agreement O
after O
applying O
rules O
above. O
We O
notably O
omit O
prepend O
rules O
suggested O
in O
(Lee O
et O
al., O
2020), O
because O
the O
input O
domain O
of O
our O
sum- O
marization O
system O
is O
unbounded, O
unlike O
with O
task- O
oriented O
spoken O
commands O
for O
virtual O
assistants. O
We O
also O
leave O
tense O
conversion O
for O
future O
research. O
4 O
Experiments O
4.1 O
Datasets O
We O
test O
our O
model O
on O
dialogue O
summarization O
datasets O
across O
multiple O
domains:1.Meetings: B-DatasetName
AMI O
(McCowan O
et O
al., O
2005), O
ICSI B-DatasetName
(Janin O
et O
al., O
2003) O
2.Day-to-day B-DatasetName
conversations: I-DatasetName
DialogSum I-DatasetName
(Chen O
et O
al., O
2021b), O
SAMSum B-DatasetName
(Gliwa O
et O
al., O
2019) O
3. O
Interview: B-DatasetName
MediaSum I-DatasetName
(Zhu O
et O
al., O
2021) O
4.Screenplay: B-DatasetName
SummScreen I-DatasetName
(Chen O
et O
al., O
2021a) O
5. O
Debate: B-DatasetName
ADS I-DatasetName
(Fabbri O
et O
al., O
2021) O
Table O
1 O
provides O
detailed O
statistics O
and O
descrip- O
tions O
for O
each O
dataset. O
For O
AMI O
and O
ICSI, O
we O
conduct O
several O
abla- O
tion O
experiments O
with O
different O
components O
of O
our O
model O
omitted: O
semi-extractive O
summariza- O
tion O
without O
POV O
conversion O
is O
compared O
with O
fully-abstractive O
summarization O
with O
POV O
conver- O
sion; O
utilization O
of O
pre-segmented O
text O
provided O
by O
Shang O
et O
al. O
(2018) O
is O
compared O
with O
application O
of O
topic O
segmentation O
suggested O
in O
this O
paper. O
4.2 O
Baselines O
For O
meeting O
summaries, O
we O
compare O
our O
method O
with O
previous O
research O
on O
unsupervised O
dialogue O
summarization. O
Along O
with O
Filippova O
(2010), O
Shang O
et O
al. O
(2018), O
and O
Fu O
et O
al. O
(2021), O
we O
se- O
lect O
Boudin O
and O
Morin O
(2013) O
and O
Mehdad O
et O
al. O
(2013) O
as O
our O
baselines. O
All O
but O
Fu O
et O
al. O
(2021) O
are O
word O
graph-based O
summarizers. O
For O
all O
other O
categories, O
we O
choose O
LEAD-3 O
as O
our O
unsupervised O
baseline. O
All O
reported O
scores O
are O
F-1 O
measures. O
In O
our O
method, O
topic O
segmentation O
is O
applied O
to O
datasets O
with O
average O
transcription O
length O
greater O
than O
5,000 O
characters O
(MediaSum, O
SummScreen), O
and O
POV O
conversion O
is O
applied O
to O
all O
datasets. O
ﬁrst O
three O
sentences O
of O
a O
document O
as O
the O
sum- O
mary. O
Because O
summary O
distributions O
in O
several O
document O
types O
tend O
to O
be O
front-heavy O
(Grenander O
et O
al., O
2019; O
Zhu O
et O
al., O
2021), O
LEAD-3 O
provides O
a O
competitive O
extractive O
baseline O
with O
negligible O
computational O
burden. O
4.3 O
Evaluation O
We O
evaluate O
the O
quality O
of O
generated O
system O
sum- O
maries O
against O
reference O
summaries O
using O
standard O
ROUGE O
scores O
(Lin, O
2004). O
Speciﬁcally, O
we O
use O
ROUGE-1 B-MetricName
(R1), O
ROUGE-2 B-MetricName
( O
R2), O
and O
ROUGE-L B-MetricName
(RL) O
scores O
that O
respectively O
measure O
unigram, O
bi- O
gram, O
and O
longest O
common O
subsequence O
coverage. O
5 O
Results O
5.1 O
Meeting O
summarization O
Table O
2 O
records O
experimental O
results O
on O
AMI O
and O
ISCI O
datasets. O
In O
all O
categories, O
our O
method O
or O
a O
baseline O
augmented O
with O
our O
POV O
conversion O
module O
outperforms O
previous O
state-of-the-art. O
5.1.1 O
Effect O
of O
suggested O
path O
reranking O
Our O
proposed O
path-reranking O
without O
POV O
con- O
version O
yields O
semi-extractive O
output O
summaries O
competitive O
with O
abstractive O
summarization O
base- O
lines. O
Segmenting O
raw O
transcripts O
into O
topic O
groups O
with O
our O
method O
generally O
yields O
higher O
F-measures B-MetricName
than O
using O
pre-segmented O
transcripts O
in O
semi-extractive O
summarization. O
5.1.2 O
Effect O
of O
topic O
segmentation O
Summarizing O
pre-segmented O
dialogue O
transcripts O
results O
in O
higher O
R2, O
while O
applying O
our O
topic O
segmentation O
method O
results O
in O
higher O
R1and O
RL. O
This O
observation O
is O
in O
line O
with O
our O
method’s B-HyperparameterName
emphasis O
on O
keyword O
extraction, O
in O
contrast O
to O
keyphrase O
extraction O
seen O
in O
several O
baselines O
(Boudin O
and O
Morin, O
2013; O
Shang O
et O
al., O
2018). O
Models O
that O
preserve O
token O
adjacency O
achievehigherR2, O
while O
models O
that O
preserve O
token O
pres- O
ence O
achieve O
higher O
R1.RLadditionally O
penalizes O
for O
wrong O
token O
order, O
but O
token O
order O
in O
extracted O
summaries O
tend O
to O
be O
well-preserved O
in O
word O
graph- O
based O
summarization O
schemes. O
5.1.3 O
Effect O
of O
POV O
conversion O
module O
Our O
POV O
conversion O
module O
improves O
benchmark O
scores O
on O
all O
tested O
baselines, O
as O
well O
as O
on O
our O
own O
system. O
It O
is O
only O
natural O
that O
a O
conversion O
module O
that O
translates O
text O
from O
semi-extractive O
to O
abstrac- O
tive O
will O
raise O
scores O
on O
abstractive O
benchmarks. O
However, O
applying O
our O
POV O
module O
to O
already O
ab- O
stractive O
summarization O
systems O
resulted O
in O
higher O
scores O
in O
all O
cases. O
We O
attribute O
this O
to O
the O
fact O
that O
previous O
abstractive O
summarization O
systems O
do O
not O
generate O
sufﬁciently O
reportive O
summaries; O
past O
re- O
search O
either O
emphasize O
other O
linguistic O
aspects O
like O
hyponym O
conversion O
(Shang O
et O
al., O
2018), O
or O
treat O
POV O
conversion O
as O
a O
byproduct O
of O
an O
end-to-end O
summarization O
pipeline O
(Fu O
et O
al., O
2021). O
5.2 O
Day-to-day, O
interview, O
screenplay, O
and O
debate O
summarization O
Our O
method O
outperforms O
the O
LEAD-3 O
baseline O
on O
most O
benchmarks O
(Table O
3). O
The O
model O
shows O
con- O
sistent O
performance O
across O
multiple O
domains O
in O
R1 O
andRL, O
but O
shows O
greater O
inconsistency O
in O
R2. O
Variance O
in O
the O
latter O
metric O
can O
be O
attributed, O
as O
in O
5.1.2, O
to O
our O
model’s O
tendency O
to O
optimize O
for O
single O
keywords O
rather O
than O
keyphrases. O
Robust- O
ness O
of O
our O
model, O
as O
measured O
by O
consistency O
of O
ROUGE B-MetricName
measures O
across O
multiple O
datasets, O
is O
shown O
in O
Figure O
4. O
Notably, O
our O
method O
falters O
in O
the O
MediaSum O
benchmark. O
Compared O
to O
other O
benchmarks, O
Me- O
diaSum’s O
reference O
summaries O
display O
heavy O
posi- O
tional O
bias O
towards O
the O
beginning O
of O
its O
transcripts, O
which O
beneﬁts O
the O
LEAD-3 O
approach. O
It O
also O
is O
the O
only O
dataset O
in O
which O
references O
summaries O
are6Transcript O
Summary O
Maya: O
Bring O
home O
the O
clothes O
that O
are O
hanging O
outside O
Maya: O
All O
of O
them O
should O
be O
dry O
already O
and O
it O
looks O
like O
it’s O
going O
to O
rain O
Boris: O
I’m O
not O
home O
right O
now O
Boris: O
I’ll O
tell O
Brian O
to O
take O
care O
of O
that O
Maya: O
Fine, O
thanksbring O
home O
the O
clothes O
that O
are O
hanging O
outside O
boris O
’ll O
tell O
brian O
to O
take O
care O
of O
that O
Keywords: O
’care’, O
’clothes’, O
’home’, O
’thanks’ O
Megan: O
Are O
we O
going O
to O
take O
a O
taxi O
to O
the O
opera? O
Joseph: O
No, O
I’ll O
take O
my O
car. O
Megan: O
Great, O
more O
convenientare O
we O
going O
to O
take O
a O
taxi O
to O
the O
opera O
? O
no O
, O
joseph O
’ll O
take O
my O
car O
Keywords: O
’car’, O
’convenient’, O
’taxi’, O
’opera’ O
Anne: O
You O
were O
right, O
he O
was O
lying O
to O
me O
:/ O
Irene: O
Oh O
no, O
what O
happened? O
Jane: O
who? O
Jane: O
that O
Mark O
guy? O
Anne: O
yeah, O
he O
told O
me O
he’s O
30, O
today O
I O
saw O
his O
passport O
- O
he’s O
40 O
Irene: O
You O
sure O
it’s O
so O
important? O
Anne: O
he O
lied O
to O
me O
Irenehe O
lied O
to O
me O
he O
’s O
30 O
, O
today O
anne O
saw O
his O
passport O
- O
he O
’s O
40 O
yeah O
, O
he O
told O
me O
oh O
no O
, O
what O
happened? O
who O
? O
annerene O
he O
lied O
to O
me O
: O
/ O
Keywords: O
’guy’, O
’/’, O
’passport’, O
’yeah’, O
’today’ O
Table O
4: O
Summarizing O
the O
SAMSum O
corpus O
(Gliwa O
et O
al., O
2019). O
Figure O
4: O
Normalized O
standard O
deviation O
(also O
called O
coefﬁcient O
of O
variance) O
of O
R1, O
R2, O
and O
RL O
scores O
across O
all O
datasets. O
Normalized O
standard O
deviation O
is O
calcu- O
lated O
asσ/¯x, O
whereσis O
the O
standard O
deviation O
and O
¯xis O
the O
mean. O
not O
generated O
for O
the O
purpose O
of O
summary O
evalua- O
tion, O
but O
are O
scraped O
from O
source O
news O
providers. O
Reference O
summaries O
for O
MediaSum O
utilize O
less O
reported O
speech O
compared O
to O
other O
datasets, O
and O
thus O
our O
POV O
module O
fails O
to O
boost O
the O
precision O
of O
summaries O
generated O
by O
our O
model.6 O
Conclusion O
6.1 O
Improving O
MSCG O
summarization O
This O
paper O
improves O
upon O
previous O
work O
on O
multi- O
sentence O
compression O
graphs O
for O
summarization. O
We O
ﬁnd O
that O
simpler O
and O
more O
adaptive O
path O
rerank- O
ing O
schemes O
can O
boost O
summarization O
quality. O
We O
also O
demonstrate O
a O
promising O
possibility O
for O
inte- O
grating O
point-of-view O
conversion O
into O
summariza- O
tion O
pipelines. O
Compared O
to O
previous O
research, O
our O
model O
is O
still O
insufﬁcient O
in O
keyphrase O
or O
bigram O
preserva- O
tion. O
This O
phenomenon O
is O
captured O
by O
inconsistent O
R2scores O
across O
benchmarks. O
We O
believe O
incor- O
porating O
ﬁndings O
from O
keyphrase-based O
summariz- O
ers O
(Riedhammer O
et O
al., O
2010; O
Boudin O
and O
Morin, O
2013) O
can O
mitigate O
such O
shortcomings. O
6.2 O
Avenues O
for O
future O
research O
While O
our O
methods O
demonstrate O
improved O
bench- O
mark O
results, O
its O
mostly O
heuristic O
nature O
leaves O
much O
room O
for O
enhancement O
through O
integration O
of O
statistical O
models. O
POV O
conversion O
in O
particular O
can O
beneﬁt O
from O
deep O
learning-based O
approaches O
(Lee O
et O
al., O
2020). O
With O
recent O
advances O
in O
unsuper- O
vised O
sequence O
to O
sequence O
transduction O
(Li O
et O
al.,72020; O
He O
et O
al., O
2020), O
we O
expect O
further O
research O
into O
more O
advanced O
POV O
conversion O
techniques O
will O
improve O
unsupervised O
dialogue O
summariza- O
tion. O
Another O
possibility O
to O
augment O
our O
research O
with O
deep O
learning O
is O
through O
employing O
graph O
networks O
(Cui O
et O
al., O
2020) O
for O
representing O
MSCGs. O
With O
graph O
networks, O
each O
word O
node O
and O
edge O
can O
be O
represented O
as O
a O
contextualized O
vector. O
Such O
schemes O
will O
enable O
a O
more O
ﬂexible O
and O
interpolat- O
able O
manipulation O
of O
syntax O
captured O
by O
traditional O
word O
graphs. O
One O
notable O
shortcoming O
of O
our O
system O
is O
the O
generation O
of O
summaries O
that O
lack O
grammatical O
co- O
herence O
or O
ﬂuency O
(Table O
4). O
We O
intentionally O
leave O
out O
complex O
path O
ﬁlters O
that O
gauge O
linguistic O
va- O
lidity O
or O
factual O
correctness. O
We O
only O
minimally O
inspect O
our O
summaries O
to O
check O
for O
inclusion O
of O
verb O
nodes, O
as O
in O
Filippova O
(2010). O
Our O
system O
can O
be O
easily O
augmented O
with O
such O
additional O
ﬁlters, O
which O
we O
leave O
for O
future O
work. O