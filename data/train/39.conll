Findings O
of O
the O
Association O
for O
Computational O
Linguistics: O
ACL O
2022 O
, O
pages O
4174 O
- O
4186 O
May O
22-27, O
2022 O
c O
2022 O
Association O
for O
Computational O
Linguistics O
Towards O
Few-shot O
Entity O
Recognition O
in O
Document O
Images: O
A O
Label-aware B-TaskName
Sequence-to-Sequence I-TaskName
Framework I-TaskName
Zilong O
Wang O
University O
of O
California, O
San O
Diego O
zlwang@ucsd.eduJingbo O
Shang‚àó O
University O
of O
California, O
San O
Diego O
jshang@ucsd.edu O
Abstract O
Entity O
recognition O
is O
a O
fundamental O
task O
in O
understanding O
document O
images. O
Traditional O
sequence O
labeling O
frameworks O
treat O
the O
entity O
types O
as O
class O
IDs O
and O
rely O
on O
extensive O
data O
and O
high-quality O
annotations O
to O
learn O
semantics O
which O
are O
typically O
expensive O
in O
practice. O
In O
this O
paper, O
we O
aim O
to O
build O
an O
entity O
recognition O
model O
requiring O
only O
a O
few O
shots O
of O
annotated O
document O
images. O
To O
overcome O
the O
data O
limi- O
tation, O
we O
propose O
to O
leverage O
the O
label O
surface O
names O
to O
better O
inform O
the O
model O
of O
the O
target O
entity O
type O
semantics O
and O
also O
embed O
the O
labels O
into O
the O
spatial O
embedding O
space O
to O
capture O
the O
spatial O
correspondence O
between O
regions O
and O
labels. O
Specifically, O
we O
go O
beyond O
sequence O
la- O
beling O
and O
develop O
a O
novel O
label-aware B-TaskName
seq2seq I-TaskName
framework, O
LASER. B-MethodName
The O
proposed O
model O
fol- O
lows O
a O
new O
labeling O
scheme O
that O
generates O
the O
label O
surface O
names O
word-by-word O
explicitly O
after O
generating O
the O
entities. O
During O
training, O
LASER B-MethodName
refines O
the O
label O
semantics O
by O
updating O
the O
label O
surface O
name O
representations O
and O
also O
strengthens O
the O
label-region O
correlation. O
In O
this O
way, O
LASER B-MethodName
recognizes O
the O
entities O
from O
docu- O
ment O
images O
through O
both O
semantic O
and O
layout O
correspondence. O
Extensive O
experiments O
on O
two O
benchmark O
datasets O
demonstrate O
the O
superiority O
of O
LASER B-MethodName
under O
the O
few-shot O
setting. O
1 O
Introduction O
Entity O
recognition O
lies O
in O
the O
foundation O
of O
docu- O
ment O
image O
understandings, O
which O
aims O
at O
extract- O
ing O
word O
spans O
that O
perform O
certain O
roles O
from O
the O
document O
images, O
such O
as O
header O
,question O
. O
Dis- O
tinct O
from O
the O
text-only O
named O
entity O
recognition O
task, O
the O
document O
images, O
such O
as O
forms, O
tables, O
receipts, O
and O
multi-columns, O
provide O
a O
perfect O
sce- O
nario O
to O
apply O
multi-modal O
techniques O
into O
practice O
where O
the O
rich O
layout O
formats O
in O
such O
document O
images O
serve O
as O
the O
new, O
complementary O
signals O
for O
entity O
recognition O
performance O
in O
addition O
to O
the O
existing O
textual O
data. O
‚àóJingbo O
Shang O
is O
the O
corresponding O
author.Recent O
methods O
(Xu O
et O
al., O
2020; O
Hong O
et O
al., O
2020; O
Garncarek O
et O
al., O
2021) O
follow O
the O
tradi- O
tional O
sequence O
labeling O
framework O
to O
extract O
the O
word O
spans O
using O
the O
standard O
IOBES O
tagging O
schemes O
(Marquez O
et O
al., O
2005; O
Ratinov O
and O
Roth, O
2009) O
in O
named O
entity O
recognition O
tasks. O
Entity O
types O
are O
treated O
as O
class O
IDs O
and O
the O
semantics O
of O
the O
label O
surface O
names O
are O
ignored. O
These O
meth- O
ods O
also O
largely O
extend O
the O
label O
space O
by O
including O
combinations O
of O
the O
boundary O
identifiers O
( O
B,I,E, O
S) O
and O
entity O
types. O
For O
instance, O
when O
there O
are O
3 O
target O
entity O
types, O
the O
extended O
label O
space O
would O
have13(i.e.,4√ó3+1) O
dimensions. O
As O
a O
result, O
they O
fail O
to O
learn O
from O
the O
data O
efficiently O
and O
require O
extensive O
datasets O
and O
high-quality O
annotations O
to O
create O
the O
connection O
between O
entities O
and O
their O
en- O
tity O
types. O
Meanwhile, O
document O
images O
typically O
include O
various O
formats O
and O
have O
a O
high O
diversity O
of O
entities O
within O
each O
page. O
It O
is O
expensive O
or O
almost O
impossible O
to O
enumerate O
all O
required O
entity O
types O
and O
obtain O
enough O
annotated O
data O
for O
them. O
More- O
over, O
ethical O
concerns O
would O
arise O
when O
it O
comes O
to O
the O
receipts O
or O
consent O
forms, O
which O
makes O
it O
even O
harder O
to O
collect O
enough O
data. O
Due O
to O
the O
inefficiency O
of O
traditional O
methods O
and O
the O
data O
limitation O
in O
real O
application O
scenarios, O
it O
is O
necessary O
to O
resort O
to O
few-shot O
learning O
for O
entity O
recognition O
in O
document O
images. O
We O
aim O
at O
exploiting O
the O
potential O
of O
a O
limited O
number O
of O
training O
pages O
and O
try O
to O
generalize O
our O
model O
on O
the O
much O
larger O
number O
of O
new O
pages O
for O
testing. O
In O
our O
method, O
we O
go O
beyond O
the O
sequence O
labeling O
framework O
and O
reformulate O
the O
entity O
recognition O
as O
a O
sequence-to-sequence O
task. O
Specifically, O
we O
propose O
a O
new O
generative O
labeling O
scheme O
for O
entity O
recognition O
‚Äî O
the O
label O
surface O
name O
is O
explicitly O
generated O
right O
after O
each O
entity O
as O
a O
part O
of O
the O
target O
sequence. O
In O
this O
way, O
different O
entity O
types O
are O
no O
longer O
independent O
dimensions O
in O
the O
la- O
bel O
space O
and O
models O
can O
leverage O
the O
semantic O
connect O
between O
the O
entities O
and O
entity O
types.4174To O
this O
end, O
we O
propose O
a O
label- O
aware O
sequence- O
to-sequence O
framework O
for O
entity O
recognition, O
LASER. B-MethodName
Our O
implementation O
is O
based O
on O
pre- O
trained O
language O
model O
LayoutReader1(Wang B-MethodName
et O
al., O
2021), O
which O
is O
a O
layout-aware B-TaskName
pre-trained I-TaskName
sequence-to-sequence I-TaskName
model. O
As O
shown O
in O
Figure O
1, O
LASER B-MethodName
extends O
the O
ar- O
chitecture O
of O
LayoutReader O
for O
our O
proposed O
gen- O
erative O
labeling O
scheme O
to O
better O
solve O
the O
few- O
shot O
entity O
recognition O
task O
for O
document O
images. O
Specifically, O
after O
generating O
certain O
word O
spans, O
the O
model O
can O
choose O
to O
generate O
either O
the O
follow- O
ing O
words O
in O
the O
source O
sequence O
or O
label O
surface O
names. O
The O
entity O
labels O
are O
explicitly O
inserted O
in O
the O
generated O
sequence O
so O
that O
the O
probabil- O
ity O
of O
the O
entity O
types O
conditioned O
on O
the O
entity, O
P(type|entity O
), O
can O
be O
maximized O
not O
only O
by O
the O
signals O
from O
the O
training O
data O
but O
also O
by O
the O
knowl- O
edge O
from O
the O
pre-training O
of O
the O
language O
models. O
We O
also O
embed O
the O
label O
surface O
names O
into O
the O
spa- O
tial O
embedding O
space, O
so O
the O
generation O
of O
labels O
is O
also O
aware O
of O
the O
correlation O
between O
labels O
and O
the O
regions O
in O
the O
page. O
Benefit O
from O
the O
novel O
generative O
labeling O
scheme O
and O
the O
semantics O
of O
labels, O
LASER B-MethodName
is O
able O
to O
effectively O
recognize O
entities O
in O
document O
images O
with O
only O
a O
limited O
number O
of O
training O
sam- O
ples. O
In O
contrast, O
the O
sequence O
labeling O
models O
use O
less O
efficient O
tagging O
scheme, O
thus O
requiring O
more O
data O
and O
failing O
in O
the O
few-shot O
settings. O
We O
validate O
LASER B-DatasetName
using O
two O
benchmarks, O
FUNSD O
(Guillaume O
Jaume, O
2019) O
and O
CORD- O
Lv1 O
(Park O
et O
al., O
2019). O
Both O
datasets O
are O
from O
real O
scenarios O
and O
fully-annotated O
with O
textual O
con- O
tents O
and O
bounding O
boxes. O
We O
compare O
our O
model O
with O
strong O
baselines O
and O
study O
the O
label-entity O
se- O
mantic O
and O
spatial O
correlations. O
We O
summarize O
our O
contribution O
as O
follows. O
‚Ä¢We O
reformulate O
the O
entity O
recognition O
task O
and O
propose O
a O
new O
generative O
labeling O
scheme O
that O
embeds O
the O
label O
surface O
names O
into O
the O
target O
sequence O
to O
explicitly O
inform O
the O
model O
of O
the O
label O
semantics. O
‚Ä¢We O
propose O
a O
novel O
label-aware B-TaskName
sequence-to- I-TaskName
sequence I-TaskName
framework O
LASER B-DatasetName
to O
better O
handle O
few-shot O
entity O
recognition O
tasks O
for O
document O
images O
than O
the O
traditional O
sequence O
labeling O
framework O
using O
both O
label O
semantics O
and O
layout O
format O
learning. O
‚Ä¢Extensive O
experiments O
on O
two O
benchmark O
1Licensed O
under O
the O
MIT B-DatasetName
Licensedatasets O
demonstrate O
the O
effectiveness O
of O
LASER B-MethodName
under O
few-shot O
settings. O
Reproducibility. O
We O
will O
release O
the O
code O
and O
datasets O
on O
Github2. O
2 O
Problem O
Formulation O
The O
few-shot O
entity O
recognition O
in O
the O
document O
images O
is O
to O
take O
the O
text O
and O
layout O
inputs O
from O
a O
limited O
number O
of O
training O
samples O
to O
predict O
the O
boundary O
of O
each O
entity O
and O
classify O
the O
en- O
tity O
into O
categories. O
Given O
a O
document O
image O
page O
P, O
the O
words O
within O
the O
page O
are O
annotated O
with O
their O
textual O
contents O
wand O
the O
bounding O
boxes O
B= O
(x0, O
y0, O
x1, O
y1)(top-left O
and O
bottom-right O
cor- O
ners) O
by O
human O
annotators O
or O
the O
OCR O
engines, O
and O
all O
the O
words O
and O
bounding O
boxes O
are O
listed O
in O
a O
sequence O
serving O
as O
the O
inputs O
from O
textual O
and O
layout O
modalities. O
In O
this O
way, O
the O
entities O
are O
spans O
of O
these O
words O
referring O
to O
precise O
concepts, O
which O
makes O
it O
possible O
to O
conduct O
entity O
recogni- O
tion O
using O
sequence O
labeling O
or O
generative O
labeling O
scheme. O
We O
randomly O
select O
a O
small O
subset O
of O
train- O
ing O
samples O
and O
evaluate O
the O
performance O
under O
thek-shot O
training, O
where O
kdenotes O
the O
number O
of O
the O
training O
samples. O
3 O
Our O
Generative O
Labeling O
Scheme O
We O
propose O
our O
labeling O
scheme O
of O
entity O
recog- O
nition O
in O
the O
generative O
manner O
which O
generates O
the O
entity O
boundaries O
and O
the O
label O
surface O
names O
explicitly. O
Specifically, O
given O
an O
entity O
e= O
[wi, O
wi+1..., O
w O
j], O
we O
use O
the O
[B] O
and[E] O
to O
de- O
note O
the O
boundary O
of O
the O
entity O
and O
append O
the O
label O
surface O
name O
afterwards. O
Overall, O
the O
generative O
formulation O
is O
to O
generate: O
wi‚àí1,[B], O
wi, O
..., O
w O
j,[E], O
œÑ1, O
..., O
œÑ O
k,[T], O
wj+1 O
where O
[B] O
and[E] O
denote O
the O
start O
and O
end O
of O
the O
entity; O
œÑ1...œÑkare O
the O
words O
in O
the O
label O
surface O
name; O
[T] O
denotes O
the O
end O
of O
the O
label O
surface O
name. O
For O
example, O
‚Äú O
Sender O
‚Äù O
and O
‚Äú O
Charles O
Dug- O
gan‚Äù O
are O
a O
pair O
of O
question O
andanswer O
from O
a O
doc- O
ument O
image. O
According O
to O
the O
generative O
labeling O
scheme, O
the O
corresponding O
generated O
sequence O
is O
that:[B] O
Sender O
[E] O
question O
[T] O
[B] O
Charles O
Duggan O
[E] O
answer O
[T]. O
4 O
Our O
LASER O
Framework O
In O
this O
section, O
we O
introduce O
our O
label-aware O
sequence-to-sequence O
framework O
for O
entity O
recog- O
2github.com/zlwang-cs/LASER-release4175[SOS]W1BPADB1B2B3B4B5BPADB1B[B]B2B3B4W2W3W4W5[EOS]W1[B]W2W3W4B[E][E]BùúèùúèB[T][T]B5W5BPAD[EOS]012345678910111213141516PositionalEmbeddingSpatialEmbeddingWordEmbeddingSource O
EmbeddingsTarget O
EmbeddingstgtsrctgtAllow O
to O
attendPrevent O
from O
attendingsrcLASERLabel-Aware O
Sequence-to-Sequence O
for O
Entity O
RecognitionH8H9H10H11H12H13H14H15‚ààùêíùê´ùêú.Target O
Hidden O
StatesNext O
Token O
TypeClassificationNext O
TokenPredictionH7 O
‚àâùêíùê´ùêú.‚àâùêíùê´ùêú.W1W2W3W4W5Src. O
WordsW4 O
[B][E][T]Func. O
Tokens[T]Label O
Surface O
Namesùùâùúè πùúè π πùùâSenderCharles O
Duggan O
Source: O
... O
Sender, O
Charles, O
Duggan...Target: O
... O
[B]Sender[E]question[T] O
[B]Charles O
Duggan[E]answer[T]Generate O
Label O
Surface O
Names O
after O
Entities... O
[B]Sender[E]questionanswerheader[T]Insert O
Spatial O
Identifiers O
into O
Spatial O
Embedding O
Space O
(a)(b)SpatialEmbSpatialIDFigure O
1: O
The O
Framework O
of O
LASER: B-MethodName
[B],[E],[T] O
denote O
the O
boundaries; O
œÑ,œÑ‚Ä≤,œÑ‚Ä≤‚Ä≤are O
the O
label O
surface O
names; O
(a) O
is O
the O
process O
of O
generative O
labeling O
scheme; O
(b) O
shows O
the O
alignment O
of O
the O
spatial O
identifiers O
and O
embeddings. O
nition O
in O
document O
images. O
First, O
we O
introduce O
our O
method O
in O
a O
bird‚Äôs O
eye O
view. O
Then O
we O
dive O
into O
the O
details O
of O
each O
part O
including O
the O
multi-modal O
prefix O
language O
model, O
the O
label-aware O
generation. O
4.1 O
Overview O
Our O
proposed O
LASER O
is O
a O
label-aware O
sequence- O
to-sequence O
model O
for O
entity O
recognition O
in O
docu- O
ment O
images. O
The O
framework O
is O
shown O
in O
Figure O
1. O
The O
model O
follows O
the O
prefix O
language O
model O
paradigm O
(Raffel O
et O
al., O
2019; O
Dong O
et O
al., O
2019a; O
Bao O
et O
al., O
2020) O
and O
is O
built O
upon O
the O
pre-trained O
language O
model, O
LayoutReader O
(Wang O
et O
al., O
2021). O
With O
extensive O
knowledge O
learned O
in O
pre-training O
stage, O
the O
model O
leverages O
the O
semantic O
meaning O
of O
label O
surface O
names O
during O
generation. O
Since O
the O
functional O
tokens O
(e.g. O
[B],[E]) O
and O
the O
label O
surface O
names O
are O
foreign O
words O
in O
the O
given O
page, O
their O
layout O
features O
are O
nonexistent. O
We O
use O
trainable O
vectors O
as O
special O
layout O
identi- O
fiers O
for O
these O
extra O
tokens O
and O
these O
vectors O
are O
well O
aligned O
into O
the O
spatial O
embedding O
space. O
In O
this O
way, O
the O
spatial O
correspondence O
between O
lay- O
out O
formats O
and O
labels O
can O
be O
learned. O
To O
reinforce O
the O
model O
to O
distinguish O
the O
func- O
tional O
tokens O
(e.g. O
[B],[E]) O
and O
ordinary O
words, O
an O
extra O
binary O
classification O
module O
is O
added, O
and O
the O
probability O
is O
used O
in O
the O
next O
token O
prediction. O
Equipped O
with O
all O
the O
components, O
our O
proposed O
model O
is O
able O
to O
conduct O
entity O
recognition O
effi-ciently O
and O
effectively O
under O
the O
few-shot O
setting. O
4.2 O
Multi-modal O
Prefix O
LM O
LASER B-MethodName
is O
built O
on O
the O
layout-aware O
prefix O
lan- O
guage O
model, O
LayoutReader O
(Wang O
et O
al., O
2021). O
Prefix O
language O
model O
refers O
to O
a O
multi-layered O
Transformer O
where O
the O
source O
sequence O
and O
tar- O
get O
sequence O
are O
packed O
together O
and O
a O
‚Äúpartially- O
triangle‚Äù O
mask O
is O
used O
to O
control O
the O
attention O
be- O
tween O
tokens O
in O
the O
two O
sequences. O
In O
LASER, B-MethodName
the O
source O
sequence O
has O
full O
self-attention O
and O
the O
target O
sequence O
only O
attends O
to O
the O
previous O
tokens O
so O
the O
conditional O
generative O
probability O
is O
learned. O
Input O
Embedding O
The O
input O
embedding O
layer O
of O
LASER B-MethodName
includes O
the O
word O
embedding, O
spatial O
em- O
bedding, O
and O
positional O
embedding. O
We O
normalize O
and O
round O
the O
bounding O
box O
coordinates O
to O
inte- O
gers O
ranging O
from O
0 O
to O
1000, O
and O
embed O
them O
as O
trainable O
vectors O
as O
spatial O
embeddings O
(Xu O
et O
al., O
2020, O
2021a,b; O
Wang O
et O
al., O
2021). O
So O
the O
input O
embeddings O
of O
the O
ordinary O
words O
are O
as O
follows: O
ewi=WordEmb O
(wi) O
+ O
SpatialEmb O
(Bi) O
+ O
PosEmb O
(i) O
where O
WordEmb, O
SpatialEmb, O
PosEmb O
are O
the O
word O
embedding, O
the O
spatial O
embedding, O
and O
the O
positional O
embedding O
lookup O
tables, O
respectively; O
i O
is O
the O
index O
of O
the O
word O
in O
the O
packed O
sequence. O
The O
functional O
tokens O
and O
label O
surface O
names O
are O
new O
tokens O
in O
the O
given O
page. O
We O
cannot O
ex- O
tract O
the O
layout O
features O
from O
the O
bounding O
boxes O
of4176them O
because O
their O
bounding O
boxes O
are O
nonexistent. O
Instead O
of O
the O
actual O
bounding O
boxes, O
we O
design O
unique O
embedding O
vectors O
for O
each O
new O
tokens O
as O
their O
layout O
identifiers. O
These O
identifiers O
can O
per- O
form O
in O
the O
same O
way O
as O
real O
bounding O
boxes O
dur- O
ing O
training O
to O
embed O
the O
functional O
tokens O
and O
la- O
bel O
surface O
names O
into O
the O
spatial O
embedding O
space. O
The O
input O
embedding O
replaces O
the O
spatial O
embed- O
ding O
with O
the O
spatial O
identifiers: O
eŒª=WordEmb O
(Œª) O
+ O
SpatialID O
(Œª) O
+ O
PosEmb O
(i) O
where O
SpatialID O
is O
the O
spatial O
identifier O
lookup O
ta- O
ble;iis O
the O
index O
of O
the O
word O
in O
the O
packed O
se- O
quence; O
Œª‚àà O
{[B],[E],[T], O
œÑ1, O
..., O
œÑ O
t}. O
Within O
the O
input O
embedding O
layer, O
the O
pre- O
trained O
model O
learns O
the O
semantic O
and O
layout O
for- O
mats O
from O
word O
embeddings O
or O
spatial O
features. O
The O
spatial O
embeddings O
are O
already O
pre-trained O
and O
further O
fine-tuned O
in O
the O
downstream O
tasks, O
and O
the O
spatial O
identifiers O
are O
new O
to O
the O
model O
and O
com- O
pletely O
trained O
in O
the O
downstream O
tasks. O
Attention O
Mask O
As O
mentioned, O
LASER B-MethodName
depends O
on O
a O
‚Äúpartially-triangle‚Äù O
mask O
to O
realize O
sequence- O
to-sequence O
training O
within O
one O
encoder. O
To O
be O
more O
specific, O
the O
‚Äúpartially-triangle‚Äù O
attention O
mask O
has O
two O
parts, O
the O
source O
part O
and O
the O
tar- O
get O
part. O
In O
the O
source O
part, O
the O
tokens O
can O
attend O
to O
each O
other, O
which O
enables O
the O
model O
to O
be O
aware O
of O
the O
entire O
sequence. O
In O
the O
target O
part, O
to O
predict O
the O
next O
token O
in O
a O
sequence-to-sequence O
way, O
we O
de- O
sign O
the O
‚Äútriangle‚Äù O
mask O
which O
prevents O
the O
tokens O
from O
attending O
to O
the O
tokens O
after O
them. O
There- O
fore, O
the O
generative O
probability O
conditioned O
on O
the O
previous O
tokens O
can O
be O
computed. O
Output O
Hidden O
States O
To O
learn O
the O
conditional O
generative O
probability O
of O
the O
next O
token, O
we O
take O
the O
output O
hidden O
states O
corresponding O
to O
the O
target O
se- O
quence O
which O
is O
denoted O
as O
hn+1,hn+2, O
...,hn+m, O
where O
n+ O
1is O
the O
beginning O
of O
the O
target O
sequence O
in O
the O
packed O
sequence. O
According O
to O
the O
‚Äúpartially- O
triangle‚Äù O
attention O
mask, O
hn+kis O
produced O
with O
the O
attention O
to O
the O
source O
tokens O
and O
the O
previous O
tar- O
get O
tokens, O
i.e., O
the O
input O
embeddings O
whose O
index O
ranges O
from O
1ton+k. O
Therefore, O
hn+kis O
used O
to O
predict O
the O
(k+ O
1) O
-th O
token O
in O
the O
target O
sequence. O
4.3 O
Label-aware B-TaskName
Generation I-TaskName
In I-TaskName
the I-TaskName
sequence-to-sequence I-TaskName
setting, O
LASER B-HyperparameterName
esti- O
mates O
the O
probability O
of O
next O
token O
conditioned O
on O
the O
previous O
context, O
i.e. O
P(xk|x<k)andxk‚àà O
C, O
where O
C={w1...wn} O
‚à™ O
{ O
œÑ1...œÑt} O
‚à™ O
{[B],[E],[T]}is O
the O
set O
of O
all O
candidate O
words. O
Following O
LayoutReader, O
we O
restrain O
the O
candi- O
dates O
within O
the O
source O
words O
instead O
of O
the O
whole O
dictionary, O
and O
we O
go O
beyond O
it O
and O
extend O
the O
can- O
didate O
set O
to O
include O
the O
functional O
tokens O
and O
label O
surface O
names. O
Moreover, O
to O
distinguish O
whether O
the O
next O
word O
belongs O
to O
the O
source O
or O
not, O
we O
design O
an O
extra O
binary O
classification O
module. O
Specifically, O
we O
take O
the O
hidden O
states O
hkto O
pre- O
dict O
whether O
the O
next O
token O
is O
from O
the O
source O
or O
not. O
We O
denote O
the O
probability O
P(xk+1‚ààsrc) O
= O
pk+1. O
Then O
we O
use O
pk+1to O
weight O
the O
next O
token O
prediction. O
The O
probability O
that O
the O
next O
token O
is O
thei-th O
word O
in O
the O
source O
is O
computed O
as O
follows: O
P(xk+1=wi|x‚â§k) O
=pk+1exp  O
eT O
wihk+bk O
P O
jexp O
eTwjhk+bk O
where O
wiis O
the O
i-th O
word O
in O
the O
source; O
ewiis O
the O
input O
embedding O
of O
wi;bkis O
the O
bias. O
Similarly, O
the O
probability O
that O
the O
next O
token O
is O
one O
of O
the O
functional O
tokens O
or O
label O
surface O
names O
is O
computed O
as O
follows: O
P(xk+1=Œª|x‚â§k) O
=(1‚àípk+1) O
exp  O
eT O
Œªhk+b‚Ä≤ O
k O
P O
Œª‚Ä≤exp  O
eT O
Œª‚Ä≤hk+b‚Ä≤ O
k O
where O
Œªis O
a O
functional O
token O
or O
label O
surface O
name, O
i.e.Œª‚àà O
{[B],[E],[T], O
œÑ1, O
..., O
œÑ O
t};1‚àípk+1is O
the O
probability O
that O
(k+ O
1) O
-th O
token O
is O
a O
functional O
token O
or O
label O
surface O
name; O
b‚Ä≤ O
kis O
the O
bias. O
Label O
Semantics O
Learning O
With O
the O
log O
like- O
lihood O
loss O
of O
generative O
language O
modeling, O
the O
model O
maximize O
the O
dot O
production O
between O
the O
hidden O
states O
hand O
the O
input O
embeddings O
e. O
The O
semantic O
correlation O
is O
learned O
considering O
that O
the O
input O
embeddings O
of O
the O
labels O
surface O
names O
are O
encoded O
in O
the O
word O
embeddings. O
Spatial O
Identifier O
Learning O
From O
the O
layout O
for- O
mat O
perspective, O
the O
input O
embedding O
of O
the O
la- O
bel O
surface O
names O
also O
includes O
the O
spatial O
iden- O
tifiers. O
When O
predicting O
the O
next O
token, O
the O
log O
likelihood O
also O
strengthens O
the O
relation O
between O
the O
spatial O
identifiers O
and O
the O
layout O
context. O
In O
this O
way, B-HyperparameterName
LASER I-HyperparameterName
inserts O
the O
spatial O
identifiers O
into O
the O
hyperspace O
of O
the O
spatial O
embeddings. O
In O
other O
words, B-HyperparameterName
LASER I-HyperparameterName
predicts O
where O
a O
certain O
label O
is O
more O
likely O
to O
be. O
Similar O
to O
the O
joint O
probability O
of O
language O
modeling, O
LASER O
maximizes O
the O
joint O
probability O
of O
a O
mixture O
of O
spatial O
identifiers O
and4177spatial O
embeddings: O
P(..., O
B O
k‚àí1, O
Bk, O
œÑ, O
B O
k+1, O
...) O
where O
Bkis O
the O
bounding O
boxes O
of O
the O
words O
in O
the O
page O
and O
the O
œÑis O
the O
label O
to O
predict. O
Further O
visualization O
is O
conducted O
in O
Section O
5.7. O
4.4 O
Sequential O
Decoding O
After O
training, B-HyperparameterName
LASER I-HyperparameterName
follows O
the O
prefix O
language O
modeling O
paradigm O
and O
generates O
the O
target O
se- O
quence O
sequentially. O
We O
input O
the O
source O
sequence O
into O
the O
model O
and O
take O
the O
last O
hidden O
states O
to O
pre- O
dict O
the O
first O
token O
in O
the O
target. O
Then O
we O
append O
the O
result O
to O
the O
end O
of O
input O
and O
repeatedly O
run O
the O
generation. O
We O
cache O
the O
states O
of O
the O
model O
and O
achieve O
generation O
in O
linear O
time. O
5 O
Experiments O
In O
this O
section, O
we O
conduct O
experiments O
and O
abla- O
tion O
study O
on O
FUNSD O
(Guillaume O
Jaume, O
2019) O
and O
CORD-Lv1 O
(Park O
et O
al., O
2019) O
under O
few-shot O
settings. O
We O
replace O
the O
original O
label O
surface O
names O
with O
other O
tokens O
to O
study O
the O
importance O
of O
semantic O
meaning. O
We O
also O
plot O
the O
heatmaps O
of O
the O
similarity O
between O
the O
spatial O
identifiers O
and O
the O
spatial O
embeddings O
to O
interpret O
the O
spatial O
cor- O
respondence. O
Case O
studies O
are O
also O
conducted. O
5.1 O
Experimental O
Setups O
All O
the O
experiments O
are O
under O
few-shot O
settings O
using O
1, O
2, O
3, O
4, O
5, O
6, O
7 O
shots. O
We O
use O
6 O
differ- O
ent O
random O
seeds O
to O
select O
the O
few-shot O
training O
samples O
and O
the O
data O
augmentation O
is O
conducted O
to O
solve O
the O
data O
sparsity. O
We O
train O
all O
the O
mod- O
els O
using O
the O
same O
data O
and O
compute O
the O
average O
performance O
and O
the B-MetricName
standard I-MetricName
deviation. I-MetricName
We O
only O
report O
the O
result O
of O
1, O
3, O
5, O
7 O
shots O
for O
space O
limi- O
tation. O
To O
evaluate O
our O
model, O
we O
first O
convert O
our O
results O
into O
IOBES O
tagging O
style O
and O
compute O
the O
word-level B-MetricName
precision, I-MetricName
recall, B-MetricName
and B-MetricName
F-1 O
score O
using O
the O
APIs O
from O
Nakayama O
(2018) O
so O
that O
all O
compar- O
isons O
with O
sequence O
labeling O
methods O
are O
under O
the O
same O
metrics. O
We O
believe O
such O
experiment O
settings O
guarantee O
the O
results O
are O
representative. O
5.2 O
Datasets O
Our O
experiments O
are O
conducted O
on O
two O
real-world O
data O
collections: B-DatasetName
FUNSD I-DatasetName
and B-DatasetName
CORD-Lv1. I-DatasetName
Both O
datasets O
provide O
rich O
annotations O
for O
the O
document O
image O
understandings O
includes O
the O
words O
and O
the O
word-level O
bounding O
boxes. O
The O
details O
and O
statis- O
tics O
of O
these O
two O
datasets O
are O
as O
follows. B-DatasetName
‚Ä¢FUNSD: I-DatasetName
FUNSD B-DatasetName
consists O
of O
199 O
fully- O
annotated, O
noisy-scanned O
forms O
with O
variousTable O
1: O
Dataset O
Statistics. O
Dataset O
# O
Train O
Pages O
# O
Test O
Pages O
# O
Entities O
/ O
Page O
FUNSD O
149 O
50 O
42.86 O
CORD-Lv1 O
800 O
100 O
13.82 O
appearance O
and O
format O
which O
makes O
the O
form O
understanding O
task O
more O
challenging. O
The O
word O
spans O
in O
this O
datasets O
are O
annotated O
with O
three O
different O
labels: O
header O
,question O
and O
answer O
, O
and O
the O
rest O
words O
are O
annotated O
as O
other O
. O
We O
use O
the O
original O
label O
names. B-DatasetName
‚Ä¢CORD-Lv1: I-DatasetName
CORD B-DatasetName
consists O
of O
about O
1000 O
re- O
ceipts O
with O
annotations O
of O
bounding O
boxes O
and O
textual O
contents. O
The O
entities O
have O
multi-level O
labels. O
We O
select O
the O
first O
level O
and O
denote O
the O
dataset O
as B-DatasetName
CORD-Lv1. I-DatasetName
The O
first O
level O
in- O
cludes O
menu O
,void-menu O
,subtotal O
and O
total O
. O
We O
simplify O
subtotal O
assub O
and O
void-menu O
asvoid O
. O
5.3 O
Compared O
Methods O
We O
evaluate B-MethodName
LASER I-MethodName
against O
several O
strong O
se- O
quence O
labeling O
methods O
as O
follows. B-MethodName
‚Ä¢BERT I-MethodName
(Devlin O
et O
al., O
2018) O
is O
a O
text-only O
auto- O
encoding O
pre-trained O
language O
model O
using O
the O
large-scale O
mask O
language O
modeling. O
We O
fine- O
tune O
the O
pre-trained B-MethodName
BERT-base I-MethodName
model O
with O
the O
few-shot O
training O
samples O
on O
each O
datasets. B-MethodName
‚Ä¢RoBERTa I-MethodName
(Liu O
et O
al., O
2019) O
extends O
the O
capac- O
ity O
of B-MethodName
BERT I-MethodName
and O
achieves O
better O
performance O
in O
multiple O
natural O
language O
understanding O
tasks. O
We O
also O
conduct O
the O
fine-tuning O
with O
few-shot O
training O
samples. B-MethodName
‚Ä¢LayoutLM I-MethodName
(Xu O
et O
al., O
2020) O
is O
a O
multi-modal O
language O
model O
which O
includes O
the O
layout O
and O
text O
information. O
It O
is O
built O
upon B-MethodName
BERT I-MethodName
and O
adds O
the O
extra O
spatial O
embeddings O
into O
the B-MethodName
BERT I-MethodName
embedding O
layer. O
Following B-MethodName
LayoutLM, I-MethodName
Lay- O
outLMv2 O
(Xu O
et O
al., O
2021a) O
leverages O
extra O
com- O
puter O
vision O
features O
and O
improves O
the O
perfor- O
mance, O
which O
are O
strong O
signals O
but O
absent O
in O
our O
settings. O
For O
a O
fair O
comparison, O
we O
do O
not O
include B-MethodName
LayoutLMv2 I-MethodName
in O
our O
comparative O
experiments. B-MethodName
‚Ä¢LayoutReader I-MethodName
(Wang O
et O
al., O
2021) O
is B-TaskName
a I-TaskName
layout- I-TaskName
aware I-TaskName
sequence-to-sequence I-TaskName
model I-TaskName
for O
reading O
order O
detection. O
We O
append O
a O
linear O
layer O
upon O
the O
hidden O
states O
to O
conduct O
sequence O
labeling. O
These O
compared O
methods O
are O
in O
their O
base O
version O
and O
follow O
the O
IOBES O
tagging O
scheme.4178Table O
2: O
Evaluation O
Results O
with O
Different O
Sizes O
of O
5.4 O
Implementation O
Details O
We O
build B-MethodName
LASER I-MethodName
on O
the O
base O
of O
LayoutReader. O
We O
use O
the O
Transformers O
(Wolf O
et O
al., O
2019) O
and O
the O
s2s-ft O
toolkits O
from O
the O
repository O
of O
Dong O
et O
al. O
(2019a). O
We O
use O
one O
NVIDIA O
A6000 O
to O
finetune O
with B-HyperparameterName
batch I-HyperparameterName
size I-HyperparameterName
of B-HyperparameterValue
8. O
We O
optimize O
the O
model O
with B-HyperparameterValue
AdamW I-HyperparameterValue
optimizer B-HyperparameterName
and O
the B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
is B-HyperparameterValue
5√ó10‚àí5. I-HyperparameterValue
5.5 O
Experimental O
Results O
From O
Table O
2 O
and O
Figure O
2, O
the O
results O
show O
that, O
under O
few-shot O
settings, O
our O
proposed O
model, B-MethodName
LASER, I-MethodName
achieves O
the O
SOTA O
overall O
performance O
compared O
with O
sequence O
labeling O
models. O
We O
con- O
clude O
that O
the O
gain O
of O
performance O
comes O
mostly O
from O
the O
generative O
labeling O
scheme O
since B-MethodName
LASER I-MethodName
largely O
outperforms O
LayoutReader O
although O
both O
of O
them O
share O
the O
same O
backbone. O
Specifically, O
compared O
with O
the O
second-bestbaseline, B-MethodName
LASER I-MethodName
improves O
the B-MetricName
F-1 O
scores O
by B-MetricValue
8.59% I-MetricValue
on B-DatasetName
FUNSD I-DatasetName
and B-MetricValue
by I-MetricValue
3.32% I-MetricValue
on B-DatasetName
CORD-Lv1 I-DatasetName
on O
aver- O
age O
across O
the O
different O
shots O
and O
LASER O
(IRLVT) O
also O
surpasses O
the O
baselines O
under O
most O
settings. O
Moreover, O
the O
improvement O
on B-MetricName
precision I-MetricName
is O
re- O
markable. O
LASER O
improves O
the B-MetricName
precision I-MetricName
by B-MetricValue
12.35% I-MetricValue
on B-DatasetName
FUNSD I-DatasetName
and O
by B-MetricValue
10.62% I-MetricValue
on B-DatasetName
CORD-Lv1 I-DatasetName
on O
average O
across O
the O
different O
shots. O
Especially, O
under O
1-shot O
setting, O
it O
surpasses O
the O
best O
sequence O
labeling O
model O
on B-DatasetName
FUNSD I-DatasetName
by B-MetricValue
19.01% I-MetricValue
on B-MetricName
precision, I-MetricName
10.47% B-MetricValue
on B-MetricName
recall I-MetricName
and B-MetricValue
17.18% I-MetricValue
on B-MetricName
F-1 I-MetricName
score. I-MetricName
We O
can O
also O
observe O
a O
drop O
in O
the O
improvement O
with O
the O
increasing O
number O
of O
training O
samples. O
We O
conclude O
that, O
with O
enough O
training O
samples, O
the O
sequence O
labeling O
learns O
the O
meaning O
of O
each O
label O
and O
the O
semantics O
of O
each O
label O
surface O
names O
no O
longer O
provides O
extra O
useful O
information. O
Based O
on O
these O
comparison, O
we O
safely O
come O
to O
the O
conclusion O
that O
our O
proposed O
generative O
label- O
ing O
scheme O
is O
superior O
to O
the O
traditional O
sequence O
labeling O
scheme O
in O
few O
shot O
settings. O
5.6 O
Ablation O
Study O
In O
the O
ablation O
study, O
we O
aim O
at O
study O
the O
role O
of O
the O
label O
surface O
names. O
We O
introduce O
an O
abla- O
tion O
version, B-MethodName
LASER I-MethodName
(IRLVT), O
by O
replacing O
the O
label O
surface O
names O
with O
irrelevant O
tokens. O
We O
also O
design O
more O
different O
sets O
of O
words O
as O
substitutes O
denoted O
Sub1 O
and O
Sub2. O
The O
detailed O
substitutes4179Table O
3: O
Ablation O
Study O
of O
Different O
Label O
Surface O
Names O
in B-MethodName
LASER. I-MethodName
IRLVT O
uses O
the O
irrelevant O
tokens O
as O
labels; O
ORIG O
uses O
the O
original O
label O
surface O
names; O
Sub1 O
and O
Sub2 O
use O
some O
reasonable O
alternative O
label O
surface O
names. O
as O
substitutes. O
To O
implement O
the O
ablation O
study, O
we O
simply O
re- O
place O
the O
word O
embedding O
of O
label O
surface O
names. O
For O
example, O
in B-MethodName
LASER I-MethodName
(Sub1) O
on B-DatasetName
FUNSD, I-DatasetName
we O
use O
the O
wording O
embedding O
of O
titleinstead O
of O
the O
original O
header O
. O
From O
Table O
3, O
we O
compare O
the O
performance O
of O
all O
the O
ablation O
models. O
We O
observe O
that B-MethodName
LASER I-MethodName
performs O
differently O
with O
distinct O
label O
semantics. O
In O
most O
cases, O
the O
human-designed O
labels O
can O
pro- O
vide O
stronger O
semantic O
correlation O
with O
the O
entities O
than O
the O
irrelevant O
labels O
so O
they O
can O
further O
im- O
prove O
the O
performance. O
However, O
there O
are O
also O
drops O
due O
to O
improper O
labels. O
Overall, O
we O
conclude O
that O
the O
semantic O
meanings O
of O
the O
label O
surface O
names O
are O
useful O
to O
bridge O
the O
gap O
between O
the O
labels O
and O
entities. O
5.7 O
Spatial O
Correspondence O
Interpretation O
In O
this O
section, O
we O
study O
the O
ability O
of B-MethodName
LASER I-MethodName
to O
capture O
the O
spatial O
correspondence O
between O
certain O
areas O
and O
the O
labels. O
The O
experiment O
is O
based O
on O
the O
results O
of B-MethodName
LASER I-MethodName
on B-DatasetName
FUNSD I-DatasetName
with O
7 O
shots. O
As O
men- O
tioned O
in O
Section O
4.2, O
we O
design O
unique O
spatial O
iden- O
tifiers O
for O
the O
label O
surface O
names. O
The O
identifiers O
are O
in O
the O
same O
form O
as O
the O
spatial O
embeddings O
and B-MethodName
LASER I-MethodName
inserts O
the O
identifiers O
into O
the O
original O
spa- O
tial O
embedding O
space O
during O
sequence-to-sequence O
training. O
Ideally, O
the O
model O
can O
learn O
where O
a O
cer- O
tain O
label O
is O
more O
likely O
to O
appear. O
To O
visualize O
such O
patterns, O
we O
compute O
the O
cosine O
similarity O
matrix O
Mof O
identifiers O
and O
the O
spatial O
embeddings O
asMij= O
cos O
( O
SpatialID O
(œÑ),SpatialEmb((i, O
j)) O
) O
(a) O
Header O
(b) O
Question O
(c) O
Answer O
Figure O
3: O
Spatial O
correspondence O
visualization O
on O
FUNSD O
for O
different O
entity O
types. O
where O
(i, O
j)is O
the O
normalized O
coordinate O
pair; O
œÑ‚àà O
{œÑ1, O
..., O
œÑ O
t}. O
Then O
we O
plot O
the O
heatmap O
of O
the O
similarity O
matrix, O
where O
the O
highlight O
areas O
mean O
the O
higher O
similarities. O
From O
Figure O
3, O
we O
observe O
that O
the O
label O
header O
is O
more O
likely O
to O
be O
in O
the O
middle O
col- O
umn O
of O
the O
page O
and O
may O
appear O
in O
the O
bottom O
part O
as O
well O
when O
there O
are O
multiple O
paragraphs. O
Intu- O
itively, O
the O
label O
question O
andanswer O
should O
appear O
in O
pairs O
and O
it O
is O
observed O
in O
Figure O
3 O
that O
their O
heatmaps O
are O
almost O
complementary O
to O
each O
other. O
Several O
examples O
from O
FUNSD O
are O
selected O
to O
demonstrate O
the O
visualization O
results O
in O
4. O
Com- O
paring O
the O
examples O
and O
the O
visualization O
results, O
we O
conclude O
that O
the O
spatial O
identifiers O
of O
labels O
capture O
the O
formats O
of O
pages O
and B-MethodName
LASER I-MethodName
leverages O
these O
features O
to O
better O
extract O
the O
entities O
under O
few O
shot O
settings. O
5.8 O
Case O
Study O
We O
visualize O
cases O
from O
the O
5-shot O
setting. O
From O
Figure O
5, O
we O
observe B-MethodName
LASER I-MethodName
can O
extract O
the O
enti-4180(a) O
Original O
Image O
(b) O
Labeled O
Entities O
(c) O
Original O
Image O
(d) O
Labeled O
Entities O
Figure O
4: O
Layout O
Format O
Examples O
from B-DatasetName
FUNSD: I-DatasetName
Bl O
, O
Bl O
, O
Bl O
denotes O
question O
,answer O
,header O
. O
(a) O
Test O
Image O
and O
Expected O
Labels O
(b) O
LASER O
Results O
(c) O
LayoutLM O
Results O
(d) O
Test O
Image O
and O
Expected O
Labels O
(e) B-MethodName
LASER I-MethodName
Results O
(f) O
LayoutLM O
Results O
Figure O
5: O
Case O
Studies. O
(a), O
(b), O
(c) O
from B-DatasetName
FUNSD; I-DatasetName
(d), O
(e), O
(f) O
from B-DatasetName
CORD-Lv1; I-DatasetName
Bl,Bl,Bl,Bldenote O
menu O
, O
question O
,answer O
,other O
;Bl O
, O
Bl O
denote O
menu O
,total O
; O
/// O
, O
/// O
denote O
the O
right, O
wrong O
predictions. O
Table O
4: O
Text-only O
Dataset O
Statistics O
Dataset O
# O
Train O
# O
Test O
# O
Entity O
Type B-DatasetName
OntoNotes I-DatasetName
60.0k O
8.3k O
18 O
Mit O
Movie O
7.8k O
2.0k O
12 O
ties O
correctly, O
and O
the O
errors O
of O
LayoutLM O
comes O
from O
the O
failure O
to O
extract O
the O
entities O
or O
wrong O
en- O
tity O
type O
predictions. O
Since O
the O
sequence O
labeling O
groups O
the O
words O
into O
spans O
through O
IOBES O
tag- O
ging, O
which O
creates O
great O
uncertainty. O
Meanwhile, B-MethodName
LASER I-MethodName
also O
learns O
question O
s O
andanswer O
s O
ap- O
pear O
in O
pairs O
(see O
Figure O
5(b)). O
It O
also O
properly O
predicts O
a O
numerical O
string O
as O
menu O
even O
if O
num- O
bers O
are O
likely O
to O
be O
total O
(see O
Figure O
5(e)). O
5.9 O
Text-only O
Entity O
Recognition B-MethodName
LASER I-MethodName
is O
designed O
for O
the O
entity O
recognition O
task O
in O
document O
images O
where O
both O
text O
and O
layoutTable O
5: O
Results O
of O
10-way-5-shot O
Experiments O
ModelOntoNotes O
MIT O
Movie O
F-1 O
F-1 O
BERT O
60.79 O
¬±0.97 O
47.88 O
¬±0.97 O
RoBERTa O
(Huang O
et O
al., O
2020) O
57.70 O
51.30 O
UniLM O
60.82 O
¬±1.26 O
51.09 O
¬±1.40 O
LASER O
61.11¬±1.08 O
51.88 O
¬±1.27 O
can O
be O
leveraged O
to O
acquire O
essential O
information. O
However, O
the O
generative O
labeling O
scheme O
is O
not O
constrained O
in O
the O
scenario O
of O
document O
images. O
We O
briefly O
explore O
the O
potential O
of O
the O
generative O
labeling O
scheme O
in O
text-only O
scenario. O
We O
ini- O
tialize B-MethodName
LASER I-MethodName
with O
a O
text-only O
language O
model, B-MethodName
UniLM I-MethodName
(Dong O
et O
al., O
2019b), O
based O
on O
the O
experi- O
ments O
in O
Wang O
et O
al. O
(2021), O
and O
apply O
it O
onto O
text- O
only O
entity O
recognition O
task. O
Following O
Huang O
et O
al. O
(2020), O
we O
conduct O
10-way-5-shot O
experiments O
on O
two O
datasets, B-DatasetName
OntoNotes I-DatasetName
(Weischedel O
et O
al., O
2013)4181and B-DatasetName
MIT I-DatasetName
Movie I-DatasetName
(Liu O
et O
al., O
2013), O
which O
cover O
general O
domains O
and O
review O
domains, O
respectively. O
The O
dataset O
statistics O
are O
shown O
in O
Table O
4 O
and O
the O
results O
are O
as O
shown O
in O
Table O
5. O
We O
observe O
that O
our O
method O
can O
also O
surpass O
the O
sequence O
labeling O
methods O
in O
these O
two O
datasets, O
showing O
the O
great O
potential O
of O
the O
generative O
labeling O
scheme O
in O
the O
entity O
recognition O
tasks. O
6 O
Related O
Work O
Layout-aware O
LMs. O
Since O
the O
post-OCR O
pro- O
cessing O
has O
great O
application O
prospects, O
existing O
works O
propose O
to O
adapt O
the O
language O
pre-training O
to O
the O
layout O
formats O
learning. B-MethodName
LayoutLM I-MethodName
(Xu O
et O
al., O
2020) O
is O
the O
pioneer O
in O
this O
area, O
which O
successfully O
uses O
the O
coordinates O
to O
represent O
the O
layout O
informa- O
tion O
in O
the O
embedding O
layer O
of B-MethodName
BERT I-MethodName
(Devlin O
et O
al., O
2018). O
Following O
LayoutLM, O
the O
upgraded O
ver- O
sion, O
LayoutLMv2 O
(Xu O
et O
al., O
2021a), O
is O
further O
pro- O
posed O
to O
leverage O
the O
visual O
features O
and O
benefits O
from O
the O
alignment O
between O
words O
and O
the O
regions O
in O
the O
page. B-MethodName
LAMBERT I-MethodName
(Garncarek O
et O
al., O
2021) O
and O
BROS O
(Hong O
et O
al., O
2020) O
continue O
studying O
the O
layout O
representation O
which O
uses O
the O
sinusoidal O
function O
or O
apply O
the O
relative O
positional O
biases O
from B-MethodName
T5 O
(Raffel O
et O
al., O
2019). O
LayoutReader O
(Wang O
et O
al., O
2021) O
aims O
to O
predict O
the O
reading O
order O
of O
words O
from O
the O
OCR O
results. O
ReadingBank O
(Wang O
et O
al., O
2021) O
is O
proposed O
to O
facilitate O
the O
pre-training O
of O
reading O
order O
detection, O
which O
annotates O
the O
read- O
ing O
order O
of O
millions O
of O
pages. O
Generalized O
Seq2Seq. O
Sequence-to-sequence O
ar- O
chitecture O
is O
basic O
in O
natural O
language O
processing O
and O
is O
originally O
designed O
for O
machine O
translation. O
With O
the O
rise O
of O
large O
pre-trained O
models, O
sequence- O
to-sequence O
models O
are O
increasingly O
used O
with O
new O
problem O
formulation. O
Existing O
works O
exploit O
the O
potential O
latent O
knowledge O
and O
stronger O
represen- O
tation O
ability O
of O
sequence-to-sequence O
modeling. O
GENRE O
(De O
Cao O
et O
al., O
2020) O
creatively O
reformu- O
lates O
the O
entity O
retrieval O
task O
into O
the O
sequence- O
to-sequence O
settings. O
It O
inferences O
the O
lined O
en- O
tities O
using O
the O
generation O
of B-MethodName
BART. I-MethodName
Recent O
works O
on O
prompt O
learning O
also O
leverage O
the O
pre-trained O
sequence-to-sequence O
language O
models O
to O
conduct O
few O
shot O
learning O
(Liu O
et O
al., O
2021; O
Puri O
and O
Catan- O
zaro, O
2019; O
Hambardzumyan O
et O
al., O
2021). O
7 O
Conclusions O
and O
Future O
Work O
In O
this O
paper, O
we O
present B-MethodName
LASER, I-MethodName
a B-TaskName
label-aware I-TaskName
sequence-to-sequence I-TaskName
framework I-TaskName
for O
entity O
recog-nition O
in O
document O
images O
under O
few-shot O
settings. O
It O
benefits O
from O
the O
generative O
labeling O
scheme O
which O
reformulates O
the O
entity O
recognition O
task O
into O
the O
sequence-to-sequence O
setting. O
The O
label O
surface O
names O
are O
embedded O
into O
the O
generated O
sequence. O
Compared O
with O
the O
sequence O
labeling O
methods, B-MethodName
LASER I-MethodName
leverages O
the O
rich O
semantics O
of O
the O
label O
surface O
names O
and O
overcome O
the O
limitation O
of O
train- O
ing O
data. O
Moreover, O
we O
design O
spatial O
identifiers O
for O
each O
label O
and O
well O
insert O
them O
into O
the O
spatial O
embedding O
hyperspace. O
In O
this O
way, B-MethodName
LASER I-MethodName
can O
inference O
the O
entity O
labels O
from O
the O
layout O
formats O
perspective O
and O
empirical O
experiments O
demonstrate O
our O
method O
can O
learn O
the O
layout O
formats O
though O
limited O
number O
of O
training O
samples. O
For O
further O
research, O
we O
will O
investigate O
the O
se- O
lection O
of O
label O
surface O
names O
and O
how O
to O
bet- O
ter O
leverage O
the O
semantics O
from O
the O
pre-trained O
sequence-to-sequence O
models. O
We O
also O
notice O
that O
such O
labeling O
scheme O
can O
cope O
with O
unknown O
cate- O
gories. O
We O
will O
focus O
on O
the O
generalization O
of O
our O
method. O
Acknowledgments O
We O
want O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments. O
Zilong O
would O
also O
like O
to O
thank O
the O
Powell O
Fellowship O
for O
the O
support O
during O
the O
challenging O
years O
at O
the O
beginning O
of O
his O
Ph.D. O
program. O
The O
research O
was O
sponsored O
in O
part O
by O
National O
Science O
Foundation O
Convergence O
Accelerator O
un- O
der O
award O
OIA-2040727 O
as O
well O
as O
generous O
gifts O
from O
Google, O
Adobe, O
and O
Teradata. O
Any O
opin- O
ions, O
findings, O
conclusions, O
or O
recommendations O
ex- O
pressed O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
views, O
either O
expressed O
or O
implied, O
of O
the O
U.S. O
Gov- O
ernment. O
The O
U.S. O
Government O
is O
authorized O
to O
reproduce O
and O
distribute O
reprints O
for O
government O
purposes O
notwithstanding O
any O
copyright O
annotation O
hereon. O
Ethical O
Consideration O
This O
paper O
focuses O
on O
the O
entity O
recognition O
in O
doc- O
ument O
images O
under O
few-shot O
setting. O
Our O
architec- O
ture O
are O
built O
upon O
open-source O
models O
and O
all O
the O
datasets O
are O
available O
online. O
We O
will O
release O
the O
code O
and O
datasets O
on O
https://github.com/ O
zlwang-cs/LASER-release O
. O
Therefore, O
we O
do O
not O
anticipate O
any O
major O
ethical O
concerns.4182References O
Hangbo O
Bao, O
Li O
Dong, O
Furu O
Wei, O
Wenhui O
Wang, O
Nan O
Yang, O
Xiaodong O
Liu, O
Yu O
Wang, O
Jianfeng O
Gao, O
Song- O
hao O
Piao, O
Ming O
Zhou, O
et O
al. O
2020. O
Unilmv2: O
Pseudo- O
masked O
language O
models O
for O
unified O
language O
model O
pre-training. O
In O
International O
Conference O
on O
Ma- O
chine O
Learning O
, O
pages O
642‚Äì652. O
PMLR. O
Nicola O
De O
Cao, O
Gautier O
Izacard, O
Sebastian O
Riedel, O
and O
Fabio O
Petroni. O
2020. O
Autoregressive O
entity O
retrieval. O
arXiv O
preprint O
arXiv:2010.00904 O
. O
Jacob O
Devlin, O
Ming-Wei O
Chang, O
Kenton O
Lee, O
and O
Kristina O
Toutanova. O
2018. O
Bert: O
Pre-training O
of O
deep O
bidirectional O
transformers O
for O
language O
understand- O
ing.arXiv O
preprint O
arXiv:1810.04805 O
. O
Li O
Dong, O
Nan O
Yang, O
Wenhui O
Wang, O
Furu O
Wei, O
Xi- O
aodong O
Liu, O
Yu O
Wang, O
Jianfeng O
Gao, O
Ming O
Zhou, O
and O
Hsiao-Wuen O
Hon. O
2019a. O
Unified O
language O
model O
pre-training O
for O
natural O
language O
understanding O
and O
generation. O
arXiv O
preprint O
arXiv:1905.03197 O
. O
Li O
Dong, O
Nan O
Yang, O
Wenhui O
Wang, O
Furu O
Wei, O
Xiaodong O
Liu, O
Yu O
Wang, O
Jianfeng O
Gao, O
Ming O
Zhou, O
and O
Hsiao- O
Wuen O
Hon. O
2019b. O
Unified O
language O
model O
pre- O
training O
for O
natural O
language O
understanding O
and O
gen- O
eration. O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
32. O
≈Åukasz O
Garncarek, O
Rafa≈Ç O
Powalski, O
Tomasz O
Stanis≈Çawek, O
Bartosz O
Topolski, O
Piotr O
Halama, O
Micha≈Ç O
Turski, O
and O
Filip O
Grali O
¬¥nski. O
2021. O
Lambert: O
Layout-aware O
language O
modeling O
for O
information O
ex- O
traction. O
In O
International O
Conference O
on O
Document O
Analysis O
and O
Recognition O
, O
pages O
532‚Äì547. O
Springer. O
Jean-Philippe O
Thiran O
Guillaume O
Jaume, O
Hazim O
Ke- O
mal O
Ekenel. O
2019. O
Funsd: O
A O
dataset O
for O
form O
under- O
standing O
in O
noisy O
scanned O
documents. O
In O
Accepted O
to O
ICDAR-OST O
. O
Karen O
Hambardzumyan, O
Hrant O
Khachatrian, O
and O
Jonathan O
May. O
2021. O
Warp: O
Word-level O
adversarial O
reprogramming. O
arXiv O
preprint O
arXiv:2101.00121 O
. O
Teakgyu O
Hong, O
DongHyun O
Kim, O
Mingi O
Ji, O
Wonseok O
Hwang, O
Daehyun O
Nam, O
and O
Sungrae O
Park. O
2020. O
Bros: O
A O
pre-trained O
language O
model O
for O
understand- O
ing O
texts O
in O
document. O
Jiaxin O
Huang, O
Chunyuan O
Li, O
Krishan O
Subudhi, O
Damien O
Jose, O
Shobana O
Balakrishnan, O
Weizhu O
Chen, O
Baolin O
Peng, O
Jianfeng O
Gao, O
and O
Jiawei O
Han. O
2020. O
Few-shot O
named O
entity O
recognition: O
A O
comprehensive O
study. O
arXiv O
preprint O
arXiv:2012.14978 O
. O
Jingjing O
Liu, O
Panupong O
Pasupat, O
Yining O
Wang, O
Scott O
Cyphers, O
and O
Jim O
Glass. O
2013. O
Query O
understanding O
enhanced O
by O
hierarchical O
parsing O
structures. O
In O
2013 O
IEEE O
Workshop O
on O
Automatic O
Speech O
Recognition O
and O
Understanding O
, O
pages O
72‚Äì77. O
IEEE.Pengfei O
Liu, O
Weizhe O
Yuan, O
Jinlan O
Fu, O
Zhengbao O
Jiang, O
Hiroaki O
Hayashi, O
and O
Graham O
Neubig. O
2021. O
Pre- O
train, O
prompt, O
and O
predict: O
A O
systematic O
survey O
of O
prompting O
methods O
in O
natural O
language O
processing. O
arXiv O
preprint O
arXiv:2107.13586 O
. O
Yinhan O
Liu, O
Myle O
Ott, O
Naman O
Goyal, O
Jingfei O
Du, O
Man- O
dar O
Joshi, O
Danqi O
Chen, O
Omer O
Levy, O
Mike O
Lewis, O
Luke O
Zettlemoyer, O
and O
Veselin O
Stoyanov. O
2019. O
Roberta: O
A O
robustly O
optimized O
bert O
pretraining O
ap- O
proach. O
arXiv O
preprint O
arXiv:1907.11692 O
. O
Lluis O
Marquez, O
Pere O
Comas, O
Jes√∫s O
Gim√©nez, O
and O
Neus O
Catala. O
2005. O
Semantic O
role O
labeling O
as O
sequential O
tagging. O
In O
Proceedings O
of O
the O
Ninth O
Conference O
on O
Computational O
Natural O
Language O
Learning O
(CoNLL- O
2005) O
, O
pages O
193‚Äì196. O
Hiroki O
Nakayama. O
2018. O
seqeval: O
A O
python O
framework O
for O
sequence O
labeling O
evaluation. O
Software O
available O
from O
https://github.com/chakki-works/seqeval. O
Seunghyun O
Park, O
Seung O
Shin, O
Bado O
Lee, O
Junyeop O
Lee, O
Jaeheung O
Surh, O
Minjoon O
Seo, O
and O
Hwalsuk O
Lee. O
2019. O
Cord: O
A O
consolidated O
receipt O
dataset O
for O
post-ocr O
parsing. O
Raul O
Puri O
and O
Bryan O
Catanzaro. O
2019. O
Zero-shot O
text O
classification O
with O
generative O
language O
models. O
arXiv O
preprint O
arXiv:1912.10165 O
. O
Colin O
Raffel, O
Noam O
Shazeer, O
Adam O
Roberts, O
Katherine O
Lee, O
Sharan O
Narang, O
Michael O
Matena, O
Yanqi O
Zhou, O
Wei O
Li, O
and O
Peter O
J O
Liu. O
2019. O
Exploring O
the O
limits O
of O
transfer O
learning O
with O
a O
unified O
text-to-text O
trans- O
former. O
arXiv O
preprint O
arXiv:1910.10683 O
. O
Lev O
Ratinov O
and O
Dan O
Roth. O
2009. O
Design O
challenges O
and O
misconceptions O
in O
named O
entity O
recognition. O