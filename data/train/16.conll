Proceedings  O
of  O
the  O
15th  O
International  O
Workshop  O
on  O
Semantic  O
Evaluation  O
(SemEval-2021)  O
,  O
pages  O
578–584  O
Bangkok,  O
Thailand  O
(online),  O
August  O
5–6,  O
2021.  O
©2021  O
Association  O
for  O
Computational  O
Linguistics578DeepBlueAI  O
at  O
SemEval-2021  O
Task  O
1:  O
Lexical  B-TaskName
Complexity  I-TaskName
Prediction  I-TaskName
with  O
A  O
Deep  B-MethodName
Ensemble  I-MethodName
Approach  O
Chunguang  O
Pan  O
Bingyan  O
Song  O
Shengguang  O
Wang  O
Zhipeng  O
Luo  O
DeepBlue  O
Technology  O
(Shanghai)  O
Co.,  O
Ltd  O
fsongby,  O
panchg,  O
wangshg,  O
luozp  O
g@deepblueai.com  O
Abstract  O
Lexical  O
complexity  O
plays  O
an  O
important  O
role  O
in  O
reading  O
comprehension.  O
lexical  B-TaskName
complex-  I-TaskName
ity  I-TaskName
prediction  I-TaskName
(LCP)  I-TaskName
can  O
not  O
only  O
be  O
used  O
as  O
a  O
part  O
of  O
Lexical  O
Simpliﬁcation  O
systems,  O
but  O
also  O
as  O
a  O
stand-alone  O
application  O
to  O
help  O
peo-  O
ple  O
better  O
reading.  O
This  O
paper  O
presents  O
the  O
win-  O
ning  O
system  O
we  O
submitted  O
to  O
the  O
LCP  B-TaskName
Shared  O
Task  O
of  O
SemEval  B-DatasetName
2021  O
that  O
capable  O
of  O
deal-  O
ing  O
with  O
both  O
two  O
subtasks.  O
We  O
ﬁrst  O
per-  O
form  O
ﬁne-tuning  O
on  O
numbers  O
of  O
pre-trained  O
language  O
models  O
(PLMs)  O
with  O
various  O
hyper-  O
parameters  O
and  O
different  O
training  O
strategies  O
such  O
as  O
pseudo-labelling  O
and  O
data  O
augmenta-  O
tion.  O
Then  O
an  O
effective  O
stacking  O
mechanism  O
is  O
applied  O
on  O
top  O
of  O
the  O
ﬁne-tuned  O
PLMs  O
to  O
ob-  O
tain  O
the  O
ﬁnal  O
prediction.  O
Experimental  O
results  O
on  O
the  O
Complex  O
dataset  O
show  O
the  O
validity  O
of  O
our  O
method  O
and  O
we  O
rank  O
ﬁrst  O
and  O
second  O
for  O
subtask  O
2  O
and  O
1.  O
1  O
Introduction  O
Lexical  O
complexity  O
is  O
one  O
of  O
the  O
main  O
reasons  O
lead-  O
ing  O
to  O
overall  O
text  O
complexity  O
and  O
thus  O
result  O
in  O
poor  O
reading  O
comprehension  O
for  O
readers  O
(DuBay,  O
2004).  O
Different  O
from  O
the  O
Complex  O
Word  O
Identiﬁ-  O
cation  O
(CWI)  O
(Shardlow,  O
2014)  O
task,  O
which  O
aims  O
to  O
predict  O
whether  O
a  O
given  O
word  O
is  O
complex  O
or  O
not,  O
the  O
goal  O
of  O
lexical  B-TaskName
complexity  I-TaskName
prediction  I-TaskName
(LCP)  I-TaskName
is  O
to  O
predict  O
the  O
complexity  O
value  O
of  O
the  O
given  O
parts  O
from  O
contexts  O
as  O
shown  O
in  O
Figure  O
1.  O
The  O
under-  O
lined  O
parts  O
of  O
the  O
sentence  O
are  O
the  O
words  O
that  O
need  O
to  O
be  O
predicted  O
and  O
the  O
same  O
words  O
in  O
different  O
con-  O
texts  O
may  O
have  O
different  O
complexity  O
scores.  O
LCP  B-TaskName
plays  O
an  O
important  O
role  O
in  O
the  O
usual  O
Lexical  O
Sim-  O
pliﬁcation  O
(LS)  O
(Bott  O
et  O
al.,  O
2012)  O
pipeline  O
since  O
it  O
can  O
help  O
simpliﬁers  O
ﬁnd  O
the  O
challenging  O
words  O
and  O
replace  O
them  O
with  O
appropriate  O
alternatives  O
that  O
easy  O
to  O
understand.  O
Either  O
LCP  B-TaskName
or  O
CWI  O
can  O
not  O
only  O
be  O
used  O
as  O
a  O
component  O
of  O
LS  O
systems  O
but  O
also  O
as  O
a  O
stand-alone  O
application  O
within  O
intelligent  O
Multi  O
-words  O
Context  O
1:  O
SEM  O
confirmed  O
many  O
of  O
the  O
observations  O
made  O
by  O
confocal  O
microscopy  O
.  O
Complexity  O
score  O
:  O
0.64473  O
Context  O
2:  O
SJ  O
and  O
SVJ  O
carried  O
out  O
confocal  O
microscopy  O
on  O
whole  O
-mounts  O
of  O
stria  O
vascularis  O
.  O
Complexity  O
score  O
:  O
0.7750Single  O
word  O
Context  O
1:  O
They  O
shall  O
be  O
to  O
you  O
for  O
a  O
refuge  O
from  O
the  O
avenger  O
of  O
blood  O
.  O
Complexity  O
score  O
:  O
0.3475  O
Context  O
2:  O
There  O
will  O
be  O
a  O
pavilion  O
for  O
a  O
shade  O
in  O
the  O
daytime  O
from  O
the  O
heat,  O
and  O
for  O
a  O
refuge  O
and  O
for  O
a  O
shelter  O
from  O
storm  O
and  O
from  O
rain  O
.  O
Complexity  O
score  O
:  O
0.075Figure  O
1:  O
Examples  O
of  O
LCP  O
including  O
single  O
words  O
and  O
multi-words.  O
The  O
complexity  O
score  O
is  O
the  O
score  O
for  O
the  O
underlined  O
words.  O
tutoring  O
systems  O
for  O
second  O
language  O
learners  O
or  O
in  O
reading  O
devices  O
for  O
people  O
with  O
low  O
literacy  O
skills  O
(Gooding  O
and  O
Kochmar,  O
2018).  O
In  O
this  O
paper,  O
we  O
introduce  O
our  O
system  O
for  O
the  O
lexical  B-TaskName
complexity  I-TaskName
prediction  I-TaskName
task  O
of  O
the  O
SemEval-  B-DatasetName
2021  O
(Matthew  O
et  O
al.,  O
2021).  O
We  O
fulﬁll  O
this  O
task  O
by  O
leveraging  O
multiple  O
pre-trained  O
language  O
models  O
(PLM)  O
with  O
different  O
training  O
strategies.  O
There  O
are  O
two  O
main  O
steps  O
for  O
our  O
system:  O
(i)ﬁne-tuning  O
numbers  O
of  O
heterogeneous  O
PLMs,  O
including  O
BERT  B-MethodName
(Devlin  O
et  O
al.,  O
2019),  O
ALBERT  B-MethodName
(Lan  O
et  O
al.,  O
2019),  O
RoBERTa  B-MethodName
(Liu  O
et  O
al.,  O
2019)  O
and  O
ERNIE  B-MethodName
(Zhang  O
et  O
al.,  O
2019),  O
with  O
various  O
hyperparameters  O
and  O
training  O
strategies,  O
obtaining  O
diverse  O
models;  O
(ii)  O
applying  O
an  O
effective  O
stacking  O
mechanism  O
on  O
top  O
of  O
these  O
PLMs  O
to  O
predict  O
the  O
ﬁnal  O
complexity  B-MetricName
scores.  I-MetricName
Our  O
experiments,  O
merging  O
PLMs  O
in  O
total,  O
indi-  O
cate  O
that  O
our  O
method  O
successfully  O
utilizes  O
weaker  O
PLMs  O
as  O
well  O
as  O
high-performing  O
PLMs.  O
As  O
a  O
re-  O
sult,  O
our  O
system  O
ranks  O
second  O
and  O
ﬁrst  O
for  O
Subtask  O
1  O
and  O
2  O
of  O
LCP  O
2021,  O
SemEval-2021.  O
2  O
Related  O
Work  O
2.1  O
Lexical  O
Complexity  O
Prediction  O
There  O
has  O
been  O
some  O
work  O
for  O
the  O
creation  O
and  O
evaluation  O
of  O
automatically  O
graded  O
vocabulary  O
lists579  O
Bible  O
River  O
There  O
came  O
up  O
out  O
of  O
river  O
seven  O
...  O
[CLS  O
]  O
[SEP]  O
query  O
context[CLS  O
]  O
q0  O
...  O
qn  O
[SEP]  O
qw  O
...  O
c0  O
...  O
cm  O
cw  O
...  O
c1  O
cm-1  O
Tokenizex[CLS]_k  O
...  O
x[CLS]_0  O
PLMWeight  O
DenseMulti  O
-sample  O
DropoutPredicted  O
ScoresFigure  O
2:  O
The  O
overall  O
architecture  O
for  O
predicting  O
complexity  O
scores.  O
for  O
analyzing  O
lexical  O
complexity.  O
Fran  O
c  O
¸ois  O
et  O
al.  O
(2014)  O
present  O
the  O
ﬁrst  O
graded  O
lexicon  O
for  O
French  O
as  O
a  O
foreign  O
language  O
that  O
reports  O
word  O
frequen-  O
cies  O
by  O
difﬁculty  O
level  O
and  O
Gala  O
et  O
al.  O
(2014)  O
train  O
two  O
SVM  B-MethodName
classiﬁers  O
with  O
49  O
features,  O
one  O
for  O
L1  O
learners  O
and  O
one  O
for  O
learners  O
of  O
French  O
as  O
a  O
foreign  O
language.  O
Alfter  O
and  O
V  O
olodina  O
(2018)  O
map  O
the  O
use  O
of  O
previously  O
created  O
word  O
lists  O
to  O
a  O
single  O
CEFR  B-MethodName
scale  O
(Common  O
European  O
Framework  O
of  O
Reference  O
for  O
Languages)  O
(De  O
l’Europe,  O
2003),  O
then  O
they  O
add  O
topics  O
as  O
additional  O
features  O
to  O
predict  O
the  O
com-  O
plexity  O
level  O
for  O
learners  O
of  O
Swedish  O
as  O
a  O
second  O
language.  O
Shardlow  O
et  O
al.  O
(2020)  O
point  O
out  O
the  O
lim-  O
itation  O
of  O
treating  O
lexical  O
complexity  O
as  O
a  O
binary  O
classiﬁcation  O
task.  O
Therefore,  O
they  O
present  O
the  O
ﬁrst  O
English  O
dataset  O
for  O
continuous  O
lexical  O
complexity  O
prediction  O
and  O
develop  O
a  O
linear  O
regression-based  O
method  O
with  O
various  O
features.  O
2.2  O
Complex  O
Word  O
Identiﬁcation  O
A  O
related  O
area  O
of  O
LCP  O
is  O
CWI.  O
Early  O
studies  O
on  O
CWI  O
either  O
attempt  O
to  O
simplify  O
all  O
words  O
(Thomas  O
and  O
Anderson,  O
2012)  O
or  O
set  O
a  O
frequency-based  O
threshold  O
(Biran  O
et  O
al.,  O
2011).  O
Shardlow  O
(2013)  O
indicates  O
that  O
a  O
classiﬁcation-based  O
method  O
to  O
CWI  O
is  O
the  O
most  O
promising  O
one.  O
Most  O
of  O
the  O
teams  O
partic-  O
ipating  O
in  O
two  O
CWI  O
shared  O
tasks  O
also  O
use  O
classiﬁca-  O
tion  O
approaches  O
with  O
extensive  O
feature  O
engineering.  O
In  O
CWI  O
2016  O
(Paetzold  O
and  O
Specia,  O
2016a),  O
com-  O
plexity  O
was  O
deﬁned  O
as  O
whether  O
or  O
not  O
a  O
word  O
is  O
dif-  O
ﬁcult  O
to  O
understand  O
for  O
non-native  O
English  O
speakers  O
and  O
the  O
words  O
in  O
the  O
dataset  O
are  O
tagged  O
as  O
complex  O
or  O
non-complex  O
by  O
400  O
non-native  O
English  O
speak-  O
ers.  O
The  O
results  O
highlight  O
the  O
effectiveness  O
of  O
Deci-  O
sion  O
Trees  O
(Quijada  O
and  O
Medero,  O
2016;  O
Mukherjee  O
et  O
al.,  O
2016)  O
and  O
Ensemble  O
methods  O
(Paetzold  O
and  O
Specia,  O
2016b;  O
Malmasi  O
et  O
al.,  O
2016)  O
for  O
the  O
task.In  O
CWI  O
2018  O
(Yimam  O
et  O
al.,  O
2018),  O
a  O
multilingual  O
dataset  O
was  O
provided  O
containing  O
English,  O
German,  O
Spanish  O
and  O
French  O
and  O
there  O
were  O
two  O
subtasks:  O
binary  O
classiﬁcation  O
and  O
probabilistic  O
classiﬁca-  O
tion.  O
The  O
submitted  O
systems  O
mainly  O
use  O
traditional  O
machine  O
learning  O
classiﬁers(e.g.  O
SVM,  B-MethodName
Random  B-MetricName
Forests)  O
with  O
features  O
(Butnaru  O
and  O
Ionescu,  O
2018;  O
Kajiwara  O
and  O
Komachi,  O
2018),  O
deep  O
learning  O
meth-  O
ods  O
(Hartmann  O
and  O
Dos  O
Santos,  O
2018;  O
De  O
Hertog  O
and  O
Tack,  O
2018)  O
and  O
ensemble  O
methods  O
(Gooding  O
and  O
Kochmar,  O
2018;  O
Aroyehun  O
et  O
al.,  O
2018).  O
More  O
recently,  O
(Gooding  O
and  O
Kochmar,  O
2019)  O
propose  O
a  O
new  O
perspective  O
by  O
treating  O
CWI  O
as  O
a  O
sequence  O
la-  O
beling  O
task  O
that  O
can  O
detect  O
both  O
complex  O
words  O
and  O
phrases.  O
All  O
these  O
methods  O
are  O
different  O
from  O
ours  O
which  O
utilizes  O
heterogeneous  O
PLMs  O
with  O
various  O
training  O
strategies.  O
3  O
Background  O
Task  O
Deﬁnition  O
There  O
are  O
two  O
subtasks  O
in  O
the  O
LCP  O
task.  O
For  O
subtask  O
1,  O
the  O
goal  O
is  O
to  O
predict  O
the  O
complexity  O
score  O
for  O
a  O
single  O
word  O
from  O
the  O
given  O
context.  O
As  O
an  O
example  O
shown  O
in  O
Figure  O
1,  O
the  O
‘refuge’  O
is  O
the  O
word  O
that  O
needs  O
to  O
be  O
predicted  O
and  O
since  O
the  O
meaning  O
of  O
it  O
is  O
harder  O
to  O
get  O
in  O
the  O
ﬁrst  O
context,  O
its  O
complexity  O
score  O
in  O
the  O
ﬁrst  O
context  O
is  O
much  O
higher.  O
For  O
subtask  O
2,  O
the  O
goal  O
is  O
to  O
predict  O
the  O
complexity  O
score  O
for  O
a  O
multi-word  O
expression  O
from  O
the  O
given  O
context.  O
An  O
example  O
is  O
also  O
shown  O
in  O
the  O
right  O
part  O
of  O
Figure  O
1.  O
Dataset  O
Shardlow  O
et  O
al.  O
(2020)  O
introduce  O
a  O
new  O
English  O
corpus,  O
Complex,  B-MetricName
as  O
the  O
dataset  O
for  O
the  O
LCP  O
task  O
of  O
SemEval-2021.  O
Instead  O
of  O
assign-  O
ing  O
binary  O
scores  O
for  O
lexical  O
complexity,  O
they  O
use  O
crowdsourcing  O
to  O
annotate  O
8979  O
instances  O
covering  O
three  O
genres  O
with  O
lexical  O
complexity  O
scores  O
Parameter  O
settings  O
for  O
different  O
base  O
models  O
a  O
5-point  O
Likert  O
scale:  O
one  O
for  O
very  O
easy,  O
two  O
for  O
easy,  O
three  O
for  O
neutral,  O
four  O
for  O
difﬁcult,  O
and  O
ﬁve  O
for  O
very  O
difﬁcult.  O
The  O
numerical  O
labels  O
were  O
trans-  O
formed  O
to  O
a  O
0-1  O
range  O
as  O
shown  O
in  O
Figure  O
1.  O
To  O
add  O
further  O
variation  O
to  O
the  O
data,  O
three  O
corpora  O
were  O
selected  O
including  O
Bible,  O
Europarl  O
(Koehn,  O
2005)  O
and  O
Biomedical  O
(Bada  O
et  O
al.,  O
2012).  O
Each  O
corpus  O
has  O
its  O
own  O
unique  O
language  O
features  O
and  O
styles.  O
In  O
addition  O
to  O
single  O
words,  O
multi-word  O
expressions  O
were  O
also  O
selected  O
for  O
annotating.  O
In  O
the  O
end,  O
there  O
were  O
9476  O
annotated  O
contexts  O
with  O
5166  O
unique  O
words.  O
4  O
System  O
4.1  O
PLMs-based  O
Method  O
PLMs  O
such  O
as  O
BERT  B-MetricName
(Bidirectional  I-MetricName
Encoder  I-MetricName
Rep-  I-MetricName
resentations  I-MetricName
from  I-MetricName
Transformers)  I-MetricName
use  O
the  O
encoder  O
structure  O
of  O
the  O
Transformer  O
(Vaswani  O
et  O
al.,  O
2017)  O
for  O
deep  O
self-supervised  O
learning,  O
which  O
requires  O
task-speciﬁc  O
ﬁne-tuning.  O
In  O
this  O
paper,  O
the  O
down-  O
stream  O
task  O
is  O
to  O
predict  O
the  O
complexity  B-MetricName
scores,  I-MetricName
a  O
real-value  O
in  O
the  O
range  O
of  O
[0,1],  O
of  O
given  O
words.  O
Our  O
method  O
is  O
capable  O
of  O
dealing  O
with  O
both  O
subtask  O
1  O
and  O
2.  O
Figure  O
2  O
shows  O
the  O
main  O
architecture  O
of  O
our  O
BERT-based  O
model  O
for  O
predicting  O
complexity  B-MetricName
scores.  I-MetricName
Since  O
PLMs  O
can  O
process  O
multiple  O
input  O
sen-  O
tences,  O
we  O
add  O
a  O
query  O
sentence  O
before  O
the  O
context  O
to  O
emphasize  O
the  O
words  O
(e.g.  O
river)  O
that  O
need  O
to  O
be  O
predicted  O
and  O
the  O
corpus  O
(e.g.  O
Bible)  O
they  O
come  O
from.  O
We  O
add  O
special  O
tokens  O
[CLS]  O
and[SEP]  O
to  O
separate  O
the  O
query  O
and  O
the  O
context  O
as  O
shown  O
in  O
Figure  O
2.  O
BERT  O
ﬁrst  O
tokenizes  O
the  O
input  O
contents  O
and  O
then  O
generates  O
contextualized  O
vector  O
represen-  O
tations  O
for  O
each  O
token  O
in  O
multiple  O
hidden  O
layers.  O
We  O
focus  O
on  O
the  O
output  O
of  O
only  O
the  O
ﬁrst  O
position  O
that  O
we  O
passed  O
the  O
special  O
[CLS]  O
token  O
to.  O
The  O
lastkhidden  O
layers  O
are  O
selected  O
to  O
get  O
the  O
ﬁnal  O
representation  O
of  O
token  O
[CLS]  O
through  O
a  O
weighted  O
calculation  O
function  O
as  O
below,  O
x[CLS]=kX  O
i=1Wix[CLS]iwhere  O
Wiis  O
the  O
learning  O
weight  O
for  O
each  O
hidden  O
layer.  O
The  O
calculated  O
representation  O
is  O
then  O
fed  O
into  O
a  O
dense  O
layer,  O
and  O
the  O
technique  O
of  O
multi-sample  O
dropout  O
(Inoue,  O
2019)  O
is  O
utilized  O
to  O
accelerate  O
train-  O
ing  O
and  O
ﬁnally  O
obtain  O
the  O
predicted  O
complexity  O
scores.  O
The  O
loss  O
function  O
can  O
be  O
chosen  O
among  O
sev-  O
eral  O
options  O
including  O
Mean  O
Square  O
Error  O
(MSE),  O
Root  O
Mean  O
Square  O
Error  O
(RMSE),  O
and  O
Mean  O
Ab-  O
solute  O
Error  O
(MAE).  O
4.2  O
Training  O
strategies  O
In  O
order  O
to  O
further  O
improve  O
the  O
diversity  O
of  O
trained  O
models,  O
we  O
incorporate  O
two  O
training  O
strategies  O
as  O
depicted  O
below.  O
Pseudo-Labelling  O
Pseudo-labelling  O
is  O
the  O
pro-  O
cess  O
of  O
using  O
a  O
labeled  O
data  O
model  O
to  O
predict  O
la-  O
bels  O
for  O
unlabeled  O
data.  O
We  O
predict  O
the  O
unlabeled  O
test  O
dataset  O
and  O
mix  O
these  O
pseudo  O
labels  O
with  O
the  O
training  O
set  O
together  O
to  O
train  O
the  O
new  O
model.  O
Data  O
augmentation  O
Data  O
augmentation  O
is  O
the  O
technique  O
used  O
to  O
increase  O
the  O
amount  O
of  O
data  O
by  O
adding  O
slightly  O
modiﬁed  O
copies  O
of  O
already  O
existing  O
data  O
or  O
newly  O
created  O
synthetic  O
data  O
from  O
existing  O
data.  O
It  O
acts  O
as  O
a  O
regularizer  O
and  O
helps  O
reduce  O
over-  O
ﬁtting  O
when  O
training  O
a  O
machine  O
learning  O
model.  O
In  O
this  O
paper,  O
data  O
augmentation  O
consists  O
of  O
two  O
parts.  O
We  O
ﬁrst  O
add  O
the  O
dataset  O
released  O
by  O
CWI  B-DatasetName
2018  I-DatasetName
into  O
the  O
training  O
set.  O
Besides,  O
for  O
subtask  O
2,  O
since  O
its  O
training  O
dataset  O
is  O
small  O
which  O
only  O
contains  O
one  O
thousand  O
samples,  O
we  O
add  O
the  O
dataset  O
of  O
subtask  O
1  O
to  O
train  O
the  O
model  O
for  O
subtask  O
2.  O
Then,  O
for  O
a  O
given  O
sentence  O
in  O
the  O
training  O
set,  O
we  O
perform  O
the  O
operations  O
containing  O
synonym  O
replacement,  O
ran-  O
dom  O
insertion,  O
random  O
swap,  O
and  O
random  O
deletion  O
introduced  O
by  O
Wei  O
and  O
Zou  O
(2019).  O
4.3  O
Stacking  O
Trained  O
Models  O
Model  O
stacking  O
is  O
an  O
efﬁcient  O
ensemble  O
method  O
to  O
improve  O
model  O
accuracy.  O
The  O
main  O
procedure  O
of  O
stacking  O
trained  O
models  O
in  O
our  O
method  O
including  O
ﬁve  O
steps.  O
First,  O
we  O
use  O
heterogeneous  O
PLMs  O
in-  O
cluding  O
BERT,  B-MethodName
RoBERTa,  B-MethodName
ALBERT,  B-MethodName
and  O
ERNIE  B-MethodName
as  O
base  O
models.  O
Second,  O
we  O
generate  O
Comparison  O
of  O
different  O
pre-trained  O
language  O
models  O
of  O
subtask  O
2  O
hyperparameter  O
sets  O
by  O
setting  O
different  O
values  O
of  O
dropout,  B-HyperparameterName
selecting  O
different  O
numbers  B-HyperparameterName
of  I-HyperparameterName
last  I-HyperparameterName
hidden  I-HyperparameterName
layers,  I-HyperparameterName
and  O
using  O
different  O
loss  B-HyperparameterName
functions.  I-HyperparameterName
Since  O
our  O
purpose  O
here  O
is  O
not  O
only  O
to  O
ﬁnd  O
the  O
best  O
hyper-  O
parameter  O
sets  O
but  O
also  O
to  O
collect  O
diverse  O
sets  O
with  O
reasonable  O
performances,  O
we  O
keep  O
all  O
the  O
training  O
results  O
from  O
different  O
sets.  O
Third,  O
we  O
perform  O
7-  O
fold  O
cross-validation  O
during  O
the  O
whole  O
training  O
pro-  O
cess  O
to  O
avoid  O
overﬁtting  O
or  O
selection  O
bias.  O
Fourth,  O
we  O
adopt  O
several  O
training  O
strategies  O
including  O
us-  O
ing  O
pseudo-labelling  O
(Iscen  O
et  O
al.,  O
2019)  O
and  O
data  O
augmentation  O
to  O
further  O
improve  O
the  O
diversity  O
of  O
trained  O
models.  O
Ultimately,  O
we  O
train  O
a  O
simple  O
linear  O
regression  O
model  O
as  O
the  O
ﬁnal  O
estimator.  O
Suppose  O
that  O
the  O
com-  O
plexity  O
score  O
predicted  O
by  O
a  O
based  O
model  O
with  O
one  O
hyperparameter  O
set  O
is  O
^yj,  O
then  O
the  O
ﬁnal  O
complexity  B-MetricName
scores  I-MetricName
will  O
be  O
calculated  O
as  O
below,  O
^y=NX  O
j=1Wj^yj  O
where  O
Nis  O
the  O
total  O
number  O
of  O
various  O
ﬁne-tuned  O
PLMs  O
with  O
different  O
hyperparameters  O
sets  O
and  O
Wj  O
is  O
the  O
weight  O
for  O
each  O
predicted  O
score  O
from  O
different  O
PLMs  O
learned  O
by  O
a  O
linear  O
regression  O
model.  O
5  O
Experiments  O
5.1  O
Evaluation  O
Metrics  O
As  O
mentioned  O
in  O
the  O
ofﬁcial  O
evaluation  O
procedure  O
of  O
LCP  B-MethodName
2021,  O
several  O
evaluation  O
metrics  O
are  O
chosen  O
including  O
Pearson  O
correlation  O
(R),  O
Spearman  O
cor-  O
relation  O
(Rho),  O
Mean  O
absolute  O
error  O
(MAE),  O
Meansquared  O
error  O
(MSE),  O
and  O
R-squared  O
(R2).  O
The  O
ﬁnal  O
results  O
are  O
ranked  O
using  O
Pearson  B-MetricName
correlation.  I-MetricName
5.2  O
Parameter  O
settings  O
All  O
models  O
are  O
implemented  O
based  O
on  O
the  O
open-  O
source  O
transformers  O
library  O
of  O
hugging  O
face  O
(Wolf  O
et  O
al.,  O
2020),  O
which  O
provides  O
thousands  O
of  O
pre-  O
trained  O
models  O
that  O
can  O
be  O
quickly  O
downloaded  O
and  O
ﬁne-tuned  O
on  O
speciﬁc  O
tasks.  O
Table  O
1  O
shows  O
the  O
four  O
employed  O
PLMs  O
and  O
different  O
parameters  O
we  O
set  O
for  O
each  O
PLM  O
including  O
different  O
numbers  O
of  O
hidden  O
layers,  O
different  O
dropout  O
pairs,  O
and  O
different  O
loss  O
functions.  O
6  O
Results  O
6.1  O
Ablation  O
Study  O
PLMs  O
with  O
Training  O
Strategies  O
For  O
subtask  O
1,  O
we  O
use  O
different  O
PLMs  O
including  O
ERNIE  O
LARGE,  O
AL-  O
BERT  O
XXLARGE  O
,  O
BERT  O
LARGE,  O
RoBERTa  O
LARGE  O
as  O
shown  O
in  O
Table  O
2.  O
The  O
results  O
are  O
the  O
average  O
scores  O
of  O
7-  O
fold  O
cross-validation  O
on  O
the  O
training  O
dataset.  O
Since  O
RoBERTa  B-MethodName
LARGE  I-MethodName
performs  O
best  O
on  O
this  O
task,  O
we  O
fur-  O
ther  O
incorporate  O
the  O
training  O
strategies  O
including  O
pseudo-labelling  O
(PL)  O
and  O
data  O
augmentation  O
(DA)  O
with  O
it.  O
However,  O
for  O
the  O
training  O
dataset,  O
we  O
ﬁnd  O
that  O
by  O
adding  O
the  O
training  O
strategies,  O
the  O
results  O
decrease  O
a  O
little  O
bit.  O
For  O
subtask2,  O
we  O
use  O
two  O
types  O
of  O
PLMs  O
which  O
are  O
RoBERTa  B-MethodName
LARGE  I-MethodName
and  O
ALBERT  B-MethodName
XXLARGE  I-MethodName
.  O
The  O
re-  O
sults  O
shown  O
in  O
Table  O
3  O
are  O
also  O
obtained  O
by  O
av-  O
eraging  O
the  O
scores  O
of  O
7-fold  O
cross-validation  O
on  O
the  O
training  O
dataset.  O
Since  O
we  O
have  O
added  O
the  O
dataset  O
of  O
subtask  O
1  O
into  O
subtask  O
2,  O
we  O
also  O
show  O
the  O
Leaderboard  O
of  O
doing  O
this  O
in  O
Table  O
3  O
and  O
we  O
can  O
ﬁnd  O
that  O
it  O
is  O
very  O
effective  O
by  O
increasing  O
0.02  O
from  O
base  O
models.  O
Stacking  O
trained  O
models  O
We  O
use  O
a  O
linear  O
regres-  O
sion  O
(LR)  O
model  O
to  O
stack  O
different  O
pre-trained  O
mod-  O
els.  O
We  O
train  O
the  O
weights  O
of  O
each  O
model  O
in  O
LR  O
on  O
the  O
training  O
set  O
and  O
then  O
use  O
the  O
learning  O
weights  O
to  O
predict  O
the  O
ﬁnal  O
scores  O
of  O
the  O
test  O
set.  O
Figure  O
3  O
shows  O
the  O
comparison  O
of  O
Pearson  O
Cor-  O
relation  O
values  O
for  O
stacking  O
different  O
models  O
of  O
sub-  O
task  O
1.  O
The  O
columns  O
in  O
blue  O
are  O
the  O
values  O
com-  O
puted  O
by  O
averaging  O
predicted  O
scores  O
of  O
different  O
models  O
while  O
the  O
columns  O
in  O
orange  O
are  O
the  O
values  O
through  O
the  O
LR  O
function.  O
We  O
can  O
clearly  O
observe  O
that  O
the  O
LR-based  O
ensemble  O
method  O
outperforms  O
those  O
with  O
the  O
mean-based  O
method,  O
which  O
veriﬁes  O
the  O
validity  O
of  O
using  O
the  O
LR  O
mechanism.  O
Besides,  O
although  O
we  O
ﬁnd  O
that  O
adding  O
training  O
strategies  O
to  O
the  O
base  O
models  O
would  O
decrease  O
performance  O
according  O
to  O
Table  O
2,  O
the  O
performance  O
will  O
be  O
im-  O
proved  O
when  O
stacking  O
them  O
all.  O
This  O
indicates  O
the  O
positive  O
effect  O
of  O
increasing  O
model  O
diversity.  O
6.2  O
Ofﬁcial  O
Ranking  O
For  O
both  O
subtask  O
1  O
and  O
subtask  O
2,  O
among  O
all  O
the  O
pre-submission  O
experiments,  O
we  O
ﬁnd  O
that  O
the  O
scores  O
obtained  O
from  O
stacking  O
all  O
the  O
models  O
performed  O
best.  O
The  O
ofﬁcial  O
ranking  O
is  O
presented  O
in  O
Table  O
4  O
and  O
it  O
demonstrates  O
that  O
our  O
system  O
is  O
ranked  O
ﬁrst  O
in  O
subtask  O
2  O
and  O
ranked  O
second  O
in  O
subtask  O
1.7  O
Conclusion  O
In  O
this  O
paper,  O
we  O
propose  O
a  O
top-performing  O
model  O
for  O
the  O
task  O
of  O
lexical  O
complexity  O
prediction.  O
We  O
ﬁne-tune  O
several  O
pre-trained  O
language  O
models  O
in-  O
cluding  O
BERT,  O
ALBERT,  O
RoBERTa,  O
and  O
ERNIE  O
with  O
different  O
training  O
strategies  O
such  O
as  O
pseudo-  O
labelling  O
and  O
data  O
augmentation  O
and  O
stack  O
them  O
with  O
a  O
simple  O
linear  O
regression  O
model.  O
Experimen-  O
tal  O
results  O
show  O
the  O
effectiveness  O
of  O
this  O
ensemble  O
method  O
and  O
we  O
win  O
ﬁrst  O
place  O
and  O
second  O
place  O
for  O
subtask  O
2  O
and  O
1.  O