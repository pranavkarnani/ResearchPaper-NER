Proceedings O
of O
the O
12th O
Workshop O
on O
Computational O
Approaches O
to O
Subjectivity, O
Sentiment O
& O
Social O
Media O
Analysis O
, O
pages O
276 O
- O
279 O
May O
26, O
2022 O
c O
2022 O
Association O
for O
Computational O
Linguistics O
An B-TaskName
Ensemble I-TaskName
Approach I-TaskName
to I-TaskName
Detect I-TaskName
Emotions I-TaskName
at I-TaskName
an I-TaskName
Essay I-TaskName
Level I-TaskName
Himanshu O
Maheshwari1and O
Vasudeva O
Varma2 O
IIIT O
Hyderabad O
1himanshu.maheshwari@research.iiit.ac.in O
,2vv@iiit.ac.in O
Abstract O
This O
paper O
describes O
our O
system O
(IREL, O
ref- O
fered O
as O
himanshu.1007 O
on O
Codalab) O
for O
Shared O
Task O
on O
Empathy O
Detection, O
Emotion O
Classifi- O
cation, O
and O
Personality O
Detection O
at O
12th O
Work- O
shop O
on O
Computational O
Approaches O
to O
Subjec- O
tivity, O
Sentiment O
& O
Social O
Media O
Analysis O
at O
ACL O
2022. O
We O
participated O
in O
track O
2 O
for O
pre- B-TaskName
dicting I-TaskName
emotion I-TaskName
at I-TaskName
the I-TaskName
essay I-TaskName
level. I-TaskName
We O
pro- O
pose O
an O
ensemble O
approach O
that O
leverages O
the O
linguistic O
knowledge O
of O
the O
RoBERTa, B-MethodName
BART- B-MethodName
large, I-MethodName
and O
RoBERTa B-MethodName
model O
finetuned O
on O
the O
GoEmotions B-DatasetName
dataset. O
Each O
brings O
in O
its O
unique O
advantage, O
as O
we O
discuss O
in O
the O
paper. O
Our O
pro- O
posed O
system O
achieved O
a O
Macro B-MetricName
F1 I-MetricName
score O
of O
0.585 B-MetricValue
and O
ranked O
one O
out O
of O
thirteen O
teams O
(the O
current O
top O
team O
on O
leaderboard O
submitted O
after O
the O
deadline). O
The O
code O
can O
be O
found O
here O
1 O
Introduction O
Emotion O
is O
a O
concept O
that O
is O
challenging O
to O
describe. O
Nevertheless, O
as O
human O
beings, O
we O
understand O
the O
emotional O
effect O
situations O
have O
or O
could O
have O
on O
other O
people O
and O
us. O
In O
this O
work, O
we O
aim O
to O
transfer B-TaskName
this I-TaskName
knowledge I-TaskName
of I-TaskName
emotion I-TaskName
detection I-TaskName
to O
machines. O
This O
work O
aims O
to O
develop O
a O
robust O
system O
that O
could O
detect B-TaskName
emotions I-TaskName
at I-TaskName
an I-TaskName
essay I-TaskName
level. I-TaskName
These O
essays O
are O
reactions O
to O
news O
stories O
and O
are O
between O
300 O
and O
800 O
characters O
in O
length. O
Existing O
literature O
on O
emotion O
detection O
mainly O
focuses O
on O
emotion O
detection O
at O
the O
sentence O
level. O
Different O
datasets O
consisting O
of O
sentences O
from O
so- B-DatasetName
cial I-DatasetName
media I-DatasetName
(Mohammad O
(2012), O
Mohammad O
et O
al. O
(2014), O
Liu O
et O
al. O
(2017), O
Demszky O
et O
al. O
(2020)), O
fairytales B-DatasetName
(Alm O
and O
Sproat, O
2005), O
dialogues B-DatasetName
(Li O
et O
al., O
2017), O
etc. O
have O
been O
made O
available. O
How- O
ever, O
the O
task O
of O
emotion O
detection O
at O
an O
essay O
level O
is O
underexplored. O
In O
essay-level O
emotion O
detection, O
the O
emotions O
are O
typically O
expressed O
by O
the O
entire O
narrative O
and O
not O
just O
a O
few O
words O
or O
phrases. O
The O
system O
must O
refer O
to O
the O
entire O
essay O
to O
get O
a O
more O
holistic O
view O
of O
the O
expressed O
emotion. O
We O
empir- O
ically O
show O
that O
systems O
trained O
on O
just O
sentencelevel O
emotion O
detection O
will O
not O
work O
essay O
level O
as O
they O
do O
not O
have O
the O
entire O
context. O
We O
propose O
an O
ensemble O
approach O
consisting O
of O
a O
finetuned O
RoBERTa B-MethodName
(Liu O
et O
al., O
2019), O
finetuned O
BART-large B-MethodName
(Lewis O
et O
al., O
2020), O
and O
RoBERTa B-MethodName
model O
first O
finetuned O
on O
the O
GoEmotions B-DatasetName
(Dem- O
szky O
et O
al., O
2020) O
dataset O
and O
then O
finetuned O
on O
our O
dataset. O
RoBERTa B-MethodName
model O
has O
shown O
amaz- O
ing O
performance O
for O
various O
NLP O
tasks O
and O
thus O
was O
the O
default O
choice O
for O
the O
task. O
BART-large B-MethodName
has O
shown O
amazing O
performance O
for O
summariza- O
tion O
tasks. O
This O
suggests O
it O
is O
suitable O
for O
a O
task O
involving O
multiple O
sentences. O
The O
last O
model O
is O
a O
RoBERTa B-MethodName
model O
that O
was O
first O
finetuned O
on O
the O
GoEmotions B-DatasetName
dataset O
and O
then O
finetuned O
on O
our O
dataset. O
The O
intuition O
is O
that O
since O
it O
has O
a O
good O
understanding O
of O
sentence-level O
emotions O
(from O
GoEmotions), B-DatasetName
it O
will O
combine O
the O
sentence-level O
knowledge O
into O
essay-level O
knowledge. O
This O
is O
especially O
important O
for O
cases O
with O
very O
strong O
expression O
of O
emotions O
in O
a O
sentence. O
Ablation O
studies O
show O
that O
the O
model O
performs O
worse O
in O
the O
absence O
of O
either O
of O
the O
three O
models. O
Another O
ab- O
lation O
study O
is O
conducted O
to O
reinforce O
our O
claim O
that O
the O
task O
can’t O
be O
solved O
by O
looking O
at O
sentence O
level. O
2 O
Dataset O
The O
training O
dataset O
is O
a O
small O
supervised O
dataset O
consisting O
of O
various O
fields. O
However, O
only O
two O
fields O
are O
helpful O
for O
emotion O
prediction: O
essay O
and O
emotion; O
thus, O
we O
use O
only O
these O
fields. O
The O
dataset O
statistics O
are O
shown O
in O
table O
1 O
and O
table O
2. O
The O
dataset O
is O
very O
small O
and O
heavily O
skewed, O
with O
anger O
and O
sadness O
making O
up O
~54% O
of O
the O
en- O
tire O
dataset. O
This O
skewed O
dataset O
affects O
the O
model’s O
performance, O
and O
it O
needs O
to O
be O
dealt O
with. O
Usually, O
NLP O
systems O
deal O
with O
skewed O
datasets O
using O
oversampling, O
undersampling, O
augmentation, O
or O
weighted O
loss O
function. O
With O
such O
few O
data O
points, O
oversampling O
and O
undersampling O
are O
not276Split O
Our O
initial O
exploration O
with O
data O
augmenta- O
tion O
did O
not O
help; O
thus, O
we O
used O
a O
weighted B-HyperparameterValue
cross- I-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
function O
to O
deal O
with O
data O
imbalance. O
The O
weights O
of O
each O
class O
were O
determined O
using O
the O
sklearn O
library.1 O
3 O
Baselines O
The O
following O
section O
describes O
different O
ap- O
proaches O
we O
tried O
before O
shifting O
to O
our O
proposed O
methodology. O
For O
each O
approach, O
grid O
search O
was O
used O
to O
find O
appropriate O
hyperparameters. O
Please O
note O
we O
compare O
different O
models O
using O
Macro B-MetricName
F1 I-MetricName
score O
which O
is O
the O
official O
evaluation O
metric. O
3.1 O
Language O
Model O
Finetuning O
The O
current O
de O
facto O
in O
NLP O
is O
to O
finetune O
a O
lan- O
guage O
model O
for O
any O
classification O
task. O
Our O
first O
approach O
was O
to O
finetune O
a O
language O
model O
and O
observe O
the O
results. O
This O
will O
serve O
as O
a O
baseline O
for O
other O
approaches. O
This O
exercise O
also O
helps O
us O
select O
the O
appropriate O
language O
model O
for O
other O
approaches. O
We O
experimented O
with O
the O
following O
language O
models: O
1. O
Roberta B-MethodName
Base I-MethodName
2. O
Bert-base-uncased B-MethodName
3. O
Roberta-large B-MethodName
4. O
Bart-large B-MethodName
5. O
Longformer-base-4096 B-MethodName
Table O
4 O
shows O
the O
results O
of O
different O
language O
models. O
Roberta-base B-MethodName
is O
performing O
the O
best; O
thus, O
1https://scikit-learn.org/stable/ O
modules/generated/sklearn.utils.class_ O
weight.compute_class_weight.htmlEmotion O
GoEmo- I-DatasetName
tions I-DatasetName
Dataset O
it O
is O
the O
suitable O
language O
model O
for O
other O
ap- O
proaches. O
Roberta-large B-MethodName
overfits O
and O
was O
produc- O
ing O
the O
same O
results O
after O
each O
epoch. O
Longformer, B-MethodName
though O
suitable O
for O
long O
sequences, O
did O
not O
perform O
well. O
3.2 O
Binary O
Classifiers O
Having O
a O
classifier O
doing O
multiclass O
classification O
is O
challenging. O
In O
this O
approach, O
we O
use O
a O
binary O
classifier O
for O
each O
emotion O
and O
take O
the O
emotion O
with O
the O
highest O
softmax O
classification O
probabil- O
ity. O
Specifically, O
we O
finetune O
a O
Roberta-base O
binary O
classifier O
for O
each O
emotion. O
The O
classifier O
aims O
to O
identify O
target O
emotion O
from O
other O
emotions. B-DatasetName
Dur- O
ing O
inference, O
we O
take O
the O
classification O
probability O
from O
each O
classifier. B-MethodName
The O
emotion O
with O
the O
highest O
classification O
probability O
from O
its O
classifier O
is O
the O
predicted O
emotion. O
Table O
4 O
shows O
the O
result O
of O
this O
approach. O
The O
results O
are O
poor O
compared O
to O
fine- O
tuning O
a O
classifier; O
thus, O
a O
binary O
view O
of O
emotion O
is O
unsuitable O
for O
our O
use O
case. O
3.3 O
Finetuning O
a O
classifier O
trained O
on O
GoEmotions B-DatasetName
dataset O
This O
approach O
introduces O
an O
additional O
layer O
of O
transfer O
learning. O
We O
first O
finetune O
a O
Roberta-base B-MethodName
model O
on O
a O
subset O
of O
the O
GoEmotions B-DatasetName
dataset. O
GoE- B-DatasetName
motions I-DatasetName
is O
a O
sentence-level O
fine-grained O
emotion O
classification O
dataset. O
We O
take O
sentences O
that O
have O
only O
one O
of O
the O
seven O
emotions O
of O
our O
task. O
This O
GoEmotions O
finetuned O
classifier O
is O
then O
further O
fine- O
tuned O
on O
our O
dataset. O
The O
idea O
is O
to O
finetune O
a O
classi- O
fier O
that O
has O
some O
understanding O
of O
emotions. O
Table O
3 O
shows O
statistics O
of O
the O
GoEmotions O
dataset. O
Table O
4 O
shows O
results O
for O
the O
same. O
The O
results O
are O
poor, O
suggesting O
that O
strong O
sentence-level O
understand- O
ing O
does O
not O
scale O
to O
essay-level O
understanding. O
4 O
Proposed O
Approach O
We O
make O
the O
following O
observations O
from O
the O
base- O
line O
models: O
a. O
Roberta-base B-MethodName
and O
Bart-large B-MethodName
perform O
better O
than O
the O
rest O
of O
the O
language O
model. O
Both O
models O
bring O
their O
advantage, O
Roberta-base B-MethodName
is O
a O
powerful O
language O
model O
for O
NLU O
tasks, O
and O
Bart-large B-MethodName
is O
suitable O
for O
tasks O
involving O
multiple O
sentences. O
b. O
Roberta-base B-MethodName
model O
that O
is O
first O
finetuned O
on O
GoEmotions B-DatasetName
dataset O
followed O
by O
finetuning O
on O
our O
dataset O
performs O
poorly O
compared O
to O
other O
base- O
lines. O
However, O
it O
has O
a O
firm O
sentence-level O
under- O
standing. O
Thus, O
this O
model O
is O
suitable O
for O
samples O
with O
very O
strong O
emotional O
sentences. O
Based O
on O
these O
observations, O
we O
combine O
the O
strength O
of O
the O
Roberta-base, B-MethodName
Bart-Large, B-MethodName
and O
Roberta-base B-MethodName
model O
that O
is O
first O
finetuned O
on O
the O
GoEmotions B-DatasetName
dataset O
in O
an O
ensemble O
fashion. O
More O
specifically, O
we O
take O
the O
linear O
combination O
of O
clas- O
sification O
probability O
by O
each O
model O
and O
predict O
the O
emotion O
with O
the O
highest O
classification O
probability O
(or O
score). O
Thus O
the O
classification O
probability O
(or O
score) O
is O
given O
by: O
semo O
=λ1PRB+λ2PBL+λ3PRBG O
Where O
semois O
the O
classification O
score O
for O
a O
par- O
ticular O
emotion O
and O
λ1,λ2,λ3are O
the O
weights O
of O
each O
model. O
PRBis O
the O
classification O
probability O
of O
Roberta-base, B-MethodName
PBLis O
the O
classification O
probability O
of O
BART B-MethodName
large I-MethodName
and O
PRBG O
is O
the O
classification O
prob- O
ability O
of O
Roberta-base B-MethodName
finetuned O
on O
GoEmotions. B-DatasetName
The O
emotion O
with O
the O
highest O
score O
is O
predicted. O
We O
found O
λ1,λ2, O
and O
λ3using O
grid O
search O
on O
thedev O
set. O
The O
value O
that O
gave O
the O
best O
result O
is O
λ1: O
0.26, O
λ2: O
0.26, O
and O
λ3: O
0.07. O
Table O
4 O
shows O
the O
re- O
sults O
of O
this O
approach. O
This O
approach O
outperforms O
all O
the O
baselines O
on O
the O
dev O
set, O
suggesting O
strength O
in O
using O
multiple O
language O
models. O
5 O
Training O
As O
discussed, O
we O
use O
grid O
search O
to O
find O
the O
appro- O
priate O
hyperparameters. O
We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
four B-HyperparameterValue
and O
a O
dropout B-HyperparameterName
of O
0.3 B-HyperparameterValue
for O
Roberta-base. B-MethodName
For O
Bart- B-MethodName
Large, I-MethodName
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
three B-HyperparameterValue
and O
a O
dropout B-HyperparameterName
of O
0.4. B-HyperparameterValue
For O
Roberta-base O
trained O
on O
the O
GoEmo- B-DatasetName
tion I-DatasetName
dataset, O
we O
use O
batch B-HyperparameterName
size I-HyperparameterName
eight B-HyperparameterValue
and O
dropout B-HyperparameterName
of O
0.2 B-HyperparameterValue
for O
the O
first O
layer O
of O
finetuning. O
For O
the O
second O
layer O
of O
finetuning, O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
four B-HyperparameterValue
and O
a O
dropout B-HyperparameterName
of O
0.3. B-HyperparameterValue
The O
learning B-HyperparameterName
rate I-HyperparameterName
and O
seed B-HyperparameterName
were O
fixed O
to O
10−5and B-HyperparameterValue
42, B-HyperparameterValue
respectively. O
The O
training O
was O
done O
on O
Nvidia O
RTX O
2080 O
TI O
(11 O
GB) O
and O
took O
about O
one O
hour O
for O
each O
model O
finetuning. O
6 O
Results O
Table O
4 O
shows O
the O
results O
of O
our O
dev O
set. O
We O
submit- O
ted O
the O
ensemble O
solution O
discussed O
above O
based O
on O
hyperparameters O
and O
results O
on O
the O
dev O
set. O
Ta- O
ble O
5 O
shows O
the O
test O
set O
results O
as O
reported O
on O
the O
Codalab O
platform. O
The O
proposed O
system O
achieved O
rank O
two. O
7 O
Ablation O
Studies O
We O
conducted O
two O
ablation O
studies O
to O
better O
un- O
derstand O
our O
proposed O
approach O
and O
the O
problem O
setting O
7.1 O
Role O
of O
Each O
Language O
Model O
In O
the O
first O
ablation O
study, O
we O
inspect O
the O
role O
of O
each O
language O
model O
described O
in O
the O
ensemble O
solution. O
We O
observe O
the O
performance O
by O
remov- O
ing O
one O
model O
at O
a O
time. O
Table O
4 O
shows O
the O
re- O
sults O
for O
the O
same. O
We O
see O
that O
removing O
even O
one O
language O
model O
degrades O
the O
overall O
performance. O
This O
builds O
confidence O
in O
our O
choice O
and O
intuition O
behind O
each O
language O
model O
for O
the O
ensemble O
so- O
lution, O
and O
each O
of O
the O
three O
language O
models O
is O
essential O
for O
our O
task. O
7.2 O
Sentence O
Level O
Treatment O
of O
the O
Task O
This O
ablation O
study O
inspects O
the O
model’s O
perfor- O
mance O
if O
we O
treat O
the O
input O
at O
a O
sentence O
level. O
Specifically, O
instead O
of O
inputting O
the O
entire O
essay O
to O
the O
Roberta-base, B-MethodName
we O
input O
the O
essay O
separated O
into O
individual O
sentences. O
We O
break O
the O
essay O
into O
sentences O
and O
separate O
them O
using O
a O
special O
token O
used O
in O
Roberta-base O
to O
separate O
sequences. O
Ta- O
ble O
4 O
shows O
the O
result O
of O
this O
ablation O
study. O
For O
a O
fair O
comparison, O
we O
compare O
results O
between O
a O
Roberta-base B-MethodName
model O
fed O
the O
entire O
sequence, O
and O
a O
Roberta-base B-MethodName
model O
fed O
the O
sentence O
separated O
sequence. O
We O
see O
that O
a O
Roberta-base B-MethodName
model O
that O
is O
fed O
the O
entire O
sequence O
performs O
better O
than O
a O
Roberta-base O
model O
that O
is O
fed O
a O
sentence-separated O
sequence. O
This O
suggests O
that O
we O
need O
to O
look O
at O
the O
entire O
sequence O
for O
a O
holistic O
understanding O
of O
the O
emotion, O
and O
we O
cannot O
just O
rely O
on O
sentence-level O
information. O
8 O
Conclusion O
In O
this O
work, O
we O
explore O
the O
task O
of O
emotion O
predic- O
tion O
at O
an O
essay O
level. O
We O
first O
explore O
different O
lan- O
guage O
models O
and O
identify O
Roberta-base B-MethodName
and O
Bart- B-MethodName
large I-MethodName
suitable O
for O
the O
task. O
Next, O
we O
observe O
that O
adding O
an O
additional O
layer O
of O
transfer O
learning O
by O
finetuning O
on O
a O
sentence-level O
dataset O
helps O
identify O
essays O
with O
very O
strong O
emotional O
sentences. O
Build-ing O
on O
these O
two O
hypotheses, O
we O
propose O
an O
ensem- O
ble O
solution O
that O
combines O
the O
linguistic O
knowledge O
of O
Roberta-base, B-MethodName
Bart-large B-MethodName
and O
Roberta-base B-MethodName
fine- O
tuned O
on O
the O
GoEmotions B-DatasetName
dataset. O
Our O
proposed O
solution O
achieved O
a O
macro B-MetricName
F1 I-MetricName
score O
of O
0.585 B-MetricValue
and O
was O
ranked O
one O
globally O
(the O
current O
top O
team O
on O
leaderboard O
submitted O
after O
the O
deadline). O