Proceedings O
of O
SemEval-2016 O
, O
pages O
879–886, O
San O
Diego, O
California, O
June O
16-17, O
2016. O
c O
2016 O
Association O
for O
Computational O
Linguistics O
SemanticZ O
at O
SemEval-2016 O
Task O
3: O
Ranking B-TaskName
Relevant I-TaskName
Answers I-TaskName
in I-TaskName
Community I-TaskName
Question I-TaskName
Answering I-TaskName
Using O
Semantic O
Similarity O
Based O
on O
Fine-tuned O
Word O
Embeddings O
Todor O
Mihaylov O
Research O
Training O
Group O
AIPHES O
Institute O
for O
Computational O
Linguistics O
Heidelberg O
University O
mihaylov@cl.uni-heidelberg.dePreslav O
Nakov O
Qatar O
Computing O
Research O
Institute, O
HBKU O
P.O. O
box O
5825 O
Doha, O
Qatar O
pnakov@qf.org.qa O
Abstract O
We O
describe O
our O
system O
for O
ﬁnding O
good O
an- O
swers O
in O
a O
community O
forum, O
as O
deﬁned O
in O
SemEval-2016, O
Task O
3 O
on O
Community O
Ques- O
tion O
Answering. O
Our O
approach O
relies O
on O
sev- O
eral O
semantic O
similarity O
features O
based O
on O
ﬁne- O
tuned O
word O
embeddings O
and O
topics O
similar- O
ities. O
In O
the O
main O
Subtask O
C, O
our O
primary O
submission O
was O
ranked O
third, O
with O
a O
MAP B-MetricName
of O
51.68 B-MetricValue
and O
accuracy B-MetricName
of O
69.94. B-MetricValue
In O
Subtask O
A, O
our O
primary O
submission O
was O
also O
third, O
with O
MAP B-MetricName
of O
77.58 B-MetricValue
and O
accuracy B-MetricName
of O
73.39. B-MetricValue
1 O
Introduction O
Posting O
questions O
that O
have O
already O
been O
asked O
and O
answered O
in O
a O
community O
forum O
is O
annoying O
to O
users O
as O
it O
usually O
ends O
up O
with O
them O
being O
referred O
to O
a O
previously O
asked O
question. O
The O
SemEval-2016 O
Task O
3 O
on O
Community O
Question O
Answering1(Nakov O
et O
al., O
2016) O
aims O
to O
solve O
this O
real-life O
problem. O
The O
main O
subtask O
(Subtask O
C) O
asks O
to O
ﬁnd O
an O
answer O
that O
al- O
ready O
exists O
in O
the O
forum O
and O
will O
be O
appropriate O
as O
a O
response O
to O
a O
newly-posted O
question. O
There O
is O
also O
a O
secondary, O
Subtask O
A, O
which O
focuses O
on O
Question- O
Comment O
Similarity O
and O
asks O
to O
rank O
the O
comments O
within O
a O
question-comment O
thread O
based O
on O
their O
rel- O
evance O
with O
respect O
to O
the O
thread’s O
question. O
Here, O
we O
examine O
the O
performance O
of O
us- O
ing O
different O
word O
embeddings O
obtained O
with O
the O
Word2Vec B-MethodName
tool O
(Mikolov O
et O
al., O
2013), O
which O
we O
use O
to O
build O
vectors O
for O
the O
questions O
and O
the O
answers. O
We O
train O
classiﬁers O
using O
features O
derived O
from O
these O
embeddings O
to O
solve O
subtasks O
A O
and O
C. O
1http://alt.qcri.org/semeval2016/task3/Our O
contribution O
is O
in O
producing O
good O
word O
em- O
beddings O
based O
on O
empirical O
evaluation O
of O
different O
conﬁgurations O
working O
in O
the O
Community O
Question O
Answering O
domain; O
as O
they O
perform O
well, O
we O
make O
them O
freely O
available O
to O
the O
research O
community.2 O
2 O
Related O
Work O
This O
year’s O
SemEval-2016 O
Task O
3 O
is O
a O
follow O
up O
of O
SemEval-2015 O
Task O
3 O
on O
Answer O
Selection O
in O
Com- O
munity O
Question O
Answering O
(Nakov O
et O
al., O
2015). O
The O
2015 O
subtask O
A O
asked O
to O
determine O
whether O
an O
answer O
was O
relevant, O
potentially O
useful, O
or O
bad, O
while O
this O
year O
this O
is O
about O
ranking. O
Here O
we O
focus O
on O
features O
that O
use O
semantic O
knowledge O
such O
as O
word O
embeddings, O
various O
fea- O
tures O
extracted O
from O
word O
embeddings, O
and O
topic O
models. O
Word O
embeddings O
and O
word O
embeddings O
similarities O
have O
been O
used O
by O
teams O
in O
the O
2015 O
edi- O
tion O
of O
the O
task O
(Belinkov O
et O
al., O
2015; O
Zamanov O
et O
al., O
2015; O
Tran O
et O
al., O
2015; O
Nicosia O
et O
al., O
2015). O
LDA B-MethodName
topic O
have O
also O
been O
used O
(Tran O
et O
al., O
2015). O
Many O
other O
features O
have O
been O
tried O
for O
the O
task. O
For O
example, O
Tran O
et O
al. O
(2015) O
used O
metadata O
about O
the O
question O
and O
the O
comment. O
User O
proﬁle O
statis- O
tics O
such O
as O
number O
of O
Good O
,Bad O
andPotentially O
Useful O
comments O
by O
a O
given O
user O
have O
been O
used O
to O
model O
user O
likelihood O
of O
posting O
different O
types O
of O
comment O
(Nicosia O
et O
al., O
2015). O
V O
o O
et O
al. O
(2015) O
and O
Nicosia O
et O
al. O
(2015) O
used O
syntactic O
tree O
similarities O
to O
compare O
questions O
to O
comments. O
The O
problem O
of O
selecting O
relevant O
answers O
has O
even O
been O
approached O
as O
a O
spam O
ﬁltering O
task O
(V O
o O
et O
al., O
2015). O
2https://github.com/tbmihailov/ O
semeval2016-task3-cqa8793 O
Data O
In O
our O
experiments, O
we O
used O
annotated O
training, O
de- O
velopment O
and O
testing O
datasets, O
as O
well O
as O
a O
large O
unannotated O
dataset, O
all O
provided O
by O
the O
SemEval- O
2016 O
Task O
3 O
organizers. O
We O
further O
collected O
some O
additional O
unannotated O
in-domain O
data O
from O
some O
other O
sources, O
as O
explained O
below; O
ﬁnally, O
we O
used O
some O
models O
pretrained O
on O
out-of-domain O
data. O
Training, O
development, O
and O
testing O
data. O
For O
Subtask O
A, O
there O
are O
6,398 O
questions O
and O
40,288 O
comments O
from O
their O
question-answer O
threads, O
and O
for O
Subtask O
C, O
there O
are O
317 O
original O
questions, O
3,169 O
related O
questions, O
and O
31,690 O
comments. O
For O
both O
subtasks, O
the O
comments O
are O
annotated O
as O
Good O
, O
PotentiallyUseful O
andBad; O
for O
subtask O
A, O
the O
an- O
notation O
is O
with O
respect O
to O
the O
question O
in O
whose O
thread O
the O
comment O
appeared, O
while O
for O
subtask O
C, O
it O
is O
with O
respect O
to O
a O
new O
question. O
For O
both O
sub- O
tasks, O
a O
successful O
ranking O
is O
one O
that O
ranks O
all O
Good O
comments O
before O
all O
PotentiallyUseful O
andBadones O
(without O
distinguishing O
between O
the O
latter O
two). O
Unannotated O
data. O
We O
performed O
experiments O
with O
Word2Vec O
embeddings O
trained O
on O
different O
unannotated O
data O
sources. O
We O
wanted O
to O
ﬁnd O
the O
best O
performing O
embeddings O
and O
to O
use O
them O
in O
our O
sys- O
tem. O
In O
Table O
1, O
we O
list O
the O
various O
data O
sources O
we O
used O
for O
training O
our O
Word2Vec B-MethodName
models, O
and O
their O
vocabulary O
size. O
Qatar O
Living O
Forum O
is O
the O
original O
Qatar O
Living.3 O
unannotated O
data O
containing O
189,941 O
questions O
and O
1,894,456 O
comments. O
It O
is O
limited O
to O
the O
forums O
sec- O
tion O
of O
the O
Qatar O
Living O
website. O
Qatar B-DatasetName
Living I-DatasetName
Forum I-DatasetName
+ I-DatasetName
Ext I-DatasetName
includes O
the O
Qatar O
Liv- O
ing O
Forum O
dataset, O
i.e., O
the O
forums, O
but O
also O
some O
other O
sections O
of O
Qatar O
Living: O
Jobs, O
Classiﬁeds, O
Pages, O
Wiki O
and O
Events O
posts. O
Doha B-DatasetName
News I-DatasetName
is O
a O
dataset O
that O
we O
built O
by O
crawl- O
ing O
about O
7,000 O
news O
publications O
about O
the O
life O
in O
Doha, O
Qatar O
from O
the O
DohaNews O
website.4 O
We O
also O
used O
an O
out-of-domain, O
general O
model, O
which O
is O
readily-pretrained O
using O
Word2Vec B-MethodName
on O
Google B-DatasetName
News I-DatasetName
,5as O
provided O
by O
Mikolov O
et O
al. O
(2013). O
3www.qatarliving.com O
is O
an O
online O
community O
for O
everyone O
living O
in O
or O
interested O
in O
the O
State O
of O
Qatar. O
4dohanews.co O
covers O
breaking O
news, O
politics, O
business, O
culture O
and O
more O
in O
and O
around O
Qatar. O
5code.google.com/archive/p/word2vec/Features O
Train O
size O
Vocab O
Qatar O
Living O
Forum O
61.84M O
104K O
Qatar O
Living O
Forum+Ext O
90M O
126K O
Google O
News O
100B O
3M O
Doha O
News O
1.45M O
17K O
Table O
1: O
Data O
used O
for O
training O
word O
embedding O
vectors. O
Shown O
are O
training O
source O
size O
(word O
tokens) O
and O
vocabulary O
size O
(word O
types). O
4 O
Method O
Below O
we O
focus O
our O
explanation O
on O
subtask O
A; O
for O
subtask O
C, O
we O
combine O
the O
predictions O
for O
subtask O
A O
with O
the O
Google’s O
reciprocal O
rank O
for O
the O
related O
question O
(see O
below). O
We O
approach O
subtask O
A O
as O
a O
classiﬁcation O
prob- O
lem. O
For O
each O
comment, O
we O
extract O
variety O
of O
fea- O
tures O
from O
both O
the O
question O
and O
the O
comment, O
and O
we O
train O
a O
classiﬁer O
to O
label O
comments O
as O
Good O
or O
Bad O
with O
respect O
to O
the O
thread O
question. O
We O
rank O
the O
comments O
in O
each O
question O
according O
to O
the O
classi- O
ﬁer’s O
score O
of O
being O
classiﬁed O
as O
Good O
with O
respect O
to O
the O
question. O
We O
ﬁrst O
train O
several O
word O
embedding O
vector O
models O
and O
we O
ﬁne-tune O
them O
using O
different O
con- O
ﬁgurations. O
For O
ﬁne-tuning O
the O
parameters O
of O
the O
word O
embeddings O
training O
conﬁguration, O
we O
setup O
a O
simple O
baseline O
system O
and O
we O
evaluate O
it O
on O
the O
of- O
ﬁcial O
MAP B-MetricName
score. O
We O
then O
use O
the O
best-performing O
embeddings O
in O
our O
further O
experiments. O
Our O
main O
features O
are O
semantic O
similarity O
based O
on O
word O
em- O
beddings O
and O
topics, O
but O
we O
also O
use O
some O
metadata O
features. O
4.1 O
Preprocessing O
Before O
extracting O
features, O
we O
preprocessed O
the O
in- O
put O
text O
using O
several O
steps. O
We O
ﬁrst O
replaced O
URLs O
in O
text O
with O
TOKEN O
URL, O
numbers O
with O
TO- O
KEN O
NUM, O
images O
with O
TOKEN O
IMG, O
and O
emoti- O
cons O
with O
TOKEN O
EMO. O
We O
then O
tokenized O
the O
text O
by O
matching O
only O
continuous O
alphabet O
characters O
in- O
cluding O
(underscore). O
Next, O
we O
lowercased O
the O
re- O
sult. O
For O
the O
training, O
the O
development, O
and O
the O
test O
datasets, O
we O
removed O
the O
stopwords O
using O
the O
En- O
glish O
stopwords O
lexicon O
from O
the O
NLTK O
toolkit O
(Bird O
and O
Loper, O
2004).8804.2 O
Features O
We O
used O
several O
semantic O
vector O
similarity O
and O
metadata O
feature O
groups. O
For O
the O
similarity O
measures O
mentioned O
below, O
we O
used O
cosine B-MetricName
similarity: I-MetricName
1−u.v O
/bardblu/bardbl./bardblv/bardbl(1) O
Semantic O
Word O
Embeddings. O
We O
used O
seman- O
tic O
word O
embeddings O
obtained O
from O
Word2Vec B-MethodName
mod- O
els O
trained O
on O
different O
unannotated O
data O
sources O
in- O
cluding O
the O
QatarLiving O
and O
DohaNews. O
We O
also O
used O
a O
model O
pre-trained O
on O
Google O
News O
text. O
For O
each O
piece O
of O
text O
such O
as O
comment O
text, O
question O
body O
and O
question O
subject, O
we O
constructed O
the O
cen- O
troid O
vector O
from O
the O
vectors O
of O
all O
words O
in O
that O
text O
(excluding O
stopwords). O
centroid O
(w1..n) O
=n/summationtext O
i=1wi O
n(2) O
We O
built O
centroid O
vectors O
(2) O
from O
the O
question O
body O
and O
the O
comment O
text. O
We O
then O
examined O
dif- O
ferent O
Word2Vec O
models O
in O
terms O
of O
training O
source O
and O
training O
conﬁguration O
including O
word B-HyperparameterName
vector I-HyperparameterName
size, I-HyperparameterName
training B-HyperparameterName
window I-HyperparameterName
size, I-HyperparameterName
minimum B-HyperparameterName
word I-HyperparameterName
occur- I-HyperparameterName
rence I-HyperparameterName
in O
the O
corpus, O
and O
number B-HyperparameterName
of I-HyperparameterName
skip-grams. I-HyperparameterName
Semantic O
Vector O
Similarities. O
We O
used O
vari- O
ous O
similarity O
features O
calculated O
using O
the O
centroid O
word O
vectors O
on O
the O
question O
body, O
on O
the O
question O
subject O
and O
on O
the O
comment O
text, O
as O
well O
as O
on O
parts O
thereof: O
Question O
to O
Answer O
similarity. O
We O
assume O
that O
a O
relevant O
answer O
should O
have O
a O
centroid O
vector O
that O
is O
close O
to O
that O
for O
the O
question. O
We O
used O
the O
ques- O
tion O
body O
to O
comment O
text, O
and O
question O
subject O
to O
comment O
text O
vector O
similarities. O
Maximized O
similarity. O
We O
ranked O
each O
word O
in O
the O
answer O
text O
to O
the O
question O
body O
centroid O
vector O
according O
to O
their O
similarity O
and O
we O
took O
the O
average O
similarity O
of O
the O
top O
Nwords. O
We O
took O
the O
top O
1,2,3 O
and O
5 O
words O
similarities O
as O
features. O
The O
assumption O
here O
is O
that O
if O
the O
average O
similarity O
for O
the O
top O
N O
most O
similar O
words O
is O
high, O
then O
the O
answer O
might O
be O
relevant. O
Aligned O
similarity. O
For O
each O
word O
in O
the O
question O
body, O
we O
chose O
the O
most O
similar O
word O
from O
the O
com- O
ment O
text O
and O
we O
took O
the O
average O
of O
all O
best O
word O
pair O
similarities O
as O
suggested O
in O
(Tran O
et O
al., O
2015).Part O
of O
speech O
(POS) O
based O
word O
vector O
similar- O
ities. O
We O
performed O
part O
of O
speech O
tagging O
using O
the O
Stanford O
tagger O
(Toutanova O
et O
al., O
2003), O
and O
we O
took O
similarities O
between O
centroid O
vectors O
of O
words O
with O
a O
speciﬁc O
tag O
from O
the O
comment O
text O
and O
the O
centroid O
vector O
of O
the O
words O
with O
a O
speciﬁc O
tag O
from O
the O
question O
body O
text. O
The O
assumption O
is O
that O
some O
parts O
of O
speech O
between O
the O
question O
and O
the O
com- O
ment O
might O
be O
closer O
than O
other O
parts O
of O
speech. O
Word O
clusters O
(WC) O
similarity. O
We O
clustered O
the O
word O
vectors O
from O
the O
Word2Vec B-MethodName
vocabulary O
in O
1,000 B-HyperparameterValue
clusters B-HyperparameterName
(with O
200 O
words O
per O
cluster O
on O
average) O
using O
K-Means B-MethodName
clustering. O
We O
then O
cal- O
culated O
the O
cluster O
similarity O
between O
the O
question O
body O
word O
clusters O
and O
the O
answer O
text O
word O
clus- O
ters. O
For O
all O
experiments, O
we O
used O
clusters O
obtained O
from O
the O
Word2Vec B-MethodName
model O
trained O
on O
QatarLiving B-DatasetName
forums O
with O
vector O
size B-HyperparameterName
of O
100, B-HyperparameterValue
window B-HyperparameterName
size I-HyperparameterName
10, B-HyperparameterValue
minimum B-HyperparameterName
words I-HyperparameterName
frequency I-HyperparameterName
of O
5, B-HyperparameterValue
and O
skip-gram B-HyperparameterName
1. B-HyperparameterValue
LDA B-MethodName
topic O
similarity. O
We O
performed O
topic O
clus- O
tering O
using O
Latent B-MethodName
Dirichlet I-MethodName
Allocation I-MethodName
(LDA) I-MethodName
as O
implemented O
in O
the O
gensim O
toolkit O
( O
ˇReh˚uˇrek O
and O
Sojka, O
2010) O
on O
Train1+Train2+Dev O
questions O
and O
comments. O
We O
built O
topic O
models O
with O
100 O
topics. O
For O
each O
word O
in O
the O
question O
body O
and O
for O
the O
com- O
ment O
text, O
we O
built O
a O
bag-of-topics O
with O
correspond- O
ing O
distribution, O
and O
calculated O
similarity. O
The O
as- O
sumption O
here O
is O
that O
if O
the O
question O
and O
the O
com- O
ment O
share O
similar O
topics, O
they O
are O
more O
likely O
to O
be O
relevant O
to O
each O
other. O
Metadata. O
In O
addition O
to O
the O
semantic O
features O
described O
above, O
we O
also O
used O
some O
common O
sense O
metadata O
features: O
Answer O
contains O
a O
question O
mark. O
If O
the O
com- O
ment O
has O
an O
question O
mark, O
it O
may O
be O
another O
ques- O
tion, O
which O
might O
indicate O
a O
bad O
answer. O
Answer O
length. O
The O
assumption O
here O
is O
that O
longer O
answers O
could O
bring O
more O
useful O
detail. O
Question O
length. O
If O
the O
question O
is O
longer, O
it O
may O
be O
more O
clear, O
which O
may O
help O
users O
give O
a O
more O
relevant O
answer. O
Question O
to O
comment O
length. O
If O
the O
question O
is O
long O
and O
the O
answer O
is O
short, O
it O
may O
be O
less O
relevant. O
The O
answer’s O
author O
is O
the O
same O
as O
the O
corre- O
sponding O
question’s O
author. O
If O
the O
answer O
is O
posted O
by O
the O
same O
user O
who O
posted O
the O
question O
and O
it O
is O
relevant, O
why O
has O
he/she O
asked O
the O
question O
in O
the O
ﬁrst O
place?881Answer O
rank O
in O
the O
thread. O
Earlier O
answers O
could O
be O
posted O
by O
users O
who O
visit O
the O
forum O
more O
often, O
and O
they O
may O
have O
read O
more O
similar O
questions O
and O
answers. O
Moreover, O
discussion O
in O
the O
forum O
tends O
to O
diverge O
from O
the O
question O
over O
time. O
Question O
category. O
We O
took O
the O
category O
of O
the O
question O
as O
a O
sparse O
binary O
feature O
vector O
(a O
feature O
with O
a O
value O
of O
1 O
appears O
if O
question O
is O
in O
the O
cat- O
egory). O
The O
assumption O
here O
is O
that O
the O
question- O
comment O
relevance O
might O
depend O
on O
the O
category O
of O
the O
question. O
4.3 O
Classiﬁer O
For O
each O
Question+Comment O
pair, O
we O
extracted O
the O
features O
explained O
above O
from O
the O
Question O
body O
and O
the O
subject O
text O
ﬁelds, O
and O
from O
the O
Comment O
text; O
we O
also O
extracted O
the O
relevant O
metadata. O
We O
concatenated O
the O
extracted O
features O
in O
a O
bag O
of O
fea- O
tures O
vector, O
scaling O
them O
in O
the O
0 O
to O
1 O
range, O
and O
feeding O
them O
to O
a O
classiﬁer. O
In O
our O
experiments, O
we O
used O
different O
feature O
conﬁgurations. O
We O
used O
L2- O
regularized O
logistic O
regression O
classiﬁer O
as O
imple- O
mented O
in O
Liblinear O
(Fan O
et O
al., O
2008). O
For O
most O
of O
our O
experiments, O
we O
tuned O
the O
classiﬁer O
with O
differ- O
ent O
values O
of O
the O
C O
(cost) O
parameter, O
and O
we O
took O
the O
one O
that O
yielded O
the O
best O
accuracy O
on O
5-fold O
cross- O
validation O
on O
the O
training O
set. O
We O
used O
binary O
clas- O
siﬁcation O
Good O
vs.Bad O
(including O
both O
Bad O
and O
Potentially O
Useful O
original O
labels). O
The O
output O
of O
the O
evaluation O
for O
each O
test O
example O
was O
a O
label, O
either O
Good O
orBad, O
and O
the O
probability O
of O
being O
Good O
in O
the O
0 O
to O
1 O
range. O
We O
then O
used O
this O
output O
proba- O
bility O
as O
a O
relevance O
rank O
for O
each O
Comment O
in O
the O
Question O
thread. O
5 O
Experiments O
and O
Evaluation O
As O
explained O
above, O
we O
rely O
mainly O
on O
semantic O
fea- O
tures O
extracted O
from O
Word2Vec B-MethodName
word O
embeddings. O
Thus, O
we O
ran O
several O
experiments O
looking O
for O
the O
best O
embeddings O
for O
the O
task. O
Table O
2 O
shows O
experiments O
with O
Word2Vec B-MethodName
mod- I-MethodName
els I-MethodName
trained O
on O
the O
unannotated O
datasets O
described O
above. O
The O
Google B-MethodName
News I-MethodName
Word2Vec I-MethodName
model I-MethodName
comes O
pretrained O
with O
vector B-HyperparameterName
size I-HyperparameterName
of O
300, B-HyperparameterValue
window B-HyperparameterName
10, B-HyperparameterValue
min- B-HyperparameterName
imum I-HyperparameterName
word I-HyperparameterName
frequency I-HyperparameterName
of O
10 B-HyperparameterValue
and O
skip-gram B-HyperparameterName
1. B-HyperparameterValue
We O
started O
with O
training O
our O
three O
Word2Vec B-MethodName
models O
us- O
ing O
the O
same O
parameters.Dev2016 O
Dataset O
MAP O
Accuracy B-MetricName
Qatar O
Living O
Forum O
0.6311 O
0.7078 O
Qatar O
Living O
Forum+Ext O
0.6269 O
0.7131 O
Google O
News O
0.6113 O
0.6996 O
Doha O
News O
0.5769 O
0.6844 O
Table O
2: O
Semantic O
vectors O
trained O
on O
different O
unanno- O
tated O
datasets O
as O
the O
only O
features O
for O
subtask O
A: O
training O
on O
train2016-part1, O
testing O
on O
dev2016. O
Table O
2 O
shows O
results O
using O
raw O
word O
vectors O
as O
features, O
together O
with O
an O
extra O
feature O
for O
question O
body O
to O
comment O
cosine O
similarity. O
We O
can O
see O
that O
training O
on O
Qatar O
Living O
Forum O
data O
performs O
best O
followed O
by O
using O
Qatar O
Living O
Forum+Ext O
,Google O
News O
, O
and O
Doha O
News O
. O
This O
is O
not O
surprising O
as O
the O
ﬁrst O
two O
datasets O
are O
in-domain, O
while O
the O
latter O
two O
cover O
more O
topics O
(as O
they O
are O
news) O
and O
more O
for- O
mal O
language. O
Overall, O
Doha B-DatasetName
News I-DatasetName
contains O
topics O
that O
largely O
overlap O
with O
the O
topics O
discussed O
in O
the O
Qatar B-DatasetName
Living I-DatasetName
forum; I-DatasetName
yet, O
it O
uses O
more O
formal O
lan- O
guage O
and O
contains O
very O
little O
conversational O
word O
types O
(mostly O
in O
quotations O
and O
interviews); O
more- O
over, O
being O
smaller O
in O
size, O
it O
covers O
much O
less O
vo- O
cabulary. O
Based O
on O
these O
preliminary O
experiments O
on O
Dev2016, O
we O
concluded O
that O
the O
domain-speciﬁc O
word O
vectors O
trained O
on O
Qatar B-DatasetName
Living I-DatasetName
Forum I-DatasetName
were O
the O
best O
for O
this O
task, O
and O
we O
used O
them O
further O
in O
our O
experiments.882After O
we O
have O
selected O
the O
best O
dataset O
for O
training O
our O
semantic O
vectors, O
we O
continued O
with O
various O
ex- O
periments O
to O
select O
the O
best O
training O
parameters O
for O
Word2Vec. O
Below O
we O
present O
the O
results O
of O
these O
experiments O
on O
Test2016, O
but O
we O
experimented O
with O
Dev2016 O
when O
developing O
our O
system. O
In O
Table O
3, O
we O
present O
experiments O
with O
different O
vector O
sizes. O
We O
trained O
our O
classiﬁer O
with O
all O
fea- O
tures O
mentioned O
above, O
extracted O
for O
the O
correspond- O
ing O
word O
vector O
model. O
We O
can O
see O
that O
word O
vectors O
of O
size O
800 O
perform O
best O
followed O
by O
sizes O
400 O
and O
700. O
However, O
we O
should O
note O
that O
using O
word B-HyperparameterName
vec- I-HyperparameterName
tors I-HyperparameterName
of O
size O
800 B-HyperparameterValue
generates O
more O
than O
1,650 B-HyperparameterValue
features O
(800+800+other O
features), O
which O
slows O
down O
train- O
ing O
and O
evaluation. O
Moreover, O
in O
our O
experiments, O
we O
noticed O
that O
using O
large O
word O
vectors O
blurs O
the O
impact O
of O
the O
other, O
non-vector O
features. O
Thus, O
next O
we O
tried O
to O
achieve O
the O
MAP B-MetricName
for O
the O
800-size B-HyperparameterValue
vector B-HyperparameterName
by O
using O
better O
parameters O
for O
smaller O
vector O
sizes. O
Table O
4 O
shows O
the O
results, O
where O
we O
used O
vectors B-HyperparameterName
of I-HyperparameterName
size I-HyperparameterName
100 B-HyperparameterValue
and O
200. B-HyperparameterValue
We O
can O
see O
that O
the O
conﬁguration O
with O
word O
vector B-HyperparameterName
size I-HyperparameterName
200, B-HyperparameterValue
window B-HyperparameterName
size I-HyperparameterName
5, B-HyperparameterValue
minimum B-HyperparameterName
word I-HyperparameterName
frequency I-HyperparameterName
1 B-HyperparameterValue
and O
skip-gram B-HyperparameterName
3 B-HyperparameterValue
performed O
best O
improving O
the O
200 O
vec- O
tors O
MAP B-MetricName
by O
0.31 B-MetricValue
(compared O
to O
Table O
3). O
However, O
the O
experiments O
with O
word O
vector B-HyperparameterName
size I-HyperparameterName
100 B-HyperparameterValue
improved O
its O
MAP B-MetricName
score O
by O
0.85, B-MetricValue
which O
suggests O
that O
there O
might O
be O
potential O
for O
improvement O
when O
using O
vec- O
tors O
of O
smaller O
size. O
We O
also O
tried O
to O
use O
Doc2Vec B-MethodName
(Le O
and O
Mikolov, O
2014) O
instead O
of O
Word2Vec, B-MethodName
but O
this O
led O
to O
noticeably O
lower O
performance. O
We O
further O
experimented O
with O
Word2Vec O
models O
trained O
with O
different O
conﬁgurations O
and O
different O
feature O
groups. O
Tables O
5 O
and O
6 O
show O
the O
results O
for O
ablation O
experiments O
using O
the O
best-performing O
con- O
ﬁguration O
for O
Subtask O
A O
and O
C, O
respectively. O
For O
Subtask O
A O
we O
achieved O
the O
best O
score O
with O
semantic O
vectors B-HyperparameterName
of I-HyperparameterName
size I-HyperparameterName
200, B-HyperparameterValue
trained O
with O
window B-HyperparameterName
size I-HyperparameterName
5, B-HyperparameterValue
minimum B-HyperparameterName
word I-HyperparameterName
frequency I-HyperparameterName
1 B-HyperparameterValue
and O
skip-grams B-HyperparameterName
3. B-HyperparameterValue
The O
best O
score O
we O
achived O
(MAP B-MetricName
78.52) O
is O
slightly O
hbetter O
than O
the O
best O
score O
from O
Table O
3 O
(MAP O
78.45), O
which O
means O
that O
it O
may O
be O
a O
good O
idea O
to O
use O
smaller O
word O
vectors O
in O
combination O
with O
other O
features. O
We O
can O
see O
that O
the O
features O
that O
con- O
tribute O
most O
(the O
bottom O
features O
are O
better) O
are O
the O
raw O
word O
centroid O
vectors O
and O
metadata O
features, O
followed O
by O
various O
similarities O
such O
as O
LDA B-MethodName
topic O
similarity O
and O
POS-tagged-word O
similarity.Test2016 O
Vectors O
used O
as O
features O
for O
subtask O
A O
(together O
with O
all O
other O
features): O
training O
on O
train2016-part1, O
testing O
on O
test2016. O
Word2Vec B-MethodName
is O
trained O
with O
word B-HyperparameterName
vector I-HyperparameterName
size I-HyperparameterName
100, B-HyperparameterValue
context B-HyperparameterName
window I-HyperparameterName
5, B-HyperparameterValue
minimum B-HyperparameterName
word I-HyperparameterName
frequency I-HyperparameterName
1, B-HyperparameterValue
and O
skip-grams B-HyperparameterName
1. B-HyperparameterValue
For O
Subtask O
C, O
we O
achieved O
the O
best O
score O
with O
vectors B-HyperparameterName
of I-HyperparameterName
size I-HyperparameterName
100, B-HyperparameterValue
trained O
with O
window B-HyperparameterName
size I-HyperparameterName
5, B-HyperparameterValue
min- B-HyperparameterName
imum I-HyperparameterName
word I-HyperparameterName
frequency I-HyperparameterName
1, B-HyperparameterValue
and O
skip-grams B-HyperparameterName
1. B-HyperparameterValue
The O
fea- O
tures O
that O
contributed O
most O
were O
mostly O
the O
same O
as O
for O
Subtask O
A. O
One O
difference O
is O
the O
maximized O
sim- O
ilarity O
features O
group, O
which O
now O
yields O
worse O
re- O
sults O
when O
excluded, O
which O
indicates O
its O
importance. O
Our O
Primary O
,Contrastive O
1 O
andContrastive O
2 O
submissions O
were O
built O
with O
the O
same O
feature O
set: O
All O
features O
- O
POS O
similarity O
& O
Meta O
Category O
, O
but O
were O
trained O
with O
ﬁxed O
C=0.55 O
on O
different O
datasets: O
Pri- O
mary O
was O
trained O
on O
Train2016-part1, O
Contrastive O
1 O
was O
trained O
on O
Train2016-part1 O
+ O
Train2016-part2, O
andContrastive O
2 O
was O
trained O
on O
Train2016-part2. O
6 O
Conclusion O
and O
Future O
Work O
We O
have O
described O
our O
system O
for O
SemEval-2016, O
Task O
3 O
on O
Community O
Question O
Answering. O
Our O
approach O
relied O
on O
several O
semantic O
similarity O
fea- O
tures O
based O
on O
ﬁne-tuned O
word O
embeddings O
and O
top- O
ics O
similarities.In O
the O
main O
Subtask O
C, O
our O
primary O
submission O
was O
ranked O
third, O
with O
a O
MAP B-MetricName
of O
51.68 B-MetricValue
and O
accu- B-MetricName
racy I-MetricName
of O
69.94. B-MetricValue
In O
Subtask O
A, O
our O
primary O
submission O
was O
also O
third, O
with O
MAP B-MetricName
of O
77.58 B-MetricValue
and O
accuracy O
of O
73.39. O
After O
the O
submission O
deadline, O
we O
improved O
our O
MAP B-MetricName
score O
to O
78.52 B-MetricValue
for O
Subtask O
A, O
and O
to O
53.39 B-MetricValue
for O
Subtask O
C, O
which O
would O
rank O
our O
system O
second. O
In O
future O
work, O
we O
plan O
to O
use O
our O
best O
perform- O
ing O
word O
embeddings O
models O
and O
features O
in O
a O
deep O
learning O
architecture, O
e.g., O
as O
in O
the O
MTE-NN O
sys- O
tem O
(Guzm O
´an O
et O
al., O
2016a; O
Guzm O
´an O
et O
al., O
2016b), O
which O
borrowed O
an O
entire O
neural O
network O
frame- O
work O
and O
achitecture O
from O
previous O
work O
on O
ma- O
chine O
translation O
evaluation O
(Guzm O
´an O
et O
al., O
2015). O
We O
also O
want O
to O
incorporate O
several O
rich O
knowledge O
sources, O
e.g., O
as O
in O
the O
SUper O
Team O
system O
(Mi- O
haylova O
et O
al., O
2016), O
including O
troll O
user O
features O
as O
inspired O
by O
(Mihaylov O
et O
al., O
2015a; O
Mihaylov O
et O
al., O
2015b; O
Mihaylov O
and O
Nakov, O
2016), O
and O
PMI- O
based O
goodness O
polarity O
lexicons O
as O
in O
the O
PMI-cool O
system O
(Balchev O
et O
al., O
2016), O
as O
well O
as O
sentiment O
polarity O
features O
(Nicosia O
et O
al., O
2015). O
We O
further O
plan O
to O
use O
information O
from O
entire O
threads O
to O
make O
better O
predictions, O
as O
using O
thread- O
level O
information O
for O
answer O
classiﬁcation O
has O
al- O
ready O
been O
shown O
useful O
for O
SemEval-2015 O
Task O
3, O
subtask O
A, O
e.g., O
by O
using O
features O
modeling O
the O
thread O
structure O
and O
dialogue O
(Nicosia O
et O
al., O
2015; O
Barr´on-Cede O
˜no O
et O
al., O
2015), O
or O
by O
applying O
thread- O
level O
inference O
using O
the O
predictions O
of O
local O
classi- O
ﬁers O
(Joty O
et O
al., O
2015; O
Joty O
et O
al., O
2016). O
How O
to O
use O
such O
models O
efﬁciently O
in O
the O
ranking O
setup O
of O
2016 O
is O
an O
interesting O
research O
question. O
Finally, O
we O
would O
like O
to O
address O
subtask O
C O
in O
a O
more O
solid O
way, O
making O
good O
use O
of O
the O
data, O
the O
gold O
annotations, O
the O
features, O
the O
models, O
and O
the O
predictions O
for O
subtasks O
A O
and O
B. O