Proceedings O
of O
the O
12th O
Workshop O
on O
Computational O
Approaches O
to O
Subjectivity, O
Sentiment O
& O
Social O
Media O
Analysis O
, O
pages O
293 O
- O
303 O
May O
26, O
2022 O
c O
2022 O
Association O
for O
Computational O
Linguistics O
Tagging B-TaskName
Without I-TaskName
Rewriting: I-TaskName
A O
Probabilistic O
Model O
for O
Unpaired B-TaskName
Sentiment I-TaskName
and I-TaskName
Style I-TaskName
Transfer I-TaskName
Shuo O
Yang O
yangshuo@toki.waseda.jp O
Abstract O
Style O
transfer O
is O
the O
task O
of O
paraphrasing O
text O
into O
a O
target-style O
domain O
while O
retaining O
the O
content. O
Unsupervised O
approaches O
mainly O
fo- O
cus O
on O
training O
a O
generator O
to O
rewrite O
input O
sen- O
tences. O
In O
this O
work, O
we O
assume O
that O
text O
styles O
are O
determined O
by O
only O
a O
small O
proportion O
of O
words; O
therefore, O
rewriting O
sentences O
via O
gener- O
ative O
models O
may O
be O
unnecessary. O
As O
an O
alter- O
native, O
we O
consider O
style O
transfer O
as O
a O
sequence O
tagging O
task. O
Specifically, O
we O
use O
edit O
opera- O
tions O
(i.e., O
deletion, O
insertion O
and O
substitution) O
to O
tag O
words O
in O
an O
input O
sentence. O
We O
train O
a O
classifier O
and O
a O
language O
model O
to O
score O
tagged O
sequences O
and O
build O
a O
conditional B-MethodName
random I-MethodName
field. I-MethodName
Finally, O
the O
optimal O
path O
in O
the O
conditional O
ran- O
dom O
field O
is O
used O
as O
the O
output. O
The O
results O
of O
experiments O
comparing O
models O
indicate O
that O
our O
proposed O
model O
exceeds O
end-to-end O
base- O
lines O
in O
terms O
of O
accuracy B-MetricName
on O
both O
sentiment O
and O
style O
transfer O
tasks O
with O
comparable O
or O
bet- O
ter O
content O
preservation. O
1 O
Introduction O
Text O
style O
refers O
to O
the O
attributes O
of O
text O
written O
in O
a O
particular O
form. O
Style O
transfer O
is O
the O
task O
of O
paraphrasing O
text O
into O
a O
target-style O
domain O
while O
retaining O
its O
content. O
In O
the O
domain O
of O
natural O
lan- O
guage O
generation, O
research O
on O
style O
transfer O
tasks O
(Li O
et O
al., O
2018; O
Chawla O
and O
Yang, O
2020) O
allows O
us O
to O
control O
the O
attributes O
of O
produced O
utterances. O
Recently, O
sentiment B-TaskName
transfer I-TaskName
(Fu O
et O
al., O
2018; O
Prabhumoye O
et O
al., O
2018) O
has O
attracted O
much O
at- O
tention O
as O
a O
subtask O
of O
style O
transfer, O
an O
exam- O
ple O
being O
’ O
The O
food O
here O
is O
delicious O
’ O
(Positive) O
→’The O
food O
here O
is O
gross O
’ O
(Negative). O
A O
style- O
indicative O
word O
is O
a O
word O
with O
a O
large O
contribution O
to O
style O
(Xu O
et O
al., O
2018). O
In O
the O
above O
example, O
’delicious’ O
and O
’gross’ O
are O
style-indicative O
words. O
A O
critical O
problem O
in O
sentiment O
transfer O
is O
the O
lack O
of O
available O
parallel O
data O
(Shen O
et O
al., O
2017; O
Luo O
et O
al., O
2019). O
As O
a O
result, O
related O
work O
has O
mainly O
focused O
on O
unsupervised O
learning. O
Among O
Figure O
1: O
An O
example O
of O
our O
proposed O
approach. O
unsupervised O
approaches, O
those O
based O
on O
word O
modification O
have O
achieved O
state-of-the-art O
perfor- O
mance O
due O
to O
their O
ability O
to O
retain O
content O
words. O
This O
paper O
mainly O
focuses O
on O
sentiment O
trans- O
fer O
and O
follows O
two O
generative O
models: O
the O
TAG B-MethodName
model O
(Madaan O
et O
al., O
2020) O
and O
LEWIS B-MethodName
model O
(Reid O
and O
Zhong, O
2021). O
The O
TAG B-MethodName
model O
calcu- O
lates O
term O
frequency-inverse O
document O
frequency O
scores O
to O
identify O
style-indicative O
words O
and O
trains O
an O
autoregressive O
model O
to O
substitute O
those O
words. O
The O
LEWIS B-MethodName
model O
removes O
style-indicative O
words O
to O
extract O
a O
content O
template O
and O
trains O
a O
generator O
to O
perform O
edit O
operations O
on O
the O
template. O
However, O
the O
aforementioned O
methods O
have O
the O
following O
drawbacks: O
(1) O
It O
is O
unnecessary O
to O
identify O
style-indicative O
words. O
The O
fact O
that O
style-indicative O
words O
con- O
tribute O
more O
to O
a O
style O
does O
not O
imply O
that O
style- O
indicative O
words O
correspond O
to O
the O
optimal O
posi- O
tions O
to O
be O
modified. O
For O
a O
negative-to-positive O
transfer O
example, O
the O
sentence O
’ O
Even O
great O
restau- O
rants O
have O
bad O
days O
’ O
should O
be O
rephrased O
as O
’ O
Great O
restaurants O
never O
have O
bad O
days O
’ O
according O
to O
a O
human O
reference. O
Here, O
both O
the O
deleted O
word293’Even’ O
and O
inserted O
word O
’never’ O
are O
far O
away O
from O
the O
style-indicative O
word O
’bad’. O
Furthermore, O
word O
identification O
may O
be O
less O
effective O
for O
non- O
descriptive O
text. O
For O
example, O
if O
there O
are O
no O
style- O
indicative O
words O
in O
a O
sentence, O
such O
as O
’ O
If O
you O
are O
into O
sports, O
this O
is O
the O
place O
for O
you O
’(Positive), O
then O
identification O
will O
not O
be O
effective. O
(2) O
No O
rationale O
is O
provided O
for O
the O
collocation O
of O
operations O
used, O
and O
models O
that O
perform O
differ- O
ent O
edit O
operations O
are O
treated O
as O
different O
models O
(Li O
et O
al., O
2018; O
Madaan O
et O
al., O
2020). O
However, O
we O
propose O
that O
edit O
operations O
should O
be O
used O
au- O
tomatically O
in O
different O
situations. O
When O
multiple O
solutions O
exist, O
a O
basis O
for O
selecting O
the O
solution O
should O
be O
provided. O
(3) O
It O
is O
redundant O
to O
rewrite O
style-independent O
words O
by O
using O
purely O
generative O
methods, O
as O
over- O
laps O
have O
been O
reported O
to O
be O
common O
between O
the O
input O
and O
output O
(Reid O
and O
Zhong, O
2021). O
Rewrit- O
ing O
all O
input O
words O
by O
using O
an O
end-to-end O
model O
increases O
the O
burden O
of O
the O
model O
and O
reduces O
its O
performance. O
In O
theory, O
additional O
learning O
of O
these O
words O
may O
be O
more O
likely O
to O
cause O
text O
degenera- O
tion O
(Holtzman O
et O
al., O
2020). O
To O
address O
the O
above-mentioned O
drawbacks, O
we O
propose O
the O
following: O
(1) O
Tagging O
all O
words O
instead O
of O
identifying O
spe- O
cific O
words. O
We O
employ O
edit O
operations O
to O
tag O
every O
word O
in O
an O
input O
sentence. O
To O
obtain O
a O
tagger O
with- O
out O
parallel O
data, O
we O
train O
a O
style O
classifier O
to O
score O
samples O
and O
build O
a O
conditional B-MethodName
random I-MethodName
field I-MethodName
(CRF) I-MethodName
(Lafferty O
et O
al., O
2001). O
We O
use O
the O
classifier O
to O
cal- O
culate O
the O
probability O
distribution O
of O
tag O
sequences. O
(2) O
Using O
a O
language O
model O
(LM) O
to O
select O
oper- O
ations. O
If O
an O
input O
sentence O
has O
multiple O
solutions, O
we O
propose O
that O
text O
fluency O
be O
the O
basis O
for O
selec- O
tion. O
For O
example, O
a O
negative O
sentence O
’ O
I’m O
not O
a O
huge O
fan O
of O
them O
’ O
can O
be O
rephrased O
as O
’ O
I’m O
a O
huge O
fan O
of O
them O
’ O
or O
’ O
I’m O
not O
a O
small O
fan O
of O
them O
’. O
In O
this O
case, O
the O
former O
sounds O
more O
natural. O
To O
measure O
text O
fluency, O
we O
build O
an O
LM O
that O
scores O
sentences O
based O
on O
their O
perplexity. B-MetricName
We O
use O
the O
score O
function O
as O
a O
joint O
feature O
function O
of O
the O
CRF. B-MethodName
(3) O
Searching O
in O
the O
CRF B-MethodName
instead O
of O
rewriting O
the O
entire O
sentence. O
As O
mentioned O
above, O
we O
train O
a O
classifier O
and O
LM O
to O
build O
the O
CRF. O
By O
searching O
in O
the O
CRF, B-MethodName
we O
generate O
an O
operation O
sequence. O
We O
apply O
the O
operation O
sequence O
to O
the O
input O
sentence O
to O
obtain O
the O
output. O
In O
this O
paper, O
we O
first O
introduce O
our O
tagging O
strat- O
egy O
and O
a O
method O
we O
employed O
to O
implement O
editoperations O
(§ O
3.1). O
Further, O
we O
introduce O
feature O
functions O
of O
the O
CRF B-MethodName
(§ O
3.2) O
and O
search O
strategies O
used O
(§ O
3.3). O
We O
tested O
our O
model O
for O
transfer O
ac- B-MetricName
curacy I-MetricName
and O
content O
preservation O
on O
four O
data O
sets O
(§ O
4) O
and O
analysed O
the O
experimental O
results O
of O
the O
automated O
evaluation O
(§ O
5.1) O
and O
the O
experimental O
results O
of O
the O
manual O
evaluation O
(§ O
5.2). O
In O
addi- O
tional O
analysis O
(§ O
5.3), O
we O
discussed O
the O
variances O
of O
sentence O
features O
in O
transformation.1 O
Our O
contributions O
are O
as O
follows: O
•We O
propose O
a O
novel O
style O
transfer O
approach. O
To O
the O
best O
of O
our O
knowledge, O
this O
study O
is O
the O
first O
to O
apply O
CRFs B-MethodName
to O
style O
transfer O
tasks. O
•We O
propose O
a O
bias O
for O
selecting O
edit O
operations. O
The O
calculation O
of O
perplexity B-MetricName
theoretically O
pre- O
vents O
generated O
words O
from O
conflicting O
with O
their O
original O
context. O
•Experimental O
results O
show O
that O
our O
proposed O
model O
surpasses O
baselines O
in O
terms O
of O
accu- O
racy O
or O
content O
retention O
on O
four O
data O
sets. O
2 O
Related O
Work O
2.1 O
Style O
Transfer O
in O
Latent O
Space O
A O
traditional O
approach O
to O
style O
transfer O
is O
to O
disen- O
tangle O
the O
style O
and O
content O
in O
a O
latent O
space. O
For O
ex- O
ample, O
Shen O
et O
al. O
(2017) O
proposed O
a O
cross-aligned O
model O
that O
aligns O
samples O
at O
a O
shared O
hidden O
con- O
tent O
distribution O
level O
across O
different O
corporations. O
In O
other O
work, O
Fu O
et O
al. O
(2018) O
proposed O
an O
ap- O
proach O
that O
uses O
generative O
adversarial O
networks O
to O
extract O
content O
representations. O
These O
represen- O
tations O
are O
decoded O
into O
a O
target-style O
domain O
as O
outputs. O
Manipulating O
representations O
in O
a O
latent O
space O
(Hu O
et O
al., O
2017; O
Prabhumoye O
et O
al., O
2018) O
is O
the O
main O
method O
used O
in O
the O
aforementioned O
studies. O
However, O
it O
has O
been O
reported O
that O
extract- O
ing O
style O
and O
content O
representations O
from O
a O
latent O
space O
is O
very O
difficult O
(Elazar O
and O
Goldberg, O
2018). O
2.2 O
Style O
Transfer O
by O
Modifying O
Words O
Instead O
of O
extracting O
representations O
in O
a O
latent O
space, O
methods O
have O
recently O
been O
proposed O
to O
di- O
rectly O
modify O
words O
(Sudhakar O
et O
al., O
2019; O
Zhang O
et O
al., O
2018). O
Li O
et O
al. O
(2018) O
proposed O
a O
delete- O
retrieve-generate O
pipeline O
that O
transfers O
samples O
based O
on O
the O
retrieval O
of O
similar O
sentences O
and O
performs O
well O
in O
sentiment O
transfer O
tasks. O
How- O
ever, O
retrieval O
has O
been O
reported O
as O
an O
unnecessary O
1Code O
is O
available O
on O
GitHub.294step O
(Madaan O
et O
al., O
2020), O
and O
models O
that O
apply O
edit O
operations O
to O
sentences O
have O
produced O
superior O
results O
(Wu O
et O
al., O
2019; O
Reid O
and O
Zhong, O
2021). O
Malmi O
et O
al. O
(2020) O
proposed O
to O
use O
Masked O
LMs O
to O
identify O
tokens O
to O
modify. O
They O
replace O
the O
iden- O
tified O
source O
tokens O
with O
target O
tokens O
to O
transform O
text O
to O
match O
the O
style O
of O
the O
target O
domain. O
How- O
ever, O
models O
(Li O
et O
al., O
2018; O
Madaan O
et O
al., O
2020) O
based O
on O
end-to-end O
approaches O
suffer O
from O
text O
degeneration O
(Holtzman O
et O
al., O
2020). O
Instead, O
we O
leverage O
intuitions O
about O
style O
transfer O
and O
uses O
smaller O
pieces O
of O
machine O
learning O
to O
build O
a O
tar- O
geted O
model. O
In O
this O
paper, O
we O
follow O
the O
second O
approach O
of O
fine-tuning O
sentences O
at O
a O
lexical O
level. O
3 O
Methodology O
Instead O
of O
training O
an O
end-to-end O
model, O
we O
per- O
form O
a O
search O
over O
small O
edits O
to O
an O
input O
sentence, O
as O
it O
provides O
an O
interpretable O
record O
of O
the O
deci- O
sions O
the O
model O
made. O
To O
formalize O
the O
problem, O
we O
consider O
sentence O
setXA= O
(x(1) O
A, O
..., O
x(M) O
A)with O
source O
style O
Aand O
another O
sentence O
set O
XB= O
(x(1) O
B, O
..., O
x(N) O
B)with O
target O
style O
B. O
The O
sentences O
in O
these O
two O
sets O
are O
non-parallel; O
that O
is, O
x(i) O
Adoes O
not O
correspond O
to O
x(i) O
B. O
The O
objective O
is O
to O
generate O
a O
new O
sentence O
setˆX= O
(ˆx(1), O
...,ˆx(M))in O
style O
B, O
where O
ˆx(i)is O
the O
result O
of O
transferring O
x(i) O
Ainto O
style O
B. O
3.1 O
Tagger O
We O
use O
three O
basic O
edit O
operations O
to O
tag O
words O
in O
input O
sentences. O
Words O
that O
do O
not O
need O
to O
be O
mod- O
ified O
are O
tagged O
with O
’[KEEP]’, O
signifying O
that O
they O
will O
be O
retained O
in O
the O
output. O
Tags O
are O
presented O
in O
Table O
1. O
We O
note O
that O
for O
words O
tagged O
with O
‘[INS]’, O
we O
will O
only O
insert O
words O
in O
front O
of O
them. O
We O
introduce O
a O
terminator, O
denoted O
’<EOS>’, O
to O
validate O
the O
insertion O
of O
words O
at O
the O
end O
of O
an O
input O
sentence. O
The O
terminator O
can O
only O
be O
tagged O
as O
’[INS]’ O
or O
’[KEEP]’; O
that O
is, O
terminators O
are O
retained O
in O
the O
output. O
For O
reference, O
(Wu O
et O
al., O
2019) O
regarded O
insertion O
in O
front O
of O
a O
word O
and O
be- O
hind O
the O
same O
word O
as O
different O
operations, O
which O
unnecessarily O
increased O
the O
burden O
on O
the O
tagger. O
Only O
one O
word O
in O
an O
input O
sentence O
is O
modified O
in O
each O
iteration; O
that O
is, O
we O
introduce O
the O
con- O
straint O
that O
only O
one O
word O
in O
each O
sentence O
cannot O
be O
tagged O
with O
’[KEEP]’. O
We O
refer O
to O
this O
as O
a O
one- O
word O
tagging O
strategy. O
For O
example, O
the O
sentence O
in O
Figure O
1 O
is O
repeatedly O
modified O
three O
times O
toTag O
Operation O
[INS] O
Insert O
a O
word O
in O
front O
of O
the O
tagged O
word. O
[SUB] O
Substitute O
the O
tagged O
word O
with O
a O
new O
word. O
[DEL] O
Delete O
the O
tagged O
word. O
[KEEP] O
Retain O
the O
tagged O
word. O
Table O
1: O
Possible O
tags O
for O
a O
word O
and O
their O
correspond- O
ing O
word O
operations. O
produce O
the O
output. O
The O
advantage O
of O
this O
method O
is O
that O
it O
reduces O
the O
modification O
of O
content O
words. O
After O
a O
sentence O
is O
tagged, O
all O
words O
are O
sub- O
jected O
to O
the O
corresponding O
operations O
to O
generate O
a O
new O
sentence. O
We O
employ O
the O
Flexible O
Text O
Edit- O
ing O
Method O
(Mallinson O
et O
al., O
2020) O
to O
edit O
tagged O
sentences. O
For O
the O
input O
sentence O
in O
Figure O
1, O
the O
first O
word, O
’Nice’, O
is O
tagged O
as O
’[SUB]’ O
in O
the O
first O
iteration. O
We O
replace O
‘Nice’ O
with O
’Worst’ O
and O
treat O
the O
modified O
sentence O
as O
input O
to O
the O
next O
iteration. O
A O
difficult O
case O
is O
one O
in O
which O
multiple O
words O
must O
be O
inserted O
before O
a O
target O
word. O
Here, O
the O
tag O
of O
the O
target O
word O
is O
difficult O
to O
determine. O
In O
previous O
work O
(Reid O
and O
Zhong, O
2021), O
additional O
models O
were O
introduced O
to O
calculate O
the O
number O
of O
inserted O
words, O
which O
unnecessarily O
increased O
the O
burden O
on O
the O
model. O
As O
an O
alternative, O
we O
use O
the O
one-word O
tagging O
strategy O
several O
times. O
When O
the O
modified O
sentence O
has O
the O
characteristics O
of O
the O
target O
style, O
we O
stop O
the O
modification O
process O
and O
output O
the O
current O
sentence. O
To O
generate O
new O
words, O
we O
fine-tune O
a O
Bidirectional B-MethodName
Encoder I-MethodName
Representa- I-MethodName
tions I-MethodName
from O
Transformers O
(BERT) B-MethodName
model O
(Devlin O
et O
al., O
2019) O
on O
the O
target O
style O
corpus O
as O
an O
LM. O
Inspired O
by O
the O
pre-training O
process O
of O
BERT, B-MethodName
we O
employ O
a O
mask-based O
training O
policy. O
For O
each O
sen- O
tence O
in O
the O
target O
corpus, O
we O
randomly O
replace O
one O
word O
with O
a O
special O
token, O
’<MASK>’, O
and O
train O
the O
LM O
fθto O
predict O
it. O
The O
objective O
function O
is O
expressed O
as O
Equation O
(1): O
LLM(θ) O
=−/summationdisplay O
jlogp(wLM O
j=wj|cj;θ),(1) O
where O
cjis O
the O
context O
of O
a O
masked O
word O
wj.wLM O
j O
is O
the O
corresponding O
prediction O
of O
the O
LM. O
The O
trained O
LM O
is O
used O
to O
perform O
substitutions O
and O
insertions. O
For O
a O
word O
tagged O
with O
’[SUB]’, O
we O
substitute O
it O
with O
the O
token O
’<MASK>’. O
For O
a O
word O
tagged O
with O
’[INS]’, O
we O
insert O
’<MASK>’ O
in O
front O
of O
it. O
After O
this O
is O
completed, O
we O
input O
the O
masked O
sentence O
to O
the O
LM. O
The O
word O
predicted O
by O
the O
LM O
then O
replaces O
the O
mask.295Figure O
2: O
Proposed O
transfer O
approach O
with O
greedy O
search. O
In O
this O
example, O
there O
are O
three O
modifications O
between O
the O
input O
and O
output. O
nis O
the O
length O
of O
the O
sentence. O
By O
using O
three O
edit O
operations O
on O
an O
input O
sen- O
tence O
with O
nwords, O
we O
can O
generate O
3n+ O
1dif- O
ferent O
sentences. O
We O
note O
that O
this O
includes O
the O
in- O
sertion O
of O
a O
word O
at O
the O
end O
of O
the O
sentence. O
These O
new O
sentences O
are O
all O
at O
a O
Levenshtein B-MetricName
distance I-MetricName
of O
1 B-MetricValue
from O
the O
previous O
sentence. O
We O
use O
3n+ O
1differ- O
ent O
operations O
to O
modify O
the O
input O
sentence O
in O
each O
iteration. O
We O
repeatedly O
modify O
the O
input O
sentence O
until O
it O
is O
transferred O
into O
the O
target O
style O
domain. O
The O
body O
of O
our O
method O
is O
a O
random O
process, O
and O
the O
sentence O
output O
in O
each O
iteration O
is O
the O
only O
input O
in O
the O
next O
iteration. O
We O
refer O
to O
these O
3n+ O
1 O
sentence-level O
operations O
as O
states. O
We O
consider O
a O
state O
set O
S1= O
(s1 O
1, O
..., O
s3n+1 O
1), O
where O
each O
ele- O
ment O
represents O
an O
operation O
that O
is O
applied O
to O
the O
current O
sentence. O
Furthermore, O
each O
use O
of O
these O
operations O
represents O
a O
step O
of O
state O
transition. O
Con- O
tinuous O
three-step O
transition O
is O
shown O
in O
Figure O
2. O
We O
aim O
at O
calculating O
the O
transfer O
probabilities O
between O
states. O
In O
this O
random O
process, O
a O
high- O
quality O
output O
sentence O
should O
correspond O
to O
a O
path O
of O
states O
with O
higher O
transition O
probability. O
3.2 O
Conditional B-MethodName
Random I-MethodName
Field I-MethodName
As O
described, O
we O
use O
a O
style O
classifier O
and O
an O
LM O
to O
calculate O
the O
transfer O
probabilities O
between O
states. O
Specifically, O
the O
classifier O
is O
used O
to O
determine O
whether O
the O
generated O
sentences O
have O
the O
target O
style O
attributes, O
while O
the O
LM O
is O
used O
to O
ensure O
that O
these O
sentences O
have O
high O
fluency. O
We O
train O
a O
multilayer O
perceptron O
(MLP) O
as O
the O
classifier O
to O
distinguish O
sentences O
in O
two O
style O
do- O
mains. O
The O
features O
for O
the O
MLP O
classifier O
fϕis O
pre- O
trained O
word O
embedding O
vectors O
(Mikolov O
et O
al., O
2013). O
The O
loss O
function O
is O
expressed O
as O
eq O
(2): O
LCLS(ϕ) O
=−/summationdisplay O
jlogP(yj|xj;ϕ)(2) O
where O
xjis O
the O
j-th O
example O
in O
a O
train O
set O
and O
yjis O
the O
style O
label O
for O
xj.For O
concerns O
about O
inference O
speed, O
we O
follow O
the O
standard O
practice O
(Dai O
et O
al., O
2019) O
and O
train O
a O
5-gram O
LM O
by O
using O
the O
KenLM O
library O
(Heafield, O
2011) O
instead O
of O
a O
pre-trained O
neural O
LM O
to O
score O
sentences O
by O
the O
probabilities O
of O
their O
occurrence O
in O
the O
target O
corpus. O
The O
learned O
models O
are O
used O
to O
calculate O
the O
transfer O
probabilities. O
For O
sentence O
xA, O
we O
consider O
that O
it O
passes O
through O
path O
pi= O
(xA, O
sj1 O
1, O
..., O
sji O
i)and O
changes O
to O
sentence O
xpi. O
If O
we O
use O
state O
sji+1 O
i+1to O
change O
sentence O
xpito O
sentence O
xpi+1, O
the O
classifier O
compute O
score O
as O
follows: O
Sstyle(sji+1 O
i+1, O
pi) O
=P(B|xpi+1;ϕ)−P(B|xpi;ϕ). O
(3) O
Here, O
the O
score O
is O
the O
difference O
in O
the O
probabilities O
thatxpiandxpi+1are O
classified O
into O
target O
style O
B. O
Similarly, O
the O
score O
function O
calculated O
by O
the O
LM O
is O
expressed O
as O
Equation O
(4): O
Sfluency O
(sji+1 O
i+1, O
pi) O
=P(xpi+1|XB)−P(xpi|XB). O
(4) O
To O
calculate O
the O
transfer O
probabilities, O
we O
use O
the O
two O
score O
functions O
as O
feature O
functions O
to O
build O
a O
CRF B-MethodName
(Lafferty O
et O
al., O
2001). O
The O
joint O
score O
STotal(si+1,j|si,t)is O
the O
weighted O
sum O
of O
the O
two: O
STotal(sji+1 O
i+1, O
pi) O
=µ1Sstyle(sji+1 O
i+1, O
pi) O
+µ2Sfluency O
(sji+1 O
i+1, O
pi),(5) O
In O
each O
iteration, O
we O
convert O
all O
the O
scores O
into O
probabilities O
using O
Equation O
(6). O
That O
is, O
we O
in- O
put O
these O
scores O
to O
a O
softmax O
layer O
to O
compute O
the O
normalised O
probability O
distribution: O
P(pi+1|pi) O
=STotal(sji+1 O
i+1, O
pi) O
/summationtext O
ptSTotal(sji+1 O
i+1, O
pt), O
(6) O
where O
pi+1= O
(xA, O
sj1 O
1, O
..., O
sji+1 O
i+1), O
and O
ptis O
a O
path O
that O
contains O
the O
initial O
sentence O
xAandistates. O
The O
probabilities O
reflect O
the O
quality O
of O
the O
trans- O
ferred O
sentences. O
Here, O
we O
transform O
the O
style O
trans- O
fer O
problem O
into O
a O
path O
search O
problem. O
The O
Yelp, B-DatasetName
Amazon B-DatasetName
and O
IMDb B-DatasetName
data O
sets O
are O
used O
for O
sentiment B-TaskName
transfer. I-TaskName
The O
GYAFC B-DatasetName
data O
set O
is O
used O
for O
formality B-TaskName
transfer. I-TaskName
pi= O
(xA, O
sj1 O
1, O
..., O
sji O
i)representing O
consecutive O
i O
modifications, O
the O
probability O
of O
transfer O
from O
xA O
toxpiis O
the O
product O
of O
all O
probabilities O
in O
the O
path: O
P(pi|xA) O
=P(p1|xA)i/productdisplay O
k=2P(pk|pk−1).(7) O
Ifxpiis O
classified O
into O
the O
target O
style O
domain, O
we O
stop O
searching O
and O
output O
that O
sentence. O
3.3 O
Viterbi O
Search O
and O
Greedy O
Search O
To O
find O
the O
global O
optimal O
solution, O
we O
employ O
the O
Viterbi O
algorithm O
(Viterbi, O
1967). O
For O
the O
i-th O
itera- O
tion, O
we O
have O
3n+ O
1paths O
from O
the O
corresponding O
states. O
We O
suppose O
that O
the O
end O
of O
a O
path O
pj O
iis O
state O
sj O
i, O
where O
jis O
a O
variable. O
For O
path O
pj O
iin O
the O
set O
of O
paths O
(p1 O
i, O
..., O
p3n+1 O
i),sj O
imay O
be O
transferred O
to O
st O
i+1 O
in O
the O
next O
iteration. O
We O
define O
a O
function O
of O
the O
transfer O
probability O
from O
xAtost O
i+1as O
follows: O
fxA→st O
i+1(pj O
i) O
=P(pt O
i+1|pj O
i)·P(pj O
i|xA),(8) O
where O
tis O
an O
integer O
between O
1 O
and O
3n+ O
1. O
We O
select O
the O
path O
with O
the O
highest O
value O
of O
fxA→st O
i+1as O
the O
optimal O
path O
to O
state O
st O
i+1. O
In O
other O
words, O
we O
retain O
only O
one O
path O
to O
each O
state: O
pt O
i+1= O
(argmax O
fxA→st O
i+1(pj O
i), O
st O
i+1).(9) O
For O
a O
modification O
with O
isteps, O
we O
find O
the O
optimal O
path O
(xA, O
sj1 O
1, O
..., O
sji O
i)from O
path O
set O
{p1 O
i, O
..., O
p3n+1 O
i}. O
This O
signifies O
that O
sentence O
xAis O
modified O
using O
the O
operation O
sequence O
(sj1 O
1, O
..., O
sji O
i) O
and O
is O
output O
as O
the O
solution O
ˆxA. O
Because O
we O
can- O
not O
confirm O
the O
sentence O
length O
during O
the O
search- O
ing, O
we O
consider O
all O
possible O
states, O
that O
is, O
the O
number O
of O
states O
is O
incremented O
by O
one O
with O
the O
number O
of O
iterative O
steps. O
Therefore, O
the O
model O
has O
a O
time O
complexity O
of O
O(n2). O
The O
time O
cost O
is O
T(n) O
= O
9 O
kn2+ O
6kn+k, O
where O
kis O
the O
number O
of O
iterations.For O
our O
model O
to O
have O
the O
same O
time O
complexity O
as O
a O
generative O
model, O
we O
also O
use O
greedy O
search O
as O
an O
alternative O
to O
the O
Viterbi O
algorithm. O
We O
define O
the O
following O
function: O
gxA→sj O
i+1(st O
i+1) O
=p(st O
i+1|pi), O
(10) O
where O
pi= O
(xA, O
sj1 O
1, O
..., O
sji O
i). O
We O
transfer O
to O
the O
state O
that O
has O
the O
highest O
trans- O
fer O
probability O
from O
the O
current O
state O
sji O
i: O
pi+1= O
(pi,argmax O
gxA→sj O
i+1(sj O
i+1)). O
(11) O
In O
this O
case, O
there O
is O
only O
one O
sentence O
as O
input O
in O
each O
iteration. O
Therefore, O
the O
model O
has O
linear O
time O
complexity, O
O(n). O
The O
time O
cost O
is O
T(n) O
= O
3kn+k, O
where O
kis O
the O
number O
of O
iterations. O
4 O
Experiments O
4.1 O
Data O
Sets O
Used O
The O
statistics O
of O
the O
used O
corpora O
are O
provided O
in O
Table O
2. O
Yelp O
The O
Yelp B-DatasetName
data O
set O
consists O
of O
reviews O
from O
Yelp O
users O
and O
is O
provided O
by O
the O
Yelp B-DatasetName
Dataset O
Challenge. O
Each O
sample O
is O
a O
sentence O
labelled O
as O
having O
either O
positive O
or O
negative O
sentiment. O
Amazon O
Similar O
to O
Yelp, O
the O
Amazon B-DatasetName
data O
set O
(He O
and O
McAuley, O
2016) O
consists O
of O
labelled O
re- O
views O
from O
Amazon O
users. O
We O
used O
the O
latest O
ver- O
sion O
provided O
by O
(Li O
et O
al., O
2018). O
IMDb O
The O
IMDb B-DatasetName
Movie I-DatasetName
Review I-DatasetName
(referred O
to O
as O
IMDb) B-DatasetName
contains O
positive O
and O
negative O
reviews O
of O
movies. O
We O
used O
the O
latest O
version O
provided O
by O
Dai O
et O
al. O
(2019), O
which O
was O
created O
based O
on O
previous O
work O
(Maas O
et O
al., O
2011). O
GYAFC B-DatasetName
Grammarly’s I-DatasetName
Yahoo I-DatasetName
Answers I-DatasetName
Formality I-DatasetName
Corpus O
(GYAFC) O
(Rao O
and O
Tetreault, O
2018) O
is O
a O
parallel O
corpus O
of O
informal O
and O
formal O
sentences. O
To O
achieve O
unsupervised O
learning, O
we O
shuffled O
all O
of O
the O
used O
sentences O
in O
training.297ModelAmazon O
“ACC.” B-MetricName
stands O
for O
Accuracy, O
“s-BLEU” B-MetricName
stands O
for O
self-BLEU O
and O
“r-BLEU” B-MetricName
stands O
for O
ref-BLEU. O
We O
report O
the O
results O
of O
baselines O
by O
following O
their O
official O
codes O
and O
outputs. O
4.2 O
Baselines O
We O
selected O
six O
style B-TaskName
transfer I-TaskName
models O
for O
sentiment O
transfer O
comparison O
and O
two O
additional O
models O
for O
formality O
transfer O
comparison. O
These O
baseline O
models O
can O
be O
broadly O
divided O
into O
two O
categories. O
Models O
in O
the O
first O
category O
transfer O
sentences O
in O
a O
latent O
space O
and O
include O
the O
cross-align O
model O
(Shen O
et O
al., O
2017), O
the O
style-transformer O
model O
(Dai O
et O
al., O
2019), O
the O
DualRL O
model O
(Luo O
et O
al., O
2019), O
the O
DIRR O
model O
(Liu O
et O
al., O
2021) O
and O
the O
DGST B-MethodName
model O
(Li O
et O
al., O
2020). O
Models O
in O
the O
sec- O
ond O
category O
are O
based O
on O
the O
substitution O
of O
words O
and O
include O
the O
DRG O
model O
(Li O
et O
al., O
2018), O
the O
TAG B-MethodName
model O
(Madaan O
et O
al., O
2020) O
and O
the O
LEWIS B-MethodName
model O
(Reid O
and O
Zhong, O
2021). O
4.3 O
Automated O
Evaluation O
Metric O
Transfer O
accuracy O
and O
content O
preservation O
are O
cur- O
rently O
the O
most O
important O
aspects O
in O
evaluating O
style O
transfer O
models O
(Huang O
et O
al., O
2021; O
Fei O
et O
al., O
2021). O
Following O
standard O
practise, O
we O
considered O
the O
following O
metrics. O
Transfer B-MetricName
Accuracy I-MetricName
Accuracy O
is O
an O
important O
evaluation O
metric O
(Cao O
et O
al., O
2020; O
Zhou O
et O
al., O
2020) O
and O
represents O
the O
rate O
of O
successful O
trans- O
fer. O
We O
trained O
an O
attention-based B-MethodName
convolutional I-MethodName
neural I-MethodName
network I-MethodName
as O
the O
evaluation O
classifier O
fωto O
calculate O
the O
accuracy. O
For O
each O
corpus, O
this O
clas- O
sifier O
is O
trained O
on O
the O
corresponding O
train O
set O
to O
distinguish O
sentences O
with O
two O
different O
styles. O
The O
accuracy O
is O
the O
probability O
that O
the O
generated O
sen- O
tences O
ˆXAare O
judged O
to O
possess O
the O
target O
style O
B. O
The O
computation O
of O
accuracy O
is O
as O
follows: O
Accuracy O
=P(B|ˆXA;ω) O
(12) O
It O
should O
be O
noted O
that O
to O
avoid O
information O
leak- O
age, O
the O
evaluation O
classifier O
is O
completely O
different O
from O
the O
one O
used O
in O
the O
training O
period O
(i.e. O
fϕ).Content O
Preservation O
The O
Bilingual O
Evaluation O
Understudy O
(BLEU) O
score O
(Papineni O
et O
al., O
2002) O
measures O
the O
similarity O
between O
two O
sentences O
at O
the O
lexical O
level. O
In O
recent O
studies O
(Lample O
et O
al., O
2019; O
Sudhakar O
et O
al., O
2019), O
two O
BLEU B-MetricName
scores O
were O
computed: O
self-BLEU, B-MetricName
which O
is O
the O
BLEU O
score O
between O
the O
input O
and O
output, O
and O
ref-BLEU, B-MetricName
which O
is O
the O
BLEU O
score O
between O
the O
output O
and O
human O
reference O
sentences. O
We O
used O
the O
Natural O
Language O
Toolkit O
(NLTK) O
(Bird O
et O
al., O
2009) O
to O
calculate O
these O
sentence O
BLEU B-MetricName
scores. O
4.4 O
Architecture O
Details O
We O
pre-processed O
the O
input O
data O
into O
mini-batches O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64. B-HyperparameterValue
The O
MLP O
used O
had O
four B-HyperparameterValue
layers B-HyperparameterName
with O
768 B-HyperparameterValue
neurons O
per O
layer. O
The O
activation O
function O
used O
was O
the O
hyperbolic O
tangent O
function. O
We O
added O
a O
linear O
layer O
with O
768 O
neurons O
after O
a O
BERT B-MethodName
to O
fine-tune O
it. O
For O
training, O
the O
Adam B-HyperparameterValue
algorithm B-HyperparameterName
(Kingma O
and O
Ba, O
2015) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.0001 B-HyperparameterValue
was O
employed O
to O
update O
the O
models. O
All O
loss O
functions O
were O
based O
on O
cross-entropy. O
5 O
Results O
and O
Discussion O
5.1 O
Analysis O
Table O
3 O
presents O
the O
results O
of O
sentiment O
transfer O
on O
the O
three O
used O
data O
sets. O
On O
the O
Amazon O
data O
set, O
our O
model O
had O
an O
accuracy B-MetricName
of O
74.3%, B-MetricValue
a O
self-BLEU B-MetricName
score O
of O
65.30 B-MetricValue
and O
a O
ref-BLEU B-MetricName
score O
of O
30.14. B-MetricValue
In O
terms O
of O
accuracy, O
our O
model O
surpassed O
the O
LEWIS O
model, O
which O
had O
similar O
content O
retention O
to O
that O
of O
our O
model. O
The O
accuracy B-MetricName
of O
our O
model O
was O
lower O
than O
that O
of O
the O
TAG B-MethodName
model O
by O
5%; B-MetricValue
however, O
the O
self-BLEU B-MetricName
and O
ref-BLEU B-MetricName
scores O
of O
our O
model O
were O
higher O
by O
7 B-MetricValue
and O
4 B-MetricValue
points, O
respectively. O
The O
DGST B-MethodName
and O
StyleTrans B-MethodName
models O
had O
higher O
BLEU B-MetricName
scores O
than O
the O
scores O
of O
our O
model; O
however, O
examining O
the O
output O
sentences O
revealed O
that O
many O
were O
sim- O
ply O
copied O
from O
the O
input O
to O
the O
output, O
which O
was298not O
considered O
a O
successful O
transformation. O
On O
the O
Yelp O
data O
set, O
our O
model O
achieved O
state-of- O
the-art O
performance O
in O
all O
metrics. O
Even O
the O
greedy O
search O
version O
of O
our O
model O
with O
linear O
time O
com- O
plexity O
outperformed O
the O
baselines. O
The O
accuracy B-MetricName
and O
BLEU B-MetricName
score O
of O
our O
model O
were O
approximately O
1% B-MetricValue
and O
two B-MetricValue
points I-MetricValue
higher, O
respectively O
than O
those O
of O
the O
StyleTrans O
and O
DIRR O
models. O
On O
the O
IMDb O
data O
set, O
our O
model O
achieved O
a O
high O
accuracy B-MetricName
of O
91.1%. B-MetricValue
In O
the O
absence O
of O
reference, O
only O
the O
results O
of O
the O
self-BLEU O
measurement O
are O
provided. O
Further, O
because O
sentences O
in O
the O
IMDb O
dataset O
are O
relatively O
long, O
a O
low O
self-BLEU O
score O
may O
not O
directly O
reflect O
semantic O
content O
retention. O
Because O
the O
GYAFC O
data O
set O
pertains O
to O
formal- O
ity O
transfer, O
it O
is O
listed O
in O
Table O
4. O
The O
accuracy O
and O
self-BLEU B-MetricName
score O
of O
our O
model O
were O
approximately O
7% B-MetricValue
and O
10 B-MetricValue
points I-MetricValue
higher, O
respectively, O
than O
those O
of O
the O
baselines. O
In O
terms O
of O
the O
ref-BLEU B-MetricName
score, O
our O
proposed O
model O
and O
the O
StyleTrans O
model O
had O
comparable O
results O
(within O
1% O
error). O
Therefore, O
we O
can O
conclude O
that O
our O
model O
had O
the O
highest O
overall O
performance O
among O
all O
compared O
models. O
Data O
set O
GYAFC O
ACC. O
self-BLEU O
ref-BLEU O
The O
confidence O
level O
of O
BLEU O
is O
0.95. O
5.2 O
Manual O
Evaluation O
To O
further O
evaluate O
the O
performance O
of O
our O
model, O
we O
randomly O
sampled O
outputs B-HyperparameterName
from O
of O
the O
most O
well-performed O
models O
(i.e., O
the O
TAG B-MethodName
model O
and O
the O
LEWIS O
model) O
to O
perform O
a O
human O
evaluation O
on O
the O
Amazon O
and O
Yelp O
data O
set O
(the O
two O
most O
commonly O
used O
data O
sets). O
Seven O
individuals O
participated O
in O
the O
evaluation. O
By O
following O
(Dai O
et O
al., O
2019), O
for O
each O
review, O
we O
displayed O
one O
input O
sentence O
and O
three O
trans- O
ferred O
samples O
to O
a O
reviewer. O
The O
reviewers O
were O
instructed O
to O
separately O
select O
the O
best O
sentence O
in O
terms O
of O
three O
aspects: O
the O
target O
style, O
content O
preservation O
and O
fluency. O
We O
also O
offered O
the O
op- O
tion O
’No O
preference’ O
to O
allow O
for O
objectivity.ModelAmazon O
Following O
standard O
practice O
(Dai O
et O
al., O
2019; O
Madaan O
et O
al., O
2020), O
we O
randomly O
selected O
100 O
sentences O
for O
evaluation. O
As O
illustrated O
in O
Table O
5, O
our O
proposed O
model O
comprehensively O
outperformed O
the O
baselines O
on O
the O
Yelp O
dataset. B-HyperparameterName
On O
the O
Amazon O
dataset, B-HyperparameterName
our O
method O
achieved O
the O
highest O
style O
transfer O
rate; O
however, O
the O
proposed O
model O
had O
slightly O
poorer O
performance O
than O
the O
LEWIS O
model O
in O
terms O
of O
content O
preservation O
and O
fluency. O
5.3 O
Additional O
Analysis O
Current O
studies O
focus O
on O
how O
to O
carefully O
design O
loss O
functions O
to O
train O
a O
generator O
for O
style O
trans- O
formation O
(Luo O
et O
al., O
2019; O
Lee, O
2020). O
However, O
they O
neglect O
to O
analyse O
the O
sentence O
features O
before O
and O
after O
the O
transformation. O
Therefore, O
we O
analyse O
the O
following O
questions: B-HyperparameterName
1.What O
is O
the O
difference O
between O
transforma- O
tions O
in O
two O
opposite O
directions? O
2. O
Do O
the O
models O
retain O
semantic O
information? B-TaskName
For O
the O
first O
question, B-HyperparameterName
we O
counted O
the O
number O
of O
edit O
operations O
used O
by O
our O
model. O
We O
calculated O
these O
numbers O
as O
percentages O
to O
visually O
compare O
the O
differences O
for O
different O
transfer O
directions. O
The O
results O
are O
presented O
in O
Figure O
3. O
For O
sentiment O
transformation, O
we O
detected O
greater O
use O
of O
the O
‘[DEL]’ O
operation O
in O
transfor- O
mations O
from O
negative-to-positive O
sentiment. O
We O
supposed O
that O
this O
was O
due O
to O
the O
presence O
of O
more O
negations O
in O
the O
negative O
sentences. O
By O
directly O
deleting O
negations, O
sentences O
can O
become O
posi- O
tive. O
In O
contrast, O
positive-to-negative O
transitions O
rely O
more O
on O
the O
use O
of O
‘[SUB]’ O
operations. O
This O
signifies O
that O
replacing O
positive O
adjectives O
with O
neg- O
ative O
adjectives O
is O
closer O
to O
natural O
human O
expres- O
sion O
than O
inserting O
negations. O
We O
note O
that O
the O
proportion O
of O
deletions O
was O
always O
greater O
than O
the O
proportion O
of O
insertions. O
According O
to O
the O
scoring O
rules O
of O
the O
statistical O
LM, O
shorter O
sentences O
had O
a O
higher O
probability O
of O
appearing O
in O
the O
target O
corpus. O
Thus, O
shorter O
sen- O
tences O
were O
more O
likely O
to O
score O
higher O
than O
longer299Figure O
3: O
Percentage O
of O
the O
used O
three O
edit O
operations. O
The O
results O
are O
based O
on O
models O
with O
Viterbi O
searching. O
In O
other O
words, O
we O
suppose O
that O
shorter O
sentences O
were O
more O
likely O
to O
be O
judged O
as O
fluent O
than O
longer O
sentences. O
For O
the O
second O
question, O
we O
performed O
anal- O
ysis O
on O
the O
data O
sets O
that O
had O
human O
references O
(i.e. O
Amazon O
and O
Yelp O
data O
sets). O
We O
calculated O
Sentence-BERT B-MetricName
(SBERT) I-MetricName
scores I-MetricName
(Reimers O
and O
Gurevych, O
2019) O
and O
BERTScores B-MetricName
(Zhang O
et O
al., O
2020) O
to O
reflect O
the O
semantic O
content O
preservation. O
The O
results O
are O
presented O
in O
Table O
6 O
and O
Table O
7. O
We O
selected O
the O
two O
best O
performing O
models O
(i.e. O
TAG B-MethodName
and O
LEWIS B-MethodName
models) O
for O
comparison. O
The O
results O
demonstrate O
that O
our O
models O
outper- O
formed O
the O
baselines O
in O
terms O
of O
semantic O
similar- O
ity O
to O
human O
references. O
On O
the O
Amazon O
dataset, O
our O
model O
improved O
the O
SBERT B-MetricName
score I-MetricName
by O
ap- O
proximately O
four B-MetricValue
points I-MetricValue
while O
obtaining O
similarBERTScores B-MetricName
with O
the O
LEWIS B-MethodName
model. I-MethodName
For O
the O
Yelp O
data O
set, O
our O
model O
improved O
the O
SBERT B-MetricName
score I-MetricName
by O
approximately O
one O
point O
and O
improved O
the O
BERTScore O
obtaining O
similar O
BERTScores B-MetricName
with O
the O
LEWIS B-MethodName
model. O
6 O
Case O
Study O
To O
further O
demonstrate O
the O
superiority O
of O
our O
model, O
Werandomly O
sampled O
some O
positive O
and O
negative O
sentences O
from O
the O
outputs O
of O
our O
model O
and O
base- O
lines O
for O
comparison, O
as O
shown O
in O
Table O
8. O
For O
the O
human O
reference O
outputs, O
although O
the O
hired O
workers O
were O
not O
asked O
to O
make O
minimal O
changes O
to O
change O
the O
sentiment O
of O
input O
sentences, O
we O
noticed O
that O
overlaps O
are O
commonly O
between O
inputs O
and O
human O
references. O
In O
other O
words, O
peo- O
ple O
naturally O
tend O
to O
retain O
content O
words O
from O
an O
input O
sentence O
when O
rewriting O
it. O
An O
interesting O
thing O
is O
that, O
for O
the O
Amazon B-DatasetName
data O
set, O
comments O
with O
1 O
or O
2 O
stars O
are O
considered O
to O
be O
negative O
and O
comments O
with O
4 O
or O
5 O
stars O
are O
considered O
to O
be O
positive. O
However, O
looking O
at O
the O
data, O
not O
all O
low O
scoring O
reviews O
contain O
only O
nega- O
tive O
sentiment, O
while O
not O
all O
high O
scoring O
reviews O
contain O
only O
positive O
sentiment. O
Furthermore, O
the O
human O
reference O
of O
the O
Amazon B-DatasetName
data O
set O
is O
not O
al- O
ways O
effective. O
For O
example, O
a O
negative O
reference O
sentence O
“ O
because O
it O
might O
not O
be O
worth O
full O
price O
.” O
is O
labelled O
as O
positive. O
Cases O
of O
mislabeling O
may O
be O
the O
reason O
why O
the O
models O
did O
not O
perform O
well O
on O
the O
Amazon B-DatasetName
data O
set. O
Comparing O
the O
two O
different O
search O
strategies,300Yelp O
Positive O
to O
negative O
Negative O
to O
positive O
Input O
it O
is O
a O
cool O
place O
, O
with O
lots O
to O
see O
and O
try O
. O
unfortunately O
, O
it O
is O
the O
worst O
. O
Human O
nothing O
to O
see O
there O
, O
not O
a O
nice O
place O
. O
fortunately O
, O
it O
is O
the O
best O
. O
TAG O
it O
is O
a O
shame O
, O
not O
to O
see O
and O
try O
. O
great O
food O
, O
great O
service O
and O
the O
staff O
is O
friendly O
. O
DGST O
it O
is O
a O
sad O
place O
, O
with O
lots O
to O
see O
and O
try O
. O
overall O
, O
it O
is O
the O
best O
. O
DIRR O
it O
is O
a O
cold O
place O
, O
with O
no O
to O
see O
and O
try O
. O
fortunately O
, O
it O
is O
the O
best O
. O
LEWIS O
it O
is O
a O
very O
busy O
place O
, O
with O
lots O
to O
see O
and O
try O
. O
cajun O
food O
, O
it O
is O
the O
best O
! O
Ours O
+ O
GS O
it O
is O
a O
place O
, O
with O
nothing O
to O
see O
and O
try O
. O
seriously O
, O
it O
is O
the O
best O
. O
Ours O
+ O
VS O
it O
is O
a O
mess O
, O
with O
nothing O
to O
see O
and O
try O
. O
seriously O
, O
it O
is O
the O
best O
. O
Amazon O
Positive O
to O
negative O
Negative O
to O
positive O
Input O
for O
my O
purpose O
this O
is O
the O
perfect O
item O
. O
because O
it O
is O
definitely O
not O
worth O
full O
price O
. O
Human O
for O
my O
purpose O
this O
is O
the O
worst O
item. O
because O
it O
might O
not O
be O
worth O
full O
price O
. O
TAG O
for O
my O
purpose O
this O
is O
the O
worst O
item O
. O
because O
it O
is O
definitely O
not O
worth O
full O
price O
. O
DGST O
for O
my O
purpose O
this O
is O
the O
perfect O
item O
. O
because O
it O
is O
definitely O
not O
worth O
full O
price O
. O
DIRR O
for O
my O
purpose O
this O
is O
the O
same O
thing O
. O
because O
it O
is O
definitely O
worth O
full O
price O
. O
LEWIS O
for O
my O
purpose O
this O
is O
the O
best O
game O
ever O
made O
. O
because O
it O
is O
definitely O
well O
made O
and O
worth O
full O
price O
. O
Ours O
+ O
GS O
for O
my O
purpose O
this O
is O
the O
item O
. O
because O
it O
is O
definitely O
well O
worth O
full O
price O
. O
Ours O
+ O
VS O
for O
my O
purpose O
this O
is O
the O
worst O
item O
. O
because O
it O
is O
definitely O
well O
worth O
full O
price O
. O
IMDb O
Positive O
to O
negative O
Negative O
to O
positive O
Input O
i O
rate O
this O
movie O
8/10 O
. O
please O
, O
do O
n’t O
see O
this O
movie O
. O
StyTrans O
i O
rate O
this O
movie O
4/10 O
. O
please O
, O
do O
also O
see O
this O
movie O
. O
DGST O
i O
rate O
this O
movie O
1/10 O
u O
, O
do O
n’t O
see O
this O
” O
DIRR O
i O
rate O
this O
movie O
1/10 O
. O
please O
, O
see O
this O
movie O
. O
Ours O
+ O
GS O
i O
rate O
this O
movie O
1/10 O
. O
please O
, O
do O
n O
’ O
t O
miss O
this O
movie O
today O
. O
Ours O
+ O
VS O
i O
rate O
this O
movie O
1/10 O
. O
please O
, O
do O
n O
’ O
t O
miss O
this O
movie O
. O
Table O
8: O
Sentences O
sampled O
from O
sentiment O
transfer O
data O
set. O
‘Human’ O
denotes O
manual O
reference. O
‘GS’ B-HyperparameterValue
denotes O
‘Greedy B-HyperparameterValue
Search’ I-HyperparameterValue
and O
‘VS’ B-HyperparameterValue
denotes O
‘Viterbi B-HyperparameterValue
Search’. I-HyperparameterValue
Red O
text O
stands O
for O
failed O
style O
transformation, O
brown O
text O
stands O
for O
poor O
content O
preservation O
and O
blue O
text O
stands O
for O
suitable O
transformation. O
our O
model O
using O
the O
Viterbi O
search O
generate O
more O
fluent O
sentences O
than O
our O
model O
using O
the O
greedy O
search. O
However, O
the O
model O
using O
Viterbi O
search O
has O
a O
time O
complexity O
of O
O(n2)and O
the O
number O
of O
states O
linearly O
increased O
with O
the O
number O
of O
it- O
erative O
steps. O
Further, O
we O
find O
that O
models O
using O
different O
search O
strategies O
have O
the O
same O
output O
in O
approximately O
half O
of O
the O
cases. O
For O
the O
method O
based O
on O
transformation O
in O
latent O
space O
(i.e., O
DGST), B-MethodName
it O
always O
copies O
sentences O
with- O
out O
transferring O
them O
into O
correct O
style O
domains. O
For O
this O
same O
reason, O
the O
DGST B-MethodName
model O
obtained O
high O
BLEU B-MetricName
values O
on O
all O
of O
the O
used O
data O
sets. O
For O
the O
method O
based O
on O
the O
modification O
of O
words O
(i.e., O
TAG B-MethodName
and O
LEWIS), B-MethodName
they O
will O
retain O
the O
majority O
of O
input O
words. O
However, O
recognition O
of O
style-indicative O
words O
may O
result O
that O
part O
of O
style- O
indicative O
words O
are O
retained O
and O
content O
words O
are O
deleted, O
that O
is, O
examples O
listed O
in O
Table O
8. O
7 O
Conclusion O
In O
this O
study, O
we O
proposed O
a O
probabilistic O
model O
for O
sentiment O
and O
style O
transfer O
on O
non-parallel O
data. O
We O
used O
a O
classifier B-MethodName
and O
an O
LM B-MethodName
to O
construct O
a O
CRF. B-MethodName
Using O
dynamic O
programming O
search O
algorithms,we O
generated O
a O
tag O
sequence O
to O
modify O
the O
input O
sentences. O
The O
experimental O
results O
revealed O
that O
our O
proposed O
model O
outperformed O
the O
baselines O
in O
terms O
of O
accuracy O
by O
approximately O
2%. O
Our O
future O
work O
will O
focus O
on O
the O
simplification O
of O
the O
search O
process. O
By O
using O
the O
policy O
gradient O
(Williams, O
1992) O
of O
reinforcement O
learning, O
we O
might O
be O
able O
to O
speed O
up O
the O
transfer O
model. O