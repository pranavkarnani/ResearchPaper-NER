Proceedings O
of O
the O
Second O
Workshop O
on O
Language O
Technology O
for O
Equality, O
Diversity O
and O
Inclusion O
, O
pages O
270 O
- O
275 O
May O
27, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
Sammaan@LT-EDI-ACL2022: O
Ensembled O
Transformers O
Against O
Homophobia O
and O
Transphobia O
Ishan O
Sanjeev O
Upadhyay O
and O
KV O
Aditya O
Srivatsa O
and O
Radhika O
Mamidi O
International O
Institute O
of O
Information O
Technology, O
Hyderabad O
{ishan.sanjeev, O
k.v.aditya}@research.iiit.ac.in O
radhika.mamidi@iiit.ac.in O
Abstract O
Hateful O
and O
offensive O
content O
on O
social O
media O
platforms O
can O
have O
negative O
effects O
on O
users O
and O
can O
make O
online O
communities O
more O
hostile O
towards O
certain O
people O
and O
hamper O
equality, O
di- O
versity O
and O
inclusion. O
In O
this O
paper, O
we O
describe O
our O
approach O
to O
classify B-TaskName
homophobia I-TaskName
and I-TaskName
trans- I-TaskName
phobia I-TaskName
in I-TaskName
social I-TaskName
media I-TaskName
comments. I-TaskName
We O
used O
an O
ensemble O
of O
transformer O
based O
models O
to O
build O
our O
classifier. O
Our O
model O
ranked O
2nd O
for O
En- O
glish, O
8th O
for O
Tamil O
and O
10th O
for O
Tamil-English. O
1 O
Introduction O
Social O
media O
platforms O
allow O
people O
from O
all O
walks O
of O
life O
to O
connect O
with O
each O
other. O
However, O
abu- O
sive O
and O
hateful O
content O
on O
these O
platforms O
can O
take O
a O
psychological O
toll O
on O
its O
users O
(Wypych O
and O
Bilewicz, O
2022) O
(Tynes O
et O
al., O
2008). O
Lesbian, O
gay, O
bisexual O
and O
transgender O
individuals O
are O
more O
vulnerable O
to O
mental O
illness O
as O
compared O
to O
their O
heterosexual O
peers O
(Gilman O
et O
al., O
2001) O
(Marshal O
et O
al., O
2011) O
(Reisner O
et O
al., O
2015). O
Hence, O
it O
be- O
comes O
even O
more O
important O
to O
be O
able O
to O
detect O
such O
hateful O
content O
for O
vulnerable O
individuals. O
There O
has O
been O
a O
lot O
of O
work O
done O
in O
the O
domain O
of O
hate B-TaskName
speech I-TaskName
detection I-TaskName
(Malmasi O
and O
Zampieri, O
2017) O
(Burnap O
and O
Williams, O
2016). O
There O
has O
also O
been O
work O
on O
hate B-TaskName
speech I-TaskName
intervention I-TaskName
(Qian O
et O
al., O
2019). O
Shared O
tasks O
like O
SemEval O
2019 O
Task O
6 O
have O
focused O
on O
identifying O
and O
categorizing O
offensive O
language O
on O
social O
media O
(Zampieri O
et O
al., O
2019). O
Datasets O
for O
this O
task O
have O
been O
created O
in O
multi- O
ple O
languages O
as O
well. O
Bohra O
et O
al. O
(2018) O
created O
a O
Hindi-English O
code O
mixed O
text O
dataset O
for O
hate O
speech O
detection O
from O
tweets O
on O
Twitter. B-DatasetName
Mubarak O
et O
al. O
(2021) O
created O
a O
1000 B-DatasetName
tweets I-DatasetName
Arabic I-DatasetName
dataset I-DatasetName
for O
offensive O
language O
detection O
with O
special O
tags O
for O
vulgarity O
and O
hate O
speech. O
Sigurbergsson O
and O
Derczynski O
(2020) O
created O
a O
Danish B-DatasetName
hate I-DatasetName
speech I-DatasetName
detection I-DatasetName
dataset I-DatasetName
containing O
3600 O
user O
generated O
comments O
social O
media O
websites. O
There O
have O
been O
datasets O
created O
for O
Greek O
(Pitenis O
et O
al., O
2020) O
andTurkish O
(Çöltekin, O
2020) O
as O
well. O
Chakravarthi O
et O
al. O
(2021a) O
created O
a O
code-mixed O
Tamil,Malayalam O
and O
Kannada O
dataset O
for O
offensive O
language O
identi- O
fication. O
Support O
vector O
machines, O
long O
short-term O
memory O
networks, O
convolutional O
neural O
networks O
and O
now O
transformer O
based O
architectures O
have O
been O
used O
to O
detect B-TaskName
hate I-TaskName
speech. I-TaskName
However, O
there O
has O
not O
been O
much O
work O
in O
trying O
to O
specifically O
identify B-TaskName
homophobic I-TaskName
or I-TaskName
transphobic I-TaskName
text. I-TaskName
In O
this O
paper, O
we O
will O
describe O
our O
approach O
for O
classifying B-TaskName
trans- I-TaskName
phobic I-TaskName
and I-TaskName
homophobic I-TaskName
comments I-TaskName
in O
the O
dataset O
provided O
by O
Chakravarthi O
et O
al. O
(2021b) O
as O
a O
part O
of O
the O
shared O
task O
on O
homophobia B-TaskName
and I-TaskName
transphobia I-TaskName
detection I-TaskName
in I-TaskName
social I-TaskName
media I-TaskName
comments I-TaskName
Chakravarthi O
et O
al. O
(2022). O
2 O
Dataset O
Description O
The O
dataset O
consists O
of O
a O
total O
of O
15,141 O
comments O
in O
3 O
languages: O
English, O
Tamil O
and O
Tamil-English O
code-mixed O
(refer O
to O
Table O
1 O
for O
data O
distribution). O
Each O
comment O
has O
one O
of O
three O
labels O
"Homopho- O
bic", O
"Transphobic" O
and O
"Non-anti-LGBT+ O
con- O
tent" O
(label O
distribution O
in O
Table O
2). O
3 O
Methodology O
In O
this O
section O
we O
will O
describe O
the O
models O
used O
in O
our O
experimentation. O
•BERT B-MethodName
: O
BERT B-MethodName
(Devlin O
et O
al., O
2019) O
is O
a O
Transformer-based O
language O
model. O
It O
con- O
sists O
of O
layered O
encoder O
units, O
each O
with O
a O
self- O
attention O
layer O
followed O
by O
fully-connected O
layers. O
It O
is O
trained O
using O
the O
Masked O
Lan- O
guage O
Modelling O
(MLM) O
task O
as O
well O
as O
the O
Next O
Sentence O
Prediction O
(NSP) O
task. O
For O
this O
shared O
task, O
we O
have O
used O
the O
pretrained O
bert- O
base-uncased O
model O
from O
HuggingFace O
(Wolf O
et O
al., O
2019). O
•RoBERTa B-MethodName
: O
RoBERTa B-MethodName
(Liu O
et O
al., O
2019) O
is O
a O
Transformer-based O
language O
model O
which O
improves O
upon O
the O
BERT B-MethodName
architecture O
along O
several O
metrics O
offered O
by O
the O
GLUE B-MetricName
bench- O
mark O
(Wang O
et O
al., O
2019). O
It O
is O
not O
trained O
on O
the O
NSP O
task O
and O
involves O
dynamic O
mask- O
ing O
for O
the O
MLM O
task. O
It O
is O
also O
trained O
over O
a O
much O
larger O
dataset O
with O
longer O
sentence O
lengths. O
For O
this O
shared O
task, O
we O
have O
used O
the O
pretrained O
roberta-base B-MethodName
model. O
•HateBERT B-MethodName
(Caselli O
et O
al., O
2021) O
is O
a O
re- O
trained O
BERT B-MethodName
model O
to O
detect O
abusive O
lan- O
guage O
in O
English. O
It O
is O
trained O
on O
large O
amounts O
of O
banned O
Reddit O
comments O
ex- O
tracted O
from O
the O
RAL-E B-DatasetName
dataset. O
It O
has O
been O
shown O
to O
outperform O
the O
BERT B-MethodName
model O
in O
sev- O
eral O
hate-speech B-TaskName
detection I-TaskName
tasks. O
•IndicBERT B-MethodName
: O
IndicBERT B-MethodName
(Kakwani O
et O
al., O
2020) O
is O
an O
ALBERT B-MethodName
Transformer O
encoder O
(Lan O
et O
al., O
2020) O
finetuned O
on O
data O
from O
12 O
major O
Indian O
languages, O
including O
549M O
to- O
kens O
of O
Tamil. O
Despite O
having O
significantly O
lower O
parameters O
than O
other O
multilingual O
en- O
coders O
such O
as O
mBERT O
(Devlin O
et O
al., O
2019) O
or O
XLM-R O
(Conneau O
et O
al., O
2020), O
it O
outperforms O
them O
on O
several O
metrics O
of O
the O
IndicGLUE O
benchmark O
(Kakwani O
et O
al., O
2020). O
We O
have O
used O
the O
IndicBERT O
model O
as O
a O
TLM O
for O
the O
Tamil O
and O
Tamil-English O
tracks. O
•XGBoost B-MethodName
Random B-MethodName
Forest I-MethodName
Classifier I-MethodName
: O
Ran- B-MethodName
dom I-MethodName
Forest I-MethodName
Classifiers I-MethodName
(Ho, O
1995) O
are O
meta O
estimators O
which O
consist O
of O
numerous O
deci- O
sion O
trees, O
each O
fit O
upon O
a O
subset O
of O
features O
from O
a O
subset O
of O
rows O
of O
the O
data. O
The O
en- O
semble O
of O
many O
such O
weak O
learners O
tends O
to O
outperform O
a O
single O
large O
decision O
tree. O
Thelow O
correlation O
between O
the O
constituent O
trees O
also O
provides O
for O
more O
feature O
coverage O
and O
curbs O
over-fitting. O
For O
this O
shared O
task, O
we O
use O
XGBoost’s B-MethodName
implementation O
of O
Random B-MethodName
Forest I-MethodName
Classifiers I-MethodName
(Chen O
and O
Guestrin, O
2016). O
•Bayesian O
Optimization O
: O
The O
aim O
of O
any O
hy- O
perparameter O
optimization O
strategy O
is O
to O
find O
the O
hyperparameter O
set O
which O
fetches O
the O
best O
value O
over O
the O
object O
function. O
Bayesian O
Opti- O
mization O
(Mockus, O
1989) O
is O
an O
iterative O
opti- O
mization O
algorithm O
that O
aims O
to O
minimize O
the O
number O
of O
hyperparameter O
sets O
that O
must O
be O
evaluated O
before O
arriving O
at O
the O
optimal O
distri- O
bution. O
It O
has O
been O
shown O
to O
generate O
optimal O
solutions O
in O
significantly O
fewer O
iterations O
than O
traditional O
methods O
such O
as O
grid O
search. O
For O
this O
task, O
we O
have O
used O
the O
Python O
library: O
bayesian-optimization O
(Fernando, O
2014). O
4 O
Experiments O
and O
Results O
The O
only O
pre-procesesing O
step O
done O
on O
the O
dataset O
before O
training O
was O
the O
change O
of O
emojis O
to O
text O
using O
the O
demoji O
library O
in O
python1. O
Our O
pipeline O
comprises O
an O
ensemble O
of O
several O
Transformer- O
based O
language O
models O
(TLM), O
namely: O
BERT, B-MethodName
RoBERTa, B-MethodName
and O
HateBERT B-MethodName
for O
the O
English O
track O
and O
IndicBERT B-MethodName
for O
the O
Tamil O
and O
Tamil-English O
tracks. O
Three O
copies O
of O
each O
TLM O
are O
used O
with O
dif- O
ferent O
parameter O
initializations O
in O
each O
track. O
This O
allows O
for O
the O
copies O
to O
capture O
different O
features O
of O
the O
data. O
In O
addition O
to O
this, O
for O
each O
track, O
a O
layer O
of O
attention O
is O
applied O
to O
each O
constituent O
encoder O
layer O
outputs O
of O
the O
TLMs. O
This O
is O
necessary O
since O
each O
layer O
captures O
a O
different O
kind O
of O
informa- O
tion, O
which O
are O
variably O
relevant O
for O
our O
task. O
The O
weighted O
and O
combined O
output O
from O
the O
attention O
layer O
is O
then O
passed O
through O
a O
final O
linear O
layer O
and O
dropout B-HyperparameterName
layer O
( O
p= O
0.3), B-HyperparameterValue
followed O
by O
a O
Softmax O
operation O
to O
generate O
the O
predicted O
probabilities O
of O
detecting B-TaskName
homophobic I-TaskName
content I-TaskName
in O
the O
given O
input O
text. O
In O
the O
English O
track, O
we O
also O
use O
a O
pretrained O
hate-speech B-TaskName
detection I-TaskName
model O
implemented O
on O
Hug- O
gingFace O
(Wolf O
et O
al., O
2019). O
Architecturally, O
is O
a O
ByT5-Base B-MethodName
model O
(Xue O
et O
al., O
2021) O
finetuned O
on O
HuggingFace’s O
tweets_hate_speech_detection B-DatasetName
dataset O
(Sharma, O
2019). O
The O
prediction O
probabilities O
are O
generated O
by O
each O
model O
of O
a O
track O
are O
passed O
as O
input O
features O
to O
a O
Random B-MethodName
Forest I-MethodName
Classifier. I-MethodName
This O
helps O
further O
optimize O
our O
predictions O
by O
weighing O
the O
impor- O
tance O
of O
the O
different O
architectures O
for O
the O
task. O
Each O
of O
the O
TLM O
pipelines O
was O
finetuned O
upon O
Cross O
Entropy O
loss O
using O
AdamW O
optimizer O
(Loshchilov O
and O
Hutter, O
2017) O
( O
β1= B-HyperparameterName
0.9, B-HyperparameterValue
β2=0.999, B-HyperparameterName
ϵ= B-HyperparameterName
10−8) B-HyperparameterValue
with O
an O
initial O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e−5for B-HyperparameterValue
6 B-HyperparameterValue
epochs B-HyperparameterName
each O
using O
a O
linear O
scheduler. O
The O
epoch O
checkpoint O
with O
the O
highest O
validation O
F1 B-MetricName
score I-MetricName
was O
selected O
for O
further O
use. O
The O
hyper- O
parameters O
of O
the O
Random B-MethodName
Forest I-MethodName
Classifier I-MethodName
were O
estimated O
using O
10 O
seeds O
and O
100 O
iterations O
of O
Bayesian O
Optimization. O
The O
ensemble O
classifier O
was O
trained O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1.0. B-HyperparameterValue
As O
can O
be O
seen O
in O
Table O
3, O
our O
ensemble O
model O
performed O
better O
than O
the O
individually O
trained O
mod- O
els O
giving O
a O
macro B-MetricName
F1 I-MetricName
score I-MetricName
of O
0.49 B-MetricValue
which O
was O
the O
2nd O
highest O
macro B-MetricName
F1 I-MetricName
score I-MetricName
in O
the O
shared O
task. O
This O
model O
also O
had O
the O
highest O
weighted B-MetricName
F1 I-MetricName
score I-MetricName
in O
the O
task. O
The O
IndicBERT B-MethodName
ensembles O
trained O
on O
the O
Tamil O
and O
Tamil-English O
dataset O
give O
us O
a O
macro B-MetricName
F1 I-MetricName
score I-MetricName
of O
0.55 B-MetricValue
and O
0.35 B-MetricValue
and O
a O
weighted B-MetricName
F1 I-MetricName
score I-MetricName
of O
0.86 B-MetricValue
and O
0.83 B-MetricValue
respectively O
(refer O
Table O
4). O
The O
Tamil O
and O
Tamil-English O
model O
ranked O
8th O
and O
10th O
respectively. O
5 O
Conclusion O
and O
Future O
Work O
In O
this O
paper, O
we O
described O
our O
approach O
for O
ho- B-TaskName
mophobia I-TaskName
and I-TaskName
transphobia I-TaskName
detection I-TaskName
in O
English, O
Tamil O
and O
Tamil-English. O
We O
used O
an O
ensemble O
of O
three O
transformed O
based O
models O
along O
with O
a O
pre-trained O
hate B-TaskName
detection I-TaskName
model O
to O
do O
the O
classi- O
fication O
for O
English. O
Our O
model O
was O
ranked O
2nd O
for O
the O
English O
classification O
task. O
For O
the O
Tamil O
and O
Tamil-English O
dataset O
three O
copies O
of O
the O
In- B-MethodName
dicBERT I-MethodName
model O
was O
used O
to O
make O
our O
ensemble O
based O
model. O
The O
models O
placed O
8th O
and O
10th O
for O
Tamil O
and O
Tamil-English O
model O
respectively. O
In O
the O
future, O
we O
can O
use O
data O
augmentation O
methods O
like O
paraphrasing O
and O
back O
translation O
to O
increase O
the O
diversity O
and O
quantity O
of O
homopho- O
bic O
and O
transphobic O
text. O
We O
can O
also O
incorporate O
transliteration O
into O
the O
pipeline O
for O
Tamil-English O
code O
mixed O
text O
since O
IndicBERT B-MethodName
is O
not O
trained O
on O
code O
mixed O
text. O
We O
could O
also O
try O
to O
finetune O
transformers O
pre-trained O
on O
code O
mixed O
data. O