Proceedings  O
of  O
the  O
19th  O
International  O
Conference  O
on  O
Spoken  O
Language  O
Translation  O
(IWSLT  O
2022)  O
,  O
pages  O
351  O
-  O
360  O
May  O
26-27,  O
2022  O
c  O
2022  O
Association  O
for  O
Computational  O
Linguistics  O
Improving  O
Machine  B-TaskName
Translation  I-TaskName
Formality  O
Control  O
with  O
Weakly-Labelled  O
Data  O
Augmentation  O
and  O
Post  O
Editing  O
Strategies  O
Danial  O
Zhang*,  O
Jiang  O
Y  O
u*,  O
Pragati  O
V  O
erma*,  O
Ashwinkumar  O
Ganesan*  O
&  O
Sarah  O
Campbell  O
Alexa  O
AI,  O
Amazon  O
{dyz,  O
janyu,  O
vpragati,  O
gashwink,  O
srh}@amazon.com  O
Abstract  O
This  O
paper  O
describes  O
Amazon  O
Alexa  O
AI’s  O
im-  O
plementation  O
for  O
the  O
IWSLT  O
2022  O
shared  O
task  O
on  O
formality  O
control.  O
We  O
focus  O
on  O
the  O
uncon-  O
strained  O
and  O
supervised  O
task  O
for  O
en  O
→  O
hi  O
(Hindi)  O
and  O
en  O
→  O
ja  O
(Japanese)  O
pairs  O
where  O
very  O
lim-  O
ited  O
formality  O
annotated  O
data  O
is  O
available.  O
We  O
propose  O
three  O
simple  O
yet  O
effective  O
post  O
editing  O
strategies  O
namely,  O
T-V  B-MethodName
conversion,  I-MethodName
utilizing  O
a  O
verb  O
conjugator  O
and  O
seq2seq  O
models  O
in  O
order  O
to  O
rewrite  O
the  O
translated  O
phrases  O
into  O
formal  O
or  O
in-  O
formal  O
language.  O
Considering  O
nuances  O
for  O
for-  O
mality  O
and  O
informality  O
in  O
different  O
languages,  O
our  O
analysis  O
shows  O
that  O
a  O
language-specific  O
post  O
editing  O
strategy  O
achieves  O
the  O
best  O
perfor-  O
mance.  O
To  O
address  O
the  O
unique  O
challenge  O
of  O
limited  O
formality  O
annotations,  O
we  O
further  O
de-  O
velop  O
a  O
formality  O
classifier  O
to  O
perform  O
weakly-  O
labelled  O
data  O
augmentation  O
which  O
automati-  O
cally  O
generates  O
synthetic  O
formality  O
labels  O
from  O
large  O
parallel  O
corpus.  O
Empirical  O
results  O
on  O
the  O
IWSLT  B-DatasetName
formality  I-DatasetName
testset  O
have  O
shown  O
that  O
pro-  O
posed  O
system  O
achieved  O
significant  O
improve-  O
ments  O
in  O
terms  O
of  O
formality  O
accuracy  O
while  O
re-  O
taining  O
BLEU  B-MetricName
score  I-MetricName
on-par  O
with  O
baseline.  O
1  O
Introduction  O
Although  O
neural  O
machine  O
translation  O
(NMT)  O
mod-  O
els  O
have  O
achieved  O
state-of-the-art  O
results  O
with  O
high  O
BLEU  B-MetricName
scores1,  O
given  O
a  O
language  O
pair,  O
they  O
are  O
trained  O
on  O
generic  O
parallel  O
corpora  O
that  O
are  O
ex-  O
tracted  O
from  O
various  O
open  O
source  O
datasets  O
such  O
as  O
the  O
Europarl  O
corpus  O
(  O
Koehn  O
;Iranzo-Sánchez  O
et  O
al.  O
,  O
2019  O
).  O
These  O
datasets  O
make  O
an  O
implicit  O
assumption  O
that  O
there  O
is  O
a  O
single  O
translation  O
in  O
the  O
target  O
lan-  O
guage  O
to  O
a  O
sentence  O
from  O
the  O
source  O
language.  O
But  O
the  O
style  O
of  O
the  O
language  O
generated,  O
through  O
which  O
meaning  O
is  O
conveyed,  O
is  O
also  O
important  O
(  O
Heylighen  O
et  O
al.  O
,1999  O
).  O
Thus,  O
there  O
is  O
a  O
need  O
to  O
control  O
certain  O
attributes  O
of  O
the  O
text  O
generated  O
in  O
a  O
target  O
language  O
such  O
as  O
politeness  O
or  O
formality.  O
*Equal  O
contribution.  O
1http://nlpprogress.com/english/machine_translation.  O
htmlIn  O
this  O
paper,  O
we  O
present  O
our  O
system  O
for  O
the  O
IWSLT  B-DatasetName
2022  I-DatasetName
formality  I-DatasetName
control  I-DatasetName
task  O
for  O
machine  O
translation.2We  O
focus  O
on  O
the  O
unconstrained  O
and  O
su-  O
pervised  O
scenario  O
for  O
en  O
→  O
hi  O
and  O
en  O
→  O
ja  O
language  O
pairs.  O
In  O
the  O
proposed  O
system,  O
we  O
explore  O
post  O
edit-  O
ing  O
strategies  O
that  O
correct  O
or  O
alter  O
textual  O
formality  O
once  O
the  O
translation  O
has  O
been  O
completed.  O
Post  O
edit-  O
ing  O
strategies  O
can  O
be  O
language  O
specific  O
or  O
language  O
agnostic.  O
We  O
propose  O
three  O
strategies,  O
T-V  O
conver-  O
sion  O
(deterministically  O
converting  O
the  O
informal  O
or  O
T-form  O
of  O
a  O
pronoun  O
to  O
its  O
corresponding  O
formal  O
or  O
V-form)  O
,  O
verb  O
conjugation,  O
and  O
a  O
seq2seq  O
model  O
that  O
learns  O
to  O
transform  O
input  O
text  O
to  O
be  O
of  O
a  O
for-  O
mal  O
or  O
informal  O
nature.  O
The  O
T-V  O
conversion  O
and  O
verb  O
conjugation  O
are  O
language-specific  O
strategies  O
that  O
are  O
applied  O
to  O
en  O
→  O
hi,  O
and  O
en  O
→  O
ja  O
pairs  O
respec-  O
tively.  O
These  O
two  O
methods  O
are  O
compared  O
against  O
an  O
alternative  O
seq2seq  O
model  O
(  O
Enarvi  O
et  O
al.  O
,2020  O
)  O
that  O
is  O
language  O
agnostic.  O
We  O
show  O
that  O
compared  O
to  O
a  O
baseline  O
translation  O
model  O
provided  O
in  O
task,  O
a  O
finetuned  O
mBART  B-MethodName
model  O
(  O
Liu  O
et  O
al.  O
,2020  O
)  O
with  O
language-specific  O
rule-based  O
post  O
editing  O
signifi-  O
cantly  O
improved  O
the  O
baseline  O
model  O
performance  O
and  O
achieved  O
the  O
best  O
formality  O
control  O
accuracy  O
and  O
BLEU  B-MetricName
score.  O
A  O
unique  O
challenge  O
in  O
this  O
IWSLT  O
Formal-  O
ity  O
shared  O
task  O
is  O
data  O
sparsity  O
-  O
only  O
few  O
hun-  O
dred  O
formality  O
annotated  O
samples  O
are  O
available  O
for  O
finetuning  O
the  O
formality  O
controlled  O
NMT  O
model.  O
Therefore,  O
we  O
further  O
devise  O
a  O
data  O
augmentation  O
method,  O
utilizing  O
linguistic  O
cues  O
to  O
automatically  O
annotate  O
a  O
small  O
seed  O
set  O
of  O
target  O
(i.e.,  O
Hindi  O
and  O
Japanese)  O
texts  O
with  O
formality  O
labels.  O
Then  O
the  O
seed  O
set  O
is  O
utilized  O
to  O
train  O
a  O
multilingual  O
text  O
formality  O
classifier  O
that  O
can  O
further  O
mine  O
massive  O
parallel  O
corpus  O
to  O
find  O
extra  O
formality  O
annotated  O
data.  O
We  O
found  O
such  O
weakly-labeled  O
data  O
augmen-  O
tation  O
strategy  O
significantly  O
improved  O
en  O
→  O
ja  O
per-  O
formance.  O
The  O
paper  O
is  O
organized  O
into  O
the  O
following  O
sec-  O
Examples  O
of  O
T-V  O
distinction  O
in  O
Hindi.  O
tions:  O
§  O
2describes  O
each  O
method,  O
§  O
3shows  O
the  O
per-  O
formance  O
of  O
each  O
method  O
and  O
language  O
it  O
is  O
applied  O
to  O
and  O
§  O
4discusses  O
the  O
prior  O
work  O
on  O
formality.  O
2  O
System  O
Design  O
2.1  O
Task  O
Definition  O
In  O
this  O
submission,  O
we  O
focus  O
on  O
unconstrained  O
and  O
supervised  O
formality  O
control  O
machine  O
trans-  O
lation  O
task.  O
Formally,  O
given  O
a  O
source  O
segment  O
X={x1,  O
x2,  O
...,  O
x  O
m},  O
and  O
a  O
formality  O
level  O
l∈  O
{formal,  O
informal  O
},  O
the  O
goal  O
is  O
to  O
find  O
the  O
model  O
characterized  O
by  O
parameters  O
Θ  O
that  O
generates  O
the  O
most  O
likely  O
translation  O
Y={y1,  O
y2,  O
...,  O
y  O
n}corre-  O
sponding  O
to  O
the  O
formality  O
level:  O
Y=  O
arg  O
max  O
YlP(X,  O
l;Θ)  O
(1)  O
The  O
overall  O
architecture  O
and  O
workflow  O
of  O
the  O
pro-  O
posed  O
system  O
is  O
described  O
in  O
Figure  O
1.  O
We  O
present  O
the  O
design  O
of  O
each  O
component  O
below.  O
2.2  O
NMT  O
&  O
Formality  O
Finetuning  O
We  O
took  O
a  O
two-step  O
process  O
to  O
finetune  O
the  O
for-  O
mality  O
controlled  O
NMT  O
model.  O
First,  O
we  O
pretrain  B-MethodName
a  I-MethodName
generic  I-MethodName
NMT  I-MethodName
model  I-MethodName
using  O
a  O
large-scale  O
par-  O
allel  O
corpus.  O
We  O
chose  O
two  O
model  O
architectures  O
for  O
building  O
the  O
NMT  O
model  O
-  O
1)  O
the  O
provided  O
Transformer-based  B-MethodName
pretrained  I-MethodName
model  I-MethodName
implemented  O
using  O
Sockeye3,  O
and  O
2)  O
a  O
mBART  B-MethodName
model  O
imple-  O
mented  O
using  O
fairseq.4We  O
described  O
the  O
datasets  O
used  O
and  O
finetuning  O
details  O
of  O
the  O
NMT  O
models  O
in  O
§3.1  O
.  O
2.3  O
Post  O
Editing  O
We  O
explore  O
three  O
post  O
editing  O
strategies  O
that  O
rewrite  O
the  O
hypotheses  O
generated  O
for  O
the  O
for-  O
mal/informal  O
translations  O
from  O
the  O
formality  O
con-  O
trolled  O
NMT  O
models.  O
T  B-MethodName
-  I-MethodName
V  I-MethodName
Conversion  I-MethodName
Many  O
languages  O
use  O
honorifics  O
to  O
convey  O
vary-  O
ing  O
levels  O
of  O
politeness,  O
social  O
distance,  O
courtesy,  O
differences  O
in  O
age,  O
etc.  O
between  O
addressor  O
and  O
ad-  O
dressee  O
in  O
a  O
conversation.  O
Even  O
though  O
the  O
use  O
of  O
3https://github.com/awslabs/sockeye  O
4https://github.com/pytorch/fairseqhonorifics  O
is  O
not  O
the  O
only  O
way  O
to  O
convey  O
register  O
(Wardhaugh  O
,1986  O
),  O
it  O
is  O
a  O
way  O
to  O
ascertain  O
regis-  O
ter  O
in  O
sentences  O
where  O
pronouns  O
are  O
explicitly  O
men-  O
tioned.  O
The  O
T-V  O
distinction  O
(  O
Brown  O
and  O
Gilman  O
,  O
1960  O
)  O
is  O
a  O
convention  O
followed  O
by  O
many  O
languages  O
wherein  O
different  O
pronouns  O
are  O
used  O
to  O
convey  O
fa-  O
miliarity  O
or  O
formality.  O
In  O
languages  O
following  O
this  O
T-V  O
distinction,  O
it  O
is  O
applied  O
to  O
most  O
pronouns  O
of  O
address  O
,  O
along  O
with  O
their  O
verb  O
conjugations.  O
For  O
sentences  O
explicitly  O
having  O
pronouns  O
of  O
address,  O
it  O
is  O
possible  O
to  O
write  O
a  O
simple,  O
albeit  O
noisy  O
regex-  O
based  O
classifier  O
to  O
deterministically  O
recognize  O
the  O
form  O
(T-form  O
or  O
informal  O
form;  O
V-form  O
or  O
formal  O
form)  O
of  O
the  O
pronoun  O
and  O
thus  O
output  O
the  O
grammat-  O
ical  O
register  O
of  O
the  O
sentence  O
in  O
question.  O
Examples  O
of  O
such  O
T-V  O
classification  O
for  O
Hindi  O
is  O
shown  O
in  O
Ta-  O
ble  O
6.  O
For  O
post  O
editing  O
using  O
the  O
T-V  O
distinction  O
in  O
Hindi,  O
we  O
use  O
a  O
deterministic  O
map  O
of  O
pronouns  O
of  O
address  O
in  O
T-form  O
and  O
their  O
corresponding  O
V-form  O
in  O
Hindi.  O
For  O
Hindi,  O
this  O
mapping  O
is  O
almost  O
one-to-  O
one,  O
i.e.  O
the  O
map  O
can  O
be  O
flipped  O
along  O
the  O
horizon-  O
tal  O
axis  O
to  O
map  O
V-form  O
keys  O
to  O
T-form  O
values  O
with-  O
out  O
any  O
loss  O
in  O
fidelity.  O
This  O
map  O
can  O
simply  O
be  O
looked  O
up  O
in  O
the  O
correct  O
direction,  O
and  O
the  O
values  O
substituted  O
for  O
the  O
keys  O
in  O
order  O
to  O
do  O
a  O
post-edit.  O
We  O
note  O
that  O
this  O
method  O
can  O
be  O
somewhat  O
noisy  O
as  O
it  O
only  O
takes  O
the  O
pronouns  O
of  O
address  O
into  O
account  O
and  O
not  O
the  O
corresponding  O
verb  O
agreement.  O
How-  O
ever,  O
in  O
our  O
experiments  O
this  O
method  O
has  O
worked  O
well  O
in  O
situations  O
where  O
some  O
noise  O
can  O
be  O
toler-  O
ated,  O
such  O
as  O
post  O
editing  O
mistakes  O
made  O
by  O
a  O
pre-  O
dictive  O
model,  O
use  O
in  O
data  O
augmentation,  O
etc.  O
The  O
rules  O
for  O
T-V  O
conversion  O
and  O
vice-versa  O
are  O
given  O
in  O
Appendix  O
A.  O
V  O
erb  O
Conjugation  O
Apart  O
from  O
pronoun-based  O
T-V  O
form  O
distinction,  O
formality  O
distinctions  O
can  O
be  O
further  O
encoded  O
with  O
verb  O
morphology.  O
For  O
example,  O
the  O
word  O
“to  O
write”  O
in  O
Japanese  O
書く  O
(kaku)  O
can  O
be  O
transformed  O
into  O
its  O
formal/polite  O
form  O
as  O
書きます  O
(kaki-  O
masu).  O
One  O
complexity  O
is  O
that  O
the  O
conjugation  O
of  O
each  O
verb  O
depends  O
on  O
the  O
class  O
of  O
the  O
verb  O
as  O
well  O
as  O
its  O
syntactic  O
context  O
in  O
the  O
sentence.  O
For  O
exam-  O
ple,  O
the  O
verb  O
“write!”  O
書け  O
(kake)  O
has  O
the  O
same  O
stem  O
“書  O
”  O
as書く  O
,  O
yet  O
its  O
formal  O
form  O
is  O
書いて  O
ください  O
(kaite  O
kudasai).  O
To  O
address  O
this  O
issue,  O
we  O
first  O
apply  O
morphological  O
analyzer  O
that  O
jointly  O
identifies  O
the  O
verb  O
and  O
its  O
corresponding  O
verb  O
class,  O
as  O
well  O
as  O
it  O
Part-of-Speech  O
Tag.  O
Then  O
dictionary  O
rules  O
adopted  O
from  O
(  O
Feely  O
et  O
al.  O
,2019a  O
)  O
are  O
applied352W  O
orkflow  O
Description.  O
1⃝  O
Parallel  O
NMT  O
corpus  O
is  O
used  O
to  O
train  O
a  O
generic  O
NMT  O
model.  O
2⃝  O
We  O
leverage  O
linguistic  O
cues  O
(dictio-  O
naries  O
of  O
formality  O
indicators)  O
to  O
extract  O
formal/informal  O
target  O
segments  O
in  O
the  O
parallel  O
corpus,  O
and  O
use  O
then  O
as  O
seed  O
formality  O
annotated  O
training  O
data.  O
3⃝  O
The  O
seed  O
training  O
data  O
is  O
used  O
to  O
train  O
a  O
multilingual  O
formality  O
classifier  O
which  O
then  O
during  O
inference  O
time,  O
automatically  O
labels  O
the  O
formality  O
in  O
the  O
unannotated  O
parallel  O
corpus.  O
4⃝  O
The  O
segments  O
that  O
have  O
prediction  O
confidence  O
>95%,  O
together  O
with  O
the  O
seed  O
formality  O
annotated  O
data  O
is  O
selected  O
as  O
augmented  O
formality  O
data.  O
5⃝  O
The  O
augmented  O
formality  O
data  O
and  O
the  O
provided  O
IWSLT  O
formality  O
training  O
data  O
together  O
finetune  O
the  O
NMT  O
model  O
for  O
the  O
formality  O
control  O
task.  O
6⃝  O
Finally,  O
the  O
translation  O
output  O
of  O
the  O
formality  O
controlled  O
NMT  O
model  O
is  O
further  O
processed  O
by  O
one  O
of  O
three  O
post  O
editing  O
strategies.  O
Figure  O
1:  O
System  O
Architecture  O
Overview  O
to  O
convert  O
the  O
verb  O
into  O
its  O
formal/informal  O
counter-  O
parts.  O
In  O
the  O
proposed  O
system,  O
we  O
applied  O
verb  O
con-  O
jugation  O
for  O
en  O
→  O
ja,  O
and  O
used  O
Kytea5as  O
the  O
mor-  O
phological  O
analyzer.  O
Using  O
Sequence-to-Sequence  O
Model  O
Similar  O
to  O
neural  O
machine  O
translations  O
architec-  O
tures,  O
post  O
editing  O
can  O
be  O
performed  O
by  O
a  O
sequence-  O
to-sequence  O
model  O
where  O
the  O
input  O
is  O
informal  O
or  O
formal  O
while  O
the  O
output  O
is  O
the  O
opposite.  O
In  O
our  O
work,  O
we  O
experiment  O
with  O
transformer  O
based  O
pointer  O
network  O
from  O
Enarvi  O
et  O
al.  O
(2020  O
).6The  O
architecture,  O
originally  O
used  O
for  O
text  O
summarizing,  O
modifies  O
the  O
NMT  O
transformer  O
architecture  O
from  O
Vaswani  O
et  O
al.  O
(2017  O
)  O
with  O
a  O
copy  O
attention  O
mech-  O
anism.  O
In  O
tasks  O
where  O
the  O
input  O
and  O
output  O
dic-  O
tionary  O
are  O
highly  O
similar  O
such  O
grammatical  O
error  O
correction  O
or  O
formality,  O
copy  O
attention  O
allows  O
the  O
model  O
to  O
replicate  O
parts  O
of  O
the  O
input  O
while  O
autore-  O
gressing  O
the  O
output  O
sequence  O
(  O
See  O
et  O
al.  O
,2017  O
).  O
The  O
main  O
benefit  O
of  O
using  O
such  O
a  O
post  O
editing  O
model  O
is  O
that  O
it  O
can  O
be  O
consistently  O
applied  O
across  O
lan-  O
guages  O
i.e.  O
it  O
is  O
language  O
agnostic  O
and  O
does  O
not  O
need  O
any  O
language  O
specific  O
editing  O
methods  O
com-  O
pared  O
to  O
prior  O
approaches.  O
In  O
our  O
implementation,  O
we  O
use  O
the  O
transformer  O
pointer  O
network  O
that  O
is  O
part  O
of  O
the  O
fairseq  O
pack-  O
age  O
and  O
additionally  O
finetune  O
a  O
pretrained  O
mBART  O
(Liu  O
et  O
al.  O
,2020  O
)  O
with  O
the  O
formal-informal  O
parallel  O
corpus  O
provided  O
in  O
this  O
task  O
and  O
monolingual  O
data  O
from  O
the  O
standard  O
translation  O
corpus.  O
For  O
the  O
mono-  O
5http://www.phontron.com/kytea/  O
6https://github.com/pytorch/fairseq/tree/main/examples/  O
pointer_generatorlingual  O
data,  O
the  O
source  O
and  O
target  O
sequences  O
are  O
the  O
same  O
(we  O
copy  O
the  O
source  O
text  O
to  O
the  O
target),  O
al-  O
lowing  O
the  O
model  O
to  O
be  O
trained  O
as  O
an  O
auto-encoder  O
(pre-training  O
the  O
copy  O
attention  O
mechanism).  O
We  O
add  O
two  O
tokens  O
i.e.  O
__F__  O
at  O
the  O
end  O
of  O
formal  O
sen-  O
tences  O
and  O
__IF__  O
at  O
the  O
end  O
of  O
informal  O
sentences  O
to  O
provide  O
a  O
signal  O
to  O
the  O
model  O
of  O
the  O
formality  O
change  O
intent  O
similar  O
to  O
Niu  O
et  O
al.  O
(2018  O
).  O
These  O
tokens  O
are  O
added  O
only  O
to  O
the  O
training  O
data  O
from  O
the  O
formality  O
control  O
corpus  O
provided  O
in  O
this  O
task  O
while  O
the  O
monolingual  O
data  O
remains  O
unchanged.  O
The  O
model  O
is  O
trained  O
in  O
two  O
phases.  O
The  O
first  O
phase  O
pretrains  O
the  O
model  O
as  O
an  O
auto-encoder.  O
The  O
second  O
phase  O
finetunes  O
the  O
model  O
to  O
perform  O
the  O
formality  O
change.  O
For  O
en  O
→  O
hi,  O
we  O
use  O
the  O
target  O
language  O
corpus  O
from  O
Kunchukuttan  O
et  O
al.  O
(2018  O
)  O
while  O
for  O
en  O
→  O
ja,  O
we  O
reuse  O
the  O
corpus  O
from  O
Morishita  O
et  O
al.  O
(2020  O
).  O
A  O
subset  O
of  O
20,000  O
Hindi  O
or  O
Japanese  O
sequences  O
are  O
randomly  O
sampled  O
from  O
the  O
dataset.  O
2.4  O
Augment  O
Weakly-Labeled  O
Data  O
We  O
further  O
explore  O
data  O
augmentation  O
technique  O
to  O
tackle  O
the  O
very  O
limited  O
access  O
to  O
formality  O
anno-  O
tated  O
data.  O
We  O
propose  O
to  O
build  O
a  O
formality  O
classi-  O
fier  O
that  O
automatically  O
labels  O
an  O
unannotated  O
text  O
as  O
“formal”  O
or  O
“informal”.  O
The  O
formality  O
classifier  O
can  O
be  O
trained  O
using  O
a  O
set  O
of  O
seed  O
training  O
data  O
with  O
rule-based  O
automatic  O
annotations.  O
In  O
partic-  O
ular,  O
we  O
apply  O
the  O
T-V  O
distinction  O
technique  O
for  O
en→  O
hi  O
to  O
automatically  O
annotate  O
Hindi  O
texts  O
in  O
the  O
en  O
→  O
hi  O
parallel  O
corpus  O
as  O
“formal”  O
or  O
“infor-  O
mal”.  O
Note  O
that  O
not  O
all  O
Hindi  O
texts  O
have  O
T-V  O
in-353dicators,  O
therefore,  O
only  O
a  O
small  O
subset  O
from  O
the  O
parallel  O
corpus  O
are  O
labelled.  O
Similarly,  O
for  O
en  O
→  O
ja,  O
we  O
follow  O
the  O
technique  O
in  O
Feely  O
et  O
al.  O
(2019b  O
),  O
where  O
we  O
search  O
for  O
Japanese  O
sentences  O
that  O
have  O
more  O
than  O
one  O
verb  O
that  O
indicates  O
formality,  O
and  O
annotate  O
these  O
sentences  O
accordingly.  O
Tables  O
6-8  O
in  O
Appendix  O
summarize  O
the  O
T-V  O
rule  O
for  O
en  O
→  O
hi  O
and  O
formality-indicating  O
verbs  O
for  O
en  O
→  O
ja  O
that  O
were  O
used  O
to  O
generate  O
seed  O
training  O
data.  O
Using  O
the  O
formality  O
labeled  O
texts,  O
we  O
train  O
a  O
mul-  O
tilingual  O
text  O
classifier  O
using  O
multilingual  O
Bert  O
im-  O
plemented  O
with  O
SimpleTransformers.7Then  O
given  O
the  O
text  O
classifier,  O
we  O
automatically  O
label  O
each  O
tar-  O
get  O
segments  O
in  O
the  O
unannotated  O
parallel  O
corpus  O
as  O
formal  O
or  O
informal,  O
which  O
will  O
be  O
used  O
during  O
for-  O
mality  O
control  O
finetuning.  O
To  O
ensure  O
the  O
quality  O
of  O
the  O
formality  O
label,  O
we  O
only  O
select  O
the  O
annotated  O
sentences  O
that  O
have  O
a  O
prediction  O
score  O
higher  O
than  O
a  O
predefined  O
threshold  O
of  O
0.95.  O
During  O
formality  O
finetuning,  O
we  O
upsampled  O
the  O
formality  O
training  O
data  O
to  O
a  O
1:1  O
ratio  O
compared  O
to  O
the  O
automatically  O
annotated  O
data.  O
We  O
summarize  O
the  O
size  O
of  O
the  O
aug-  O
mented  O
data  O
as  O
well  O
as  O
the  O
formality  O
classifier  O
ac-  O
curacy  O
in  O
Appendix  O
C.  O
3  O
Experiments  O
3.1  O
Training  O
Details  O
The  O
NMT  O
model  O
is  O
first  O
finetuned  O
using  O
a  O
large  O
par-  O
allel  O
corpus.  O
For  O
the  O
en  O
→  O
hi  O
pair,  O
we  O
use  O
IIT  O
Bom-  O
bay  O
English-Hindi  O
parallel  O
corpus  O
(  O
Kunchukuttan  O
et  O
al.  O
,2017  O
)  O
that  O
contains  O
1.6  O
Million  O
segments  O
for  O
training.  O
For  O
en  O
→  O
ja,  O
we  O
use  O
two  O
parallel  O
cor-  O
pora  O
-  O
WikiMatrix  O
(  O
Schwenk  O
et  O
al.  O
,2019  O
)  O
and  O
JParaCrawl  O
(  O
Morishita  O
et  O
al.  O
,2019  O
).  O
When  O
fine-  O
tuning  O
the  O
mBART  O
models  O
for  O
both  O
en  O
→  O
hi  O
and  O
en→  O
ja  O
formality  O
tasks,  O
we  O
set  O
the  O
following  O
hyper-  O
parameters:  O
maximum  O
tokens  O
=  O
512,  O
drop  O
out  O
=  O
0.3,  O
learning  O
rate  O
is  O
3e-05  O
for  O
en  O
→  O
ja  O
and  O
3e-04  O
for  O
en→  O
hi,  O
random  O
seed  O
=  O
222,  O
attention-dropout  O
=  O
0.1,  O
weight-decay  O
=  O
0.0.  O
The  O
model  O
is  O
trained  O
for  O
a  O
total  O
of  O
20,000  O
updates  O
for  O
en  O
→  O
ja  O
and  O
160,000  O
updates  O
for  O
en  O
→  O
hi,  O
and  O
the  O
first  O
500  O
updates  O
are  O
used  O
as  O
warmup  O
steps.  O
The  O
model  O
is  O
trained  O
using  O
Adam  O
Optimizer  O
(  O
Kingma  O
and  O
Ba  O
,2015  O
)  O
with  O
β1  O
=  O
0.9.  O
β2=  O
0.98,  O
and  O
ϵ=  O
1e-06.  O
For  O
the  O
alterna-  O
tive  O
Transformer-based  O
NMT  O
architecture,  O
we  O
pre-  O
trained  O
the  O
model  O
with  O
the  O
same  O
dataset,  O
using  O
the  O
same  O
model  O
architecture  O
and  O
setup  O
as  O
the  O
WMT14  O
en-de  O
Transformer  O
model  O
(  O
Gehring  O
et  O
al.  O
,2017  O
).  O
7https://simpletransformers.ai/We  O
further  O
finetune  O
the  O
NMT  O
models  O
using  O
the  O
IWSLT  O
Formality  O
dataset  O
for  O
1,000  O
steps  O
for  O
both  O
language  O
pairs.  O
We  O
chose  O
a  O
small  O
number  O
of  O
train-  O
ing  O
steps  O
for  O
this  O
finetuning  O
step  O
to  O
avoid  O
over-  O
fitting  O
the  O
model  O
and  O
maintain  O
a  O
balanced  O
BLEU  O
score  O
on  O
the  O
generic  O
NMT  O
performance.  O
3.2  O
Evaluation  O
Dataset  O
&  O
Metrics  O
We  O
evaluate  O
the  O
proposed  O
system  O
using  O
the  O
novel  O
IWSLT  O
F  O
ormality  O
Dataset  O
from  O
Nădejde  O
et  O
al.  O
(2022  O
),  O
which  O
is  O
part  O
of  O
the  O
shared  O
IWSLT  O
task.  O
This  O
dataset  O
comprises  O
of  O
source  O
segments  O
paired  O
with  O
two  O
contrastive  O
reference  O
translations,  O
one  O
for  O
each  O
formality  O
level  O
(informal  O
and  O
formal).  O
Since  O
the  O
reference  O
was  O
not  O
disclosed  O
during  O
submission,  O
we  O
used  O
a  O
random  O
sample  O
of  O
25%  O
of  O
the  O
training  O
set  O
as  O
validation  O
data  O
and  O
another  O
non-overlapping  O
25%  O
of  O
the  O
training  O
set  O
as  O
test  O
data.  O
We  O
report  O
the  O
BLEU  O
score  O
(  O
Post  O
,2018  O
)  O
for  O
measuring  O
machine  O
translation  O
quality.  O
We  O
also  O
report  O
the  O
formality  O
control  O
accuracy  O
leveraging  O
phrase-level  O
formal-  O
ity  O
annotations.8We  O
use  O
training  O
/  O
test  O
dataset  O
from  O
both  O
domains,  O
i.e.,  O
telephony  O
and  O
topical-  O
chats  O
(  O
Gopalakrishnan  O
et  O
al.  O
,2019  O
).  O
3.3  O
Results  O
&  O
Findings  O
The  O
performance  O
of  O
all  O
candidates  O
are  O
presented  O
in  O
Table  O
2.  O
We  O
make  O
the  O
following  O
observations.  O
First,  O
compared  O
to  O
the  O
pretrained  O
base  O
model,  O
finetuning  O
strategies  O
significantly  O
improved  O
both  O
BLEU  O
score  O
and  O
formality  O
accuracy.  O
Moreover,  O
the  O
rule-based  O
post  O
editing  O
strategy  O
significantly  O
improves  O
the  O
formality  O
accuracy  O
as  O
compared  O
to  O
the  O
finetuned  O
model  O
without  O
post  O
editing,  O
while  O
maintaining  O
on-par  O
BLEU  O
scores.  O
In  O
particular,  O
the  O
formal  O
accuracy  O
improved  O
from  O
93.9%  O
to  O
95.5%,  O
whereas  O
the  O
informal  O
accuracy  O
improved  O
from  O
98.1%  O
to  O
100%  O
for  O
the  O
en  O
→  O
ja  O
pair.  O
For  O
en  O
→  O
hi,  O
the  O
formal  O
accuracy  O
already  O
reached  O
100%  O
accu-  O
racy  O
without  O
post  O
editing.  O
Therefore,  O
post  O
editing  O
was  O
only  O
performed  O
to  O
improve  O
the  O
informal  O
accu-  O
racy  O
where  O
we  O
observe  O
a  O
huge  O
improvement  O
from  O
84.4%  O
to  O
97.8%.  O
For  O
the  O
seq2seq  O
model-based  O
post  O
editing  O
strat-  O
egy,  O
we  O
only  O
change  O
formal  O
text  O
to  O
informal  O
text.  O
The  O
hypothesis  O
generated  O
is  O
assumed  O
to  O
be  O
for-  O
mal  O
and  O
then  O
post  O
editing  O
is  O
applied  O
to  O
make  O
it  O
in-  O
formal  O
when  O
necessary.  O
Hence,  O
the  O
performance  O
of  O
the  O
model  O
for  O
formal  O
translation  O
is  O
the  O
same  O
Summary  O
of  O
overall  O
performance  O
.  O
The  O
Base  O
model  O
is  O
the  O
pretrained  O
translation  O
model  O
available  O
through  O
sockeye  O
(Domhan  O
et  O
al.  O
,2020  O
).  O
The  O
Finetuned  O
model  O
represents  O
the  O
model  O
finetuned  O
on  O
the  O
IWSLT  O
dataset  O
provided.  O
We  O
utilize  O
two  O
different  O
types  O
of  O
encoder-decoder  O
models.  O
TRF  O
is  O
the  O
Transformer-based  O
translation  O
model  O
available  O
from  O
sockeye,  O
while  O
mBART  O
is  O
the  O
multilingual  O
BART  O
model.  O
We  O
provide  O
results  O
with  O
data  O
augmentation  O
and  O
post  O
editing  O
strategies  O
that  O
include  O
rule-base  O
editing  O
(T-V  O
conversion  O
or  O
verb  O
conjugation)  O
and  O
model-based  O
editing  O
(using  O
mBART  O
transformers  O
from  O
Enarvi  O
et  O
al.  O
(2020  O
)).  O
*represents  O
the  O
type  O
that  O
is  O
generated  O
directly  O
by  O
the  O
Finetuned  O
mBART/TRF  O
model  O
without  O
post  O
editing.  O
as  O
Finetuned  O
mBART  O
,  O
while  O
the  O
informal  O
accuracy  O
and  O
BLEU  O
score  O
changes.  O
We  O
observe  O
that  O
in  O
case  O
of  O
Japanese,  O
the  O
model  O
improves  O
the  O
BLEU  O
score  O
from  O
23.1to25.8but  O
the  O
informal  O
output’s  O
accu-  O
racy  O
score  O
is  O
low  O
at  O
26.2%  O
.  O
For  O
Hindi,  O
the  O
BLEU  O
score  O
is  O
30.9while  O
informal  O
accuracy  O
is  O
1.00%  O
.  O
Analysis  O
of  O
generated  O
informal  O
sentences  O
shows  O
that  O
the  O
model  O
arbitrarily  O
creates  O
copies  O
of  O
text  O
segments  O
(repetition),  O
leading  O
to  O
a  O
reduced  O
BLEU  O
score.  O
We  O
also  O
observe  O
that  O
the  O
data  O
augmentation  O
strategy  O
improves  O
the  O
en  O
→  O
ja  O
pair  O
significantly,  O
re-  O
sulting  O
in  O
formal  O
accuracy  O
increased  O
from  O
93.9%  O
to  O
96.2%,  O
and  O
informal  O
accuracy  O
increases  O
from  O
98.1%  O
to  O
100%.  O
In  O
contrast,  O
the  O
data  O
augmentation  O
causes  O
degradation  O
on  O
the  O
formality  O
accuracy  O
for  O
en→  O
hi  O
and  O
did  O
not  O
improve  O
the  O
BLEU  O
score.  O
This  O
may  O
be  O
due  O
to  O
the  O
noisy  O
seed  O
training  O
data  O
where  O
we  O
used  O
single  O
T-V  O
pronoun  O
matching  O
heuristics  O
for  O
Hindi  O
to  O
select  O
formal/informal  O
seed  O
data  O
in-  O
stead  O
of  O
using  O
a  O
more  O
complete  O
set  O
of  O
heuristics  O
in-  O
cluding  O
verb  O
conjugation  O
matching  O
together  O
with  O
T-V  O
pronoun  O
matching.  O
For  O
Japanese  O
however,  O
the  O
annotations  O
are  O
more  O
accurate  O
as  O
we  O
only  O
select  O
seed  O
data  O
that  O
contains  O
multiple  O
formality  O
indicat-  O
ing  O
verbs.  O
While  O
applying  O
post  O
editing  O
strategies,  O
we  O
made  O
an  O
observation  O
that  O
using  O
different  O
conversion  O
di-  O
rections  O
lead  O
to  O
very  O
different  O
results  O
as  O
indi-  O
cated  O
in  O
Table  O
3.  O
In  O
particular,  O
we  O
found  O
that  O
uni-  O
directional  O
conversions,  O
including  O
formal  O
→  O
formal  O
(i.e.,  O
convert  O
formal  O
hypothesis  O
to  O
formal)  O
and  O
in-  O
formal  O
→  O
informal  O
perform  O
much  O
better  O
than  O
cross-  O
directional  O
conversions  O
such  O
as  O
formal  O
→  O
informal(i.e.,convert  O
formal  O
hypothesis  O
to  O
informal)  O
and  O
in-  O
formal  O
→  O
formal.  O
This  O
is  O
expected  O
due  O
to  O
the  O
typ-  O
ically  O
high  O
precision  O
but  O
low  O
recall  O
of  O
rule-based  O
formality  O
conversions  O
(  O
Feely  O
et  O
al.  O
,2019a  O
),  O
mean-  O
ing  O
that  O
it  O
cannot  O
capture  O
all  O
formality  O
pairs  O
during  O
the  O
conversion,  O
causing  O
degraded  O
accuracy.  O
Rule-based  O
Post  O
Editing  O
Effect  O
w.r.t.  O
Conver-  O
sion  O
Directions.  O
→  O
represents  O
the  O
direction  O
in  O
which  O
post  O
editing  O
happens.  O
Generic  O
NMT  O
performance.  O
Finally,  O
we  O
report  O
the  O
performance  O
of  O
our  O
sub-  O
mitted  O
system  O
on  O
generic  O
NMT  O
test  O
set,  O
and  O
blind  O
IWSLT  O
test  O
set  O
in  O
Table  O
4  O
and  O
Table  O
5  O
as  O
re-  O
quired  O
by  O
the  O
task.  O
For  O
en  O
→  O
hi,  O
our  O
submitted  O
sys-  O
tem  O
employed  O
finetuned  O
mBART  O
+  O
data  O
augmen-  O
tation  O
strategy  O
which  O
demonstrated  O
the  O
best  O
perfor-  O
mance  O
on  O
the  O
development  O
set.  O
For  O
en  O
→  O
ja,  O
the  O
sub-  O
mitted  O
system  O
employs  O
finetuned  O
mBART  O
+  O
data  O
augmentation  O
+  O
post  O
editing  O
(verb  O
conjugation).  O
We  O
have  O
observed  O
that  O
the  O
formality  O
accuracy  O
im-  O
provements  O
are  O
consistent  O
with  O
the  O
observation  O
Formality  O
control  O
performance  O
on  O
blind  O
submission.  O
Table  O
2.  O
Specifically,  O
compared  O
to  O
the  O
finetuned  O
mBART  O
candidate  O
system,  O
we  O
observed  O
0.09%  O
for-  O
mal  O
and  O
7.4%  O
informal  O
absolute  O
accuracy  O
improve-  O
ments  O
for  O
en  O
→  O
hi.  O
For  O
en  O
→  O
ja,  O
we  O
observed  O
3.0%  O
formal  O
and  O
and  O
3.9%  O
informal  O
absolute  O
accuracy  O
improvements.  O
These  O
results  O
indicate  O
the  O
effective-  O
ness  O
of  O
the  O
proposed  O
post  O
editing  O
and  O
data  O
augmen-  O
tation  O
strategies.  O
We  O
observed  O
en  O
→  O
ja  O
improved  O
BLEU  O
score  O
as  O
well.  O
Interestingly,  O
we  O
observed  O
that  O
the  O
proposed  O
system  O
for  O
en  O
→  O
hi  O
had  O
worse  O
BLEU  O
score  O
compared  O
to  O
the  O
finetuned  O
mBART  O
model.  O
One  O
potential  O
cause  O
of  O
this  O
is  O
that  O
the  O
for-  O
mality  O
augmented  O
data  O
for  O
en  O
→  O
hi  O
came  O
from  O
a  O
different  O
domain  O
than  O
the  O
test  O
set  O
which  O
is  O
con-  O
versational  O
in  O
nature.  O
We  O
can  O
potentially  O
improve  O
the  O
BLEU  O
score  O
by  O
augmenting  O
the  O
training  O
data  O
with  O
more  O
conversational  O
data  O
or  O
up-sampling  O
the  O
IWSLT  O
formality  O
data  O
during  O
training.  O
We  O
leave  O
these  O
directions  O
for  O
future  O
improvement.  O
4  O
Background  O
The  O
task  O
of  O
controlling  O
formality  O
in  O
the  O
output  O
of  O
machine  O
translation  O
has  O
drawn  O
much  O
attention  O
in  O
recent  O
MT  O
architectures.  O
Earlier  O
approaches  O
are  O
rule-based  O
systems  O
where  O
non-linguistic  O
informa-  O
tion  O
such  O
as  O
speaker  O
profile  O
and  O
gender  O
information  O
is  O
used  O
to  O
personalized  O
MT  O
with  O
gender/speaker-  O
specific  O
data  O
(  O
Rabinovich  O
et  O
al.  O
,2016  O
;Michel  O
and  O
Neubig  O
,2018  O
).  O
More  O
recently,  O
Niu  O
et  O
al.  O
(2017  O
)  O
coined  O
the  O
term  O
Formality  O
Sensitive  O
Ma-  O
chine  O
Translation  O
(FSMT),  O
and  O
proposed  O
lexical  O
formality  O
models  O
to  O
control  O
the  O
level  O
of  O
formal-  O
ity  O
of  O
MT  O
output  O
by  O
selecting  O
phrases  O
of  O
that  O
are  O
most  O
similar  O
to  O
a  O
desired  O
formality  O
level  O
from  O
the  O
k-best  O
list  O
during  O
decoding.  O
Alternatively,  O
a  O
pop-  O
ular  O
formality  O
control  O
approach  O
is  O
by  O
leveraging  O
side  O
constraints  O
in  O
NMT  O
where  O
a  O
style  O
tag  O
(e.g.,  O
<Formal>/<Informal>)  O
is  O
attached  O
to  O
the  O
beginning  O
of  O
each  O
source  O
example,  O
and  O
the  O
NMT  O
model  O
is  O
forced  O
to  O
“pay  O
attention  O
to”  O
these  O
style  O
tags  O
during  O
translation  O
(  O
Sennrich  O
et  O
al.  O
,2016  O
;Niu  O
and  O
Carpuat  O
,  O
2020  O
).  O
Formality  O
control  O
for  O
machine  O
translation  O
is  O
closely  O
related  O
to  O
formality  O
transfer  O
(FT),  O
whichis  O
the  O
task  O
of  O
automatically  O
transforming  O
text  O
in  O
one  O
formality  O
style  O
(e.g.,  O
”informal”)  O
into  O
another  O
(e.g.,  O
polite)  O
(  O
Niu  O
et  O
al.  O
,2018  O
).  O
The  O
FT  O
task  O
usu-  O
ally  O
takes  O
a  O
seq2seq-like  O
approach  O
(  O
Zhang  O
et  O
al.  O
,  O
2020  O
)  O
given  O
parallel  O
corpus  O
such  O
as  O
Grammarly  O
’s  O
Yahoo  O
Answers  O
Formality  O
Corpus  O
(GYAFC)  O
(  O
Rao  O
and  O
Tetreault  O
,2018  O
).  O
These  O
FT  O
models  O
are  O
often  O
applied  O
as  O
a  O
rewriting  O
mechanism  O
after  O
the  O
MT  O
out-  O
puts  O
are  O
generated.  O
Recently,  O
Niu  O
et  O
al.  O
(2018  O
)  O
pro-  O
posed  O
a  O
novel  O
multi-task  O
model  O
that  O
jointly  O
per-  O
form  O
FT  O
and  O
FSMT.  O
Honorifics  O
based  O
post  O
editing  O
approaches  O
have  O
also  O
been  O
widely  O
deployed  O
for  O
for-  O
mality  O
control  O
tasks.  O
A  O
widespread  O
instance  O
of  O
us-  O
ing  O
honorifics  O
to  O
determine  O
register  O
is  O
the  O
grammat-  O
ical  O
T-V  O
distinction  O
(  O
Brown  O
and  O
Gilman  O
,1960  O
),  O
distinguishing  O
between  O
the  O
informal  O
(Latin  O
T  O
u  O
)  O
and  O
the  O
formal  O
(Latin  O
V  O
os  O
).  O
Alternatively,  O
verb  O
conjuga-  O
tion  O
combined  O
with  O
syntactic  O
parsing  O
has  O
been  O
used  O
to  O
alter  O
the  O
inflection  O
of  O
the  O
main  O
verb  O
of  O
the  O
sen-  O
tence  O
to  O
achieve  O
multiple  O
levels  O
of  O
formality  O
(  O
Feely  O
et  O
al.  O
,2019a  O
).  O
5  O
Conclusion  O
In  O
this  O
paper,  O
we  O
target  O
improving  O
the  O
ma-  O
chine  O
translation  O
formality  O
control  O
performance  O
given  O
limited  O
formality  O
annotated  O
training  O
data.  O
We  O
explored  O
three  O
different  O
strategies  O
including  O
rule-based  O
post  O
editing,  O
seq2seq  O
point  O
networks,  O
and  O
formality  O
classifier-based  O
augmentation.  O
We  O
found  O
that  O
data  O
augmentation  O
using  O
formality  O
clas-  O
sifier  O
significantly  O
improved  O
formality  O
accuracy  O
on  O
en→  O
ja  O
pair.  O
We  O
also  O
found  O
that  O
post  O
editing  O
strate-  O
gies  O
on  O
top  O
of  O
finetuned  O
mBART  O
models  O
are  O
sim-  O
ple  O
and  O
effective  O
ways  O
to  O
improve  O
the  O
formality  O
control  O
performance.  O
Results  O
on  O
the  O
IWSLT  O
test-  O
set  O
have  O
indicated  O
performance  O
improvements  O
in  O
terms  O
of  O
formality  O
accuracy  O
in  O
both  O
en  O
→  O
hi  O
and  O
en→  O
ja  O
pairs  O
while  O
retaining  O
on-par  O
BLEU  O
score.  O