Findings O
of O
the O
Association O
for O
Computational O
Linguistics: O
NAACL O
2022 O
, O
pages O
2715 O
- O
2726 O
July O
10-15, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
ID10M: O
Idiom B-TaskName
Identiﬁcation I-TaskName
in O
10 O
Languages O
Simone O
Tedeschi1,2, O
Federico O
Martelli2andRoberto O
Navigli2 O
1Babelscape, O
Italy O
2Sapienza O
NLP O
Group, O
Sapienza O
University O
of O
Rome O
tedeschi@babelscape.com O
,martelli@di.uniroma1.it O
, O
navigli@diag.uniroma1.it O
Abstract O
Idioms O
are O
phrases O
which O
present O
a O
ﬁgura- O
tive O
meaning O
that O
cannot O
be O
(completely) O
de- O
rived O
by O
looking O
at O
the O
meaning O
of O
their O
in- O
dividual O
components. O
Identifying B-TaskName
and I-TaskName
under- I-TaskName
standing I-TaskName
idioms I-TaskName
in O
context O
is O
a O
crucial O
goal O
and O
a O
key O
challenge O
in O
a O
wide O
range O
of O
Natu- O
ral O
Language O
Understanding O
tasks. O
Although O
efforts O
have O
been O
undertaken O
in O
this O
direc- O
tion, O
the O
automatic B-TaskName
identiﬁcation I-TaskName
and I-TaskName
under- I-TaskName
standing I-TaskName
of I-TaskName
idioms I-TaskName
is O
still O
a O
largely O
under- O
investigated O
area, O
especially O
when O
operating O
in O
a O
multilingual O
scenario. O
In O
this O
paper, O
we O
address O
such O
limitations O
and O
put O
forward O
sev- O
eral O
new O
contributions: O
we O
propose O
a O
novel O
multilingual O
Transformer-based O
system O
for O
the O
identiﬁcation O
of O
idioms; O
we O
produce O
a O
high- O
quality O
automatically-created O
training O
dataset O
in O
10 O
languages, O
along O
with O
a O
novel O
manually- O
curated O
evaluation O
benchmark; O
ﬁnally, O
we O
carry O
out O
a O
thorough O
performance O
analysis O
and O
release O
our O
evaluation O
suite O
at O
https:// O
github.com/Babelscape/ID10M O
. O
1 O
Introduction O
Idioms O
pertain O
to O
a O
wider O
family O
of O
linguistic O
phenomena O
referred O
to O
as O
multi-word O
expressions O
(MWEs). O
Broadly O
speaking, O
an O
MWE O
can O
be O
de- O
ﬁned O
as O
a O
combination O
of O
two O
or O
more O
words, O
be- O
having O
as O
a O
complex O
lexical O
unit O
and O
showing O
id- O
iosyncratic O
properties O
(Baldwin O
and O
Kim, O
2010). O
Over O
the O
course O
of O
the O
last O
few O
years, O
several O
at- O
tempts O
have O
been O
made O
to O
classify O
MWEs O
based O
on O
speciﬁc O
dimensions O
such O
as O
polylexicality, O
ﬁxed- O
ness, O
compositionality O
and O
idiomaticity O
(Sailer O
and O
Markantonatou, O
2018). O
According O
to O
Sag O
et O
al. O
(2002), O
MWEs O
can O
be O
divided O
into O
lexicalized O
and O
institutionalized O
phrases. O
While O
the O
former O
show O
syntactic O
or O
semantic O
idiosyncrasies, O
e.g. O
kingdom O
come O
andspill O
the O
beans O
, O
the O
latter O
are O
composi- O
tional O
from O
a O
syntactic O
and O
semantic O
perspective, O
but O
statistically O
idiosyncratic, O
e.g. O
trafﬁc O
light O
and O
telephone O
booth O
.Among O
lexicalized O
phrases, O
idioms O
are O
of O
par- O
ticular O
interest O
in O
that O
their O
meaning O
cannot O
be O
ob- O
tained O
by O
compositionally O
interpreting O
their O
word O
constituents. O
These O
include O
non-compositional O
phrases, O
e.g. O
kick O
the O
bucket O
, O
and O
partially- O
compositional O
phrases, O
e.g. O
rain O
cats O
and O
dogs O
(Nunberg O
et O
al., O
1994). O
Given O
their O
complex O
nature, O
idioms O
are O
hard O
to O
be O
automatically O
identiﬁed O
and O
pose O
a O
crucial O
chal- O
lenge O
to O
the O
entire O
ﬁeld O
of O
Natural O
Language O
Under- O
standing O
(NLU). O
Although O
research O
in O
this O
ﬁeld O
has O
recently O
achieved O
great O
advancements, O
the O
current O
formulation O
of O
many O
tasks O
tends O
to O
overlook O
the O
idiomatic O
usage O
of O
language. O
Instead, O
idioms O
ought O
to O
be O
playing O
an O
important O
role O
in O
NLU O
as O
they O
are O
a O
frequent O
phenomenon O
which O
can O
be O
observed O
in O
all O
languages. O
The O
correct O
identiﬁcation O
of O
id- O
ioms O
in O
context O
is O
crucial O
for O
tasks O
such O
as O
Word O
Sense O
Disambiguation O
(Bevilacqua O
et O
al., O
2021) O
and O
Entity O
Linking O
(Sevgili O
et O
al., O
2020), O
but O
also O
for O
many O
downstream O
applications. O
For O
instance, O
in O
Question O
Answering O
or O
dialog, O
a O
system O
must O
be O
able O
to O
understand O
"It O
was O
a O
piece O
of O
cake" O
in O
relation O
to O
the O
question O
"How O
was O
the O
test?" O
(Jham- O
tani O
et O
al., O
2021; O
Mishra O
and O
Jain, O
2016). O
Simi- O
larly, O
if O
the O
idiom O
kick O
the O
bucket O
is O
identiﬁed, O
then O
a O
Text O
Summarization O
system O
would O
be O
able O
to O
summarize O
all O
its O
occurrences O
within O
a O
text O
with O
"die" O
(Chu O
and O
Wang, O
2018; O
Gambhir O
and O
Gupta, O
2017). O
Finally, O
once O
an O
idiom O
is O
identiﬁed, O
a O
Ma- O
chine O
Translation O
system O
would O
then O
be O
able O
to O
avoid O
its O
compositional O
translation, O
and O
treat O
it O
as O
a O
whole O
(Anastasiou, O
2010). O
Furthermore, O
idioms O
are O
widely O
studied O
in O
linguistics O
and O
psycholinguistics O
(Cacciari O
and O
Tabossi, O
1988; O
Gibbs O
Jr, O
1992; O
Nun- O
berg O
et O
al., O
1994; O
Cacciari O
and O
Tabossi, O
2014; O
Liu, O
2017), O
hence O
a O
system O
capable O
of O
effectively O
identi- B-TaskName
fying I-TaskName
idioms I-TaskName
in I-TaskName
texts I-TaskName
would O
signiﬁcantly O
improve O
many O
research O
areas, O
far O
beyond O
NLU. O
Most O
of O
the O
past O
idiom O
extraction O
strategies O
fo- O
cused O
on O
speciﬁc O
domains O
and O
on O
a O
limited O
number2715of O
languages. O
In O
our O
work, O
we O
tackle O
these O
short- O
comings O
and, O
taking O
inspiration O
from O
the O
Named O
Entity O
Recognition O
(NER) O
task O
(Yadav O
and O
Bethard, O
2018), O
we O
reformulate O
the O
identiﬁcation B-TaskName
of I-TaskName
idioms I-TaskName
as O
a O
sequence O
labeling O
task. O
Speciﬁcally, O
we O
pro- O
pose O
the O
following O
new O
contributions: O
•We O
design O
a O
novel O
multilingual O
Transformer- O
based O
system O
for O
the O
identiﬁcation O
of O
idioms; O
•We O
release O
a O
high-quality O
silver O
training O
dataset O
in O
10 O
languages O
and O
a O
novel O
manually- O
curated O
evaluation O
benchmark O
in O
4 O
languages; O
•We O
measure O
the O
quality O
of O
the O
data O
produced O
and O
of O
our O
system O
design O
through O
an O
extensive O
evaluation. O
We O
hope O
that O
this O
work O
will O
provide O
a O
step- O
ping O
stone O
for O
further O
studies O
regarding O
idiomatic O
expressions O
and O
their O
applications, O
and O
encour- O
age O
further O
work O
on O
the O
identiﬁcation B-TaskName
of I-TaskName
idioms I-TaskName
in I-TaskName
multiple I-TaskName
languages. I-TaskName
We O
release O
the O
produced O
datasets O
and O
software O
at O
https://github.com/ O
Babelscape/ID10M O
. O
2 O
Related O
Work O
Systems O
Over O
the O
course O
of O
the O
past O
two O
decades, O
several O
approaches O
have O
been O
put O
forward O
to O
ad- O
dress O
the O
idiom B-TaskName
identiﬁcation I-TaskName
task. I-TaskName
To O
this O
end, O
two O
main O
properties O
of O
idioms O
have O
been O
leveraged, O
namely O
their O
syntactic O
and O
semantic O
idiosyncrasies. O
While O
the O
former O
refers O
to O
the O
peculiar O
syntactic O
behaviour O
of O
idioms, O
the O
latter O
indicates O
the O
linguis- O
tic O
property O
in O
which O
the O
meaning O
of O
an O
idiomatic O
expression O
cannot O
be O
completely O
derived O
from O
the O
meaning O
of O
its O
individual O
components. O
Initial O
studies O
regarding O
idiom O
identiﬁcation O
fo- O
cused O
on O
syntactic O
idiosyncrasy, O
concentrating O
on O
verb/noun O
idioms, O
e.g. O
shoot O
the O
breeze O
(Fazly O
and O
Stevenson, O
2006; O
Cook O
et O
al., O
2007; O
Diab O
and O
Bhutada, O
2009), O
on O
verb/particle O
idioms, O
e.g. O
call O
off(Ramisch O
et O
al., O
2008) O
or O
on O
idioms O
satisfying O
speciﬁc O
restrictions, O
i.e. O
subject/verb O
such O
as O
ten- O
sion O
mounted O
and O
verb/direct-object, O
e.g. O
break O
the O
ice(Shutova O
et O
al., O
2010). O
Subsequent O
approaches O
exploited O
semantic O
id- O
iosyncrasies. O
This O
property O
implies O
that O
idiomatic O
expressions O
often O
occur O
in O
contexts O
typically O
unre- O
lated O
to O
the O
meaning O
of O
their O
individual O
constituents, O
thus O
providing O
a O
key O
feature O
to O
be O
exploited O
in O
an O
au- O
tomatic O
approach. O
In O
particular, O
Muzny O
and O
Zettle- O
moyer O
(2013) O
introduced O
new O
lexical O
and O
graph-based O
features O
that O
use O
WordNet1and O
Wiktionary2, O
and O
proposed O
a O
simple O
yet O
efﬁcient O
binary O
Percep- O
tron O
classiﬁer O
to O
distinguish O
between O
idiomatic O
and O
non-idiomatic O
expressions O
by O
exploiting O
their O
com- O
ponents O
and O
dictionary O
deﬁnitions. O
A O
similar, O
but O
unsupervised O
approach O
was O
adopted O
by O
Verma O
and O
Vuppuluri O
(2015) O
which O
relied O
on O
the O
dictionary O
deﬁnitions O
of O
each O
component O
of O
a O
given O
idiom. O
These O
latter O
methods O
have O
more O
recently O
been O
superseded O
by O
approaches O
making O
use O
of O
dis- O
tributional O
similarity O
in O
the O
form O
of O
both O
static O
and O
contextualized O
word O
embeddings O
(Gharbieh O
et O
al., O
2016; O
Ehren, O
2017; O
Senaldi O
et O
al., O
2019; O
Hashempour O
and O
Villavicencio, O
2020; O
Fakharian, O
2021; O
Garcia O
et O
al., O
2021; O
Nedumpozhimana O
and O
Kelleher, O
2021), O
while O
keeping O
the O
underlying O
as- O
sumption O
unchanged: O
the O
vector O
representation O
of O
the O
component O
words O
should O
be O
distant O
from O
the O
vector O
representation O
of O
the O
context O
or O
of O
the O
ex- O
pression O
as O
a O
whole. O
Notwithstanding O
the O
recent O
improvements, O
to O
the O
best O
of O
our O
knowledge, O
the O
identiﬁcation O
of O
id- O
iomatic O
expressions O
in O
multiple O
languages O
is O
largely O
under-investigated. O
Datasets O
In O
the O
early O
2000s, O
several O
datasets O
for O
idiom B-TaskName
identiﬁcation I-TaskName
were O
created. O
For O
instance, O
Cook O
et O
al. O
(2008) O
and O
Sporleder O
et O
al. O
(2010) O
man- O
ually O
selected O
a O
limited O
number O
of O
idioms, O
and O
then O
extracted O
sentences O
containing O
such O
idioms O
from O
the O
British O
National O
Corpus O
(BNC, O
Consor- O
tium O
et O
al., O
2007). O
Similarly, O
Sporleder O
and O
Li O
(2009) O
extracted O
a O
dataset O
from O
the O
Gigaword O
cor- O
pus O
(Graff O
and O
Christopher, O
2003). O
Street O
et O
al. O
(2010), O
instead, O
used O
multiple O
annotators O
to O
vali- O
date O
sentences O
from O
the O
American O
National O
Cor- O
pus O
(ANC, O
Ide O
and O
Macleod, O
2001). O
Additionally, O
Muzny O
and O
Zettlemoyer O
(2013) O
created O
a O
dataset O
by O
applying O
the O
aforementioned O
classiﬁer O
on O
Wik- O
tionary O
entries, O
more O
than O
doubling O
the O
number O
of O
idiomatic O
expressions O
in O
Wiktionary. O
Furthermore, O
Korkontzelos O
et O
al. O
(2013) O
intro- O
duced O
Task O
5b O
at O
SemEval-2013 O
regarding O
the O
de- O
tection O
of O
semantic O
compositionality O
in O
context. O
The O
authors O
selected O
idioms O
from O
Wiktionary, O
and O
extracted O
instances O
from O
the O
ukWaC O
corpus O
(Fer- O
raresi O
et O
al., O
2008). O
Schneider O
et O
al. O
(2016), O
in- O
stead, O
proposed O
the O
DiMSUM O
dataset O
for O
Task O
10 O
at O
SemEval-2016, O
and O
extracted O
annotations O
from O
reviews, O
tweets O
and O
TED O
talks. O
However, O
this O
work O
1https://wordnet.princeton.edu/ O
2https://www.wiktionary.org/2716did O
not O
categorise O
MWEs O
into O
subtypes, O
making O
it O
difﬁcult O
to O
quantify O
the O
number O
of O
idioms O
in O
the O
corpus. O
Finally, O
Peng O
et O
al. O
(2015) O
expanded O
the O
dataset O
introduced O
by O
Cook O
et O
al. O
(2008) O
by O
retrieving O
fur- O
ther O
sentences O
from O
the O
BNC O
corpus, O
while O
more O
recently O
Gong O
et O
al. O
(2017) O
introduced O
a O
small-scale O
dataset O
derived O
from O
Google O
Books3for O
English O
and O
Chinese. O
Unfortunately, O
almost O
all O
the O
aforementioned O
ap- O
proaches O
focused O
on O
English. O
The O
ﬁrst O
concrete O
attempt O
to O
scale O
to O
multiple O
languages O
was O
made O
by O
Madabushi O
et O
al. O
(2021) O
who O
also O
proposed O
a O
SemEval-2022 O
task O
on O
idiom O
identiﬁcation. O
Never- O
theless, O
their O
datasets O
are O
limited O
in O
size O
and O
they O
only O
cover O
three O
languages, O
namely O
English, O
Por- O
tuguese O
and O
Galician. O
3 O
ID10M O
In O
what O
follows, O
we O
ﬁrst O
describe O
the O
creation O
pro- O
cess O
of O
our O
training O
datasets O
(Section O
3.1) O
and O
the O
manually-curated O
test O
sets O
(Section O
3.2). O
Then, O
we O
introduce O
our O
new O
task O
formulation O
and O
illustrate O
the O
architecture O
of O
our O
idiom B-TaskName
identiﬁcation I-TaskName
system I-TaskName
(Section O
3.3). O
3.1 O
Silver-Standard O
Data O
Creation O
Automatic O
Annotation O
In O
order O
to O
create O
our O
training O
data, O
we O
exploit O
Wiktionary4as B-DatasetName
the O
main O
source, O
as O
it O
provides O
access O
to O
a O
large O
number O
of O
MWEs O
along O
with O
usage O
examples O
in O
multiple O
languages. O
However, O
since O
such O
examples O
are O
pro- O
vided O
for O
a O
limited O
number O
of O
MWEs, O
we O
search O
for O
further O
textual O
contexts O
in O
a O
large O
external O
source, O
namely O
WikiMatrix5(Schwenk B-DatasetName
et O
al., O
2021), O
a O
mul- O
tilingual O
corpus O
that O
covers O
83 O
languages O
and O
con- O
tains O
parallel O
sentences O
extracted O
from O
Wikipedia6. B-DatasetName
We O
perform O
data O
extraction O
as O
follows. O
Let O
El O
be O
the O
set O
of O
MWEs O
available O
in O
Wiktionary B-DatasetName
in O
the O
languagel, O
with|El|=n, O
and O
let O
us O
deﬁne O
the O
functionL(p)that, O
given O
a O
phrase O
p, O
outputs O
its O
lemma. O
Then, O
we O
apply O
a O
heuristic O
which O
allows O
3https://books.google.com/ O
4We O
employ O
the O
Wiktextract O
library O
to O
collect O
the O
neces- O
sary O
data O
from O
Wiktionary. B-DatasetName
WikiExtract O
( O
https://pypi. O
org/project/wiktextract/ O
) O
provides O
a O
preprocessed O
version O
of O
the O
Wiktionary B-DatasetName
dump O
together O
with O
useful O
APIs. O
5https://github.com/facebookresearch/ O
LASER/tree/main/tasks/WikiMatrix O
6The O
encyclopedia-style O
prose O
of O
Wikipedia B-DatasetName
could O
have O
a O
lower O
idiom O
density O
compared O
to O
other O
textual O
sources, O
but O
the O
large O
size O
of O
WikiMatrix B-DatasetName
should O
balance O
this O
lower O
density.us, O
for O
each O
expression O
ei∈El, O
to O
search O
for O
a O
sen- O
tence O
in O
WikiMatrix B-DatasetName
such O
that O
there O
exists O
at O
least O
a O
span O
of O
tokens O
Sk−jstarting O
at O
index O
kand O
ending O
at O
indexj, O
whereei=Sk−j∨ei=L(Sk−j)∨ O
L(ei) O
=Sk−j∨L(ei) O
=L(Sk−j). O
By O
applying O
this O
heuristic, O
not O
only O
do O
we O
obtain O
a O
large O
set O
of O
sentences O
containing O
potentially O
idiomatic O
expres- O
sions O
(PIEs), O
but O
– O
thanks O
to O
the O
lemmatization O
step O
– O
we O
also O
collect O
several O
morphological O
variations O
of O
the O
original O
expressions O
in O
El, O
e.g. O
starting O
from O
‘kick O
the O
bucket O
’, O
we O
also O
obtain O
‘ O
kickedthe O
bucket O
’ O
and O
‘ O
kicksthe O
bucket O
’. O
In O
particular, O
if O
an O
MWE O
is O
marked O
as O
idiomatic O
in O
Wiktionary, O
we O
mark O
all O
its O
occurrences O
as O
idiomatic O
too. O
Similarly, O
if O
an O
MWE O
is O
not O
marked O
as O
idiomatic O
in O
Wiktionary, O
we O
mark O
all O
its O
occurrences O
as O
literal. O
However, O
this O
has O
a O
limitation: O
if O
an O
MWE O
is O
labeled O
as O
idiomatic O
(or O
literal) O
in O
Wiktionary, O
it O
will O
not O
necessarily O
always O
also O
be O
idiomatic O
(or O
literal) O
in O
the O
WikiMatrix B-DatasetName
sen- O
tences O
in O
which O
it O
appears. O
We O
adopt O
the O
above-described O
procedure O
to O
cre- O
ate O
datasets O
in O
the O
following O
10 O
languages: O
Chinese, O
Dutch, O
English, O
French, O
German, O
Italian, O
Japanese, O
Polish, O
Portuguese O
and O
Spanish. O
Automatic O
Validation O
Since O
the O
data O
derived O
from O
Wiktionary B-DatasetName
and O
WikiMatrix B-DatasetName
may O
contain O
er- O
rors, O
we O
aim O
at O
automatically O
improving O
their O
qual- O
ity. O
To O
achieve O
this O
goal, O
we O
exploit O
the O
semantic O
idiosyncrasy O
property O
of O
idiomatic O
expressions, O
and O
the O
consequent O
fact O
that O
the O
meaning O
of O
the O
indi- O
vidual O
constituents O
of O
idiomatic O
expressions O
are O
unrelated O
to O
the O
surrounding O
context. O
Speciﬁcally, O
following O
this O
intuition, O
and O
by O
taking O
inspiration O
from O
recent O
advances O
in O
the O
main O
disambiguation O
tasks O
(Blevins O
and O
Zettlemoyer, O
2020; O
Botha O
et O
al., O
2020; O
Tedeschi O
et O
al., O
2021), O
we O
design O
a O
dual- B-MethodName
encoder I-MethodName
architecture I-MethodName
(Figure O
1) O
to O
produce O
a O
vector O
representation O
for O
both O
the O
expression O
and O
its O
con- O
text, O
and O
then, O
based O
on O
their O
cosine O
similarity, O
label O
the O
expression O
as O
idiomatic O
or O
literal. O
More O
formally, O
let O
us O
deﬁne O
an O
expression B-MethodName
en- I-MethodName
coder I-MethodName
Ψand I-MethodName
a O
context B-MethodName
encoder I-MethodName
Ω. I-MethodName
Then, O
given O
an O
expression-context O
pair O
/angbracketlefte,c/angbracketright, O
the O
output O
of O
the O
dual-encoder B-MethodName
architecture I-MethodName
Φis I-MethodName
deﬁned O
as O
follows: O
Φ(e,c) O
= O
 O
1,ifΨ(e)TΩ(c) O
/bardblΨ(e)/bardbl/bardblΩ(c)/bardbl≤δ O
0,otherwise O
where O
Φ(e,c) O
= O
1 O
means O
thateis O
idiomatic O
in O
c, O
while O
Φ(e,c) O
= O
0 O
ifehas O
a O
literal O
meaning2717Figure O
1: O
Graphical O
representation O
of O
the O
dual-encoder O
architecture O
given O
as O
input O
an O
example O
sentence. O
“E" O
stands O
for O
Embedding. O
A O
potentially O
idiomatic O
expression O
eis O
labeled O
as O
idiomatic O
when O
the O
cosine O
similarity O
score O
between O
the O
representations O
Ω(c)andΨ(e), O
wherecis O
the O
surrounding O
context, O
is O
lower O
than O
the O
threshold O
δ. B-HyperparameterName
inc.δis B-HyperparameterName
a O
manually-tuned O
threshold. O
Both O
en- O
coders O
are O
bert-base-multilingual-cased O
ar- O
chitectures O
that O
take O
as O
input O
the O
tokenized O
versions O
of O
expressions O
and O
their O
contexts, O
respectively, O
sur- O
rounded O
by O
the O
special O
tokens O
[CLS] O
and O
[SEP]. O
To O
encode O
an O
expression, O
we O
take O
the O
sum O
of O
the O
indi- O
vidual O
representations O
of O
all O
its O
subwords. O
Instead, O
for O
the O
representation O
of O
the O
context O
we O
take O
the O
representation O
of O
the O
[CLS] O
token. O
We O
evaluate O
the O
quality O
of O
our O
dual O
encoder O
in O
Section O
4.3. O
Additionally, O
to O
further O
improve O
the O
quality O
of O
the O
annotations O
produced, O
we O
follow O
the O
recent O
ﬁnd- O
ings O
of O
Tedeschi O
and O
Navigli O
(2022) O
which O
demon- O
strated O
how O
NER O
can O
be O
exploited O
to O
better O
dis- O
criminate O
between O
idiomatic O
and O
literal O
usages O
of O
potentially O
idiomatic O
expressions. O
3.2 O
Gold-Standard O
Data O
Creation O
To O
evaluate O
the O
performance O
of O
our O
idiom O
identiﬁca- O
tion O
system, O
we O
manually O
create O
a O
novel O
evaluation O
benchmark O
in O
4 O
languages, O
i.e. O
English, O
German, O
Italian O
and O
Spanish. O
As O
explained O
in O
Section O
3.1, O
we O
start O
by O
producing O
a O
set O
of O
sentences O
contain- O
ing O
PIEs. O
Then, O
to O
properly O
label O
the O
expressions, O
depending O
on O
the O
context O
in O
which O
they O
occur, O
weask O
professional O
annotators7to O
perform O
the O
fol- O
lowing O
binary O
classiﬁcation O
task: O
given O
a O
context- O
expression O
pair/angbracketlefte,c/angbracketright, O
the O
goal O
is O
to O
tag O
this O
pair O
with O
a O
label O
y∈{Idiomatic,Literal O
}. O
In O
order O
to O
make O
our O
gold O
standard O
more O
challenging, O
and O
better O
evaluate O
the O
system O
performance, O
we O
also O
ask O
annotators O
to O
include O
unseen O
idioms, O
i.e. O
idioms O
that O
do O
not O
appear O
in O
the O
training O
set. O
3.3 O
Idiom B-TaskName
Identiﬁcation I-TaskName
Task O
Formulation O
Current O
and O
past O
approaches O
to O
idiom B-TaskName
identiﬁcation I-TaskName
typically O
take O
expressions- O
context O
pairs/angbracketlefte,c/angbracketrightas O
input O
and O
limit O
themselves O
to O
determining O
whether O
eis O
used O
with O
a O
ﬁgurative O
meaning O
or O
not O
in O
c(Madabushi O
et O
al., O
2021; O
Muzny O
and O
Zettlemoyer, O
2013). O
However, O
this O
formulation O
has O
a O
major O
drawback: O
potentially O
idiomatic O
ex- O
pressions O
need O
to O
be O
pre-identiﬁed. O
Importantly, O
we O
drop O
this O
requirement O
and O
reformulate O
the O
task O
as O
a O
sequence-labeling O
task, O
by O
employing O
the O
well- O
known O
BIO O
tagging O
scheme8. O
7We O
hired O
a O
mother-tongue O
professional O
annotator O
for O
each O
language. O
8The O
BIO O
tagging O
scheme O
(short O
for O
Beginning, O
Interme- O
diate, O
Out) O
is O
a O
popular O
tagging O
scheme O
where O
the O
B O
label O
indicates O
that O
the O
corresponding O
token O
is O
the O
ﬁrst O
token O
of O
a2718Language O
# O
Sentences O
# O
Tokens O
# O
Idioms O
# O
B O
# O
I O
# O
O O
# O
Seen O
# O
Unseen O
# O
LiteralSilver O
DataChinese O
(ZH) O
9543 O
244422 O
1301 O
5272 O
3823 O
235327 O
- O
- O
3918 O
Dutch O
(NL) O
20935 O
548872 O
189 O
4530 O
10543 O
533799 O
- O
- O
16366 O
English O
(EN) O
37919 O
1199492 O
4568 O
10102 O
19884 O
1169506 O
- O
- O
27408 O
French O
(FR) O
35588 O
939161 O
188 O
12112 O
25248 O
901801 O
- O
- O
23238 O
German O
(DE) O
26963 O
722109 O
819 O
8311 O
11500 O
702298 O
- O
- O
18488 O
Italian O
(IT) O
29523 O
813445 O
452 O
8768 O
12353 O
792324 O
- O
- O
20506 O
Japanese O
(JA) O
6388 O
211437 O
165 O
2534 O
1662 O
207241 O
- O
- O
3852 O
Polish O
(PL) O
36333 O
862265 O
648 O
12971 O
14364 O
834930 O
- O
- O
22467 O
Portuguese O
(PT) O
30942 O
764017 O
559 O
5824 O
8871 O
749322 O
- O
- O
24816 O
Spanish O
(ES) O
28647 O
648776 O
1229 O
9994 O
13927 O
624855 O
- O
- O
17851Gold O
DataEnglish O
(EN) O
200 O
3287 O
142 O
159 O
373 O
2755 O
62 O
80 O
41 O
German O
(DE) O
200 O
4529 O
111 O
181 O
377 O
3971 O
71 O
40 O
19 O
Italian O
(IT) O
200 O
5043 O
139 O
155 O
271 O
4617 O
87 O
52 O
48 O
Spanish O
(ES) O
200 O
2240 O
78 O
133 O
348 O
1759 O
19 O
59 O
66 O
Table O
1: O
Statistics O
concerning O
the O
automatically-created O
(Silver O
Data) O
training O
sets O
and O
our O
manually-curated O
test O
sets O
(Gold O
Data). O
"# O
Seen" O
represents O
the O
number O
of O
expressions O
in O
the O
test O
set O
already O
encountered O
in O
the O
training O
set, O
whereas O
"# O
Unseen" O
is O
the O
number O
of O
expressions O
never O
encountered. O
In O
the O
count O
of O
individual O
idioms O
(# O
Idioms), O
morphological O
variations O
of O
a O
certain O
idiom O
are O
mapped O
to O
the O
same O
idiom. O
More O
formally, O
given O
as O
input O
a O
raw O
text O
se- O
quenceXofntokensx1,...,x O
n, O
eachximust O
be O
labeled O
by O
the O
system O
with O
a O
tag O
yi∈{B,I,O} O
for O
eachi∈[1,n]. O
This O
formulation O
also O
allows O
us O
to O
easily O
handle O
multiple O
idiomatic O
expressions O
within O
the O
same O
text. O
In O
order O
to O
use O
our O
new O
formulation, O
we O
convert O
all O
the O
datasets O
constructed O
in O
Section O
3.1 O
and O
Sec- O
tion O
3.2 O
in O
BIO O
format. O
Table O
2 O
shows O
an O
example O
of O
instance O
labeled O
using O
the O
BIO O
tagging O
scheme. O
Our O
System O
Our O
model O
for O
idiom O
identiﬁcation O
is O
inspired O
by O
the O
BERT-based O
neural O
architecture O
of O
Mueller O
et O
al. O
(2020) O
used O
for O
Named O
Entity O
Recognition, O
however, O
rather O
than O
encoding O
a O
word O
with O
the O
ﬁrst O
contextualized O
subword O
representa- O
tion O
as O
indicated O
by O
Devlin O
et O
al. O
(2019), O
we O
take O
the O
mean O
of O
its O
subwords, O
as O
suggested O
by O
recent O
literature O
(Ács O
et O
al., O
2021). O
Then, O
the O
resulting O
vectors O
are O
passed O
through O
a O
multi-layer O
sentence- O
level O
BiLSTM B-MethodName
network, O
whose O
logits O
are O
ﬁnally O
fed O
into O
a O
CRF B-MethodName
model, O
trained O
to O
maximize O
the O
log- O
likelihood O
of O
the O
span-based O
gold O
label O
sequences O
(Huang O
et O
al., O
2015). O
4 O
Experiments O
In O
this O
Section, O
we O
describe O
our O
experimental O
setup O
(Section O
4.1), O
the O
datasets O
we O
use O
to O
train O
and O
eval- O
uate O
our O
idiom O
identiﬁcation O
system O
(Section O
4.2), O
and O
the O
results O
obtained O
(Section O
4.3). O
span, O
in O
this O
case O
an O
idiomatic O
expression, O
the O
I O
label O
denotes O
an O
intermediate O
token O
of O
a O
span, O
and O
O O
means O
out O
of O
a O
span.Token O
Label O
After O
O O
some O
O O
reﬂection O
O O
, O
O O
he O
O O
decided O
O O
to O
O O
bite O
B-IDIOM O
the O
I-IDIOM O
bullet O
I-IDIOM O
. O
O O
Table O
2: O
Example O
of O
instance O
labeled O
according O
to O
the O
BIO O
tagging O
scheme. O
4.1 O
Experimental O
Setup O
We O
implement O
our O
idiom B-TaskName
identiﬁcation I-TaskName
system O
(Sec- O
tion O
3.3) O
and O
our O
dual-encoder B-MethodName
discriminator I-MethodName
(Sec- O
tion O
3.1) O
with O
PyTorch O
(Paszke O
et O
al., O
2019), O
using O
the O
Transformers O
library O
(Wolf O
et O
al., O
2019) O
to O
load O
the O
weights O
of O
BERT-base-multilingual-cased B-MethodName
(mBERT). I-MethodName
We O
ﬁne-tune O
our O
idiom O
identiﬁcation O
system O
for O
30 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
Cross-Entropy O
loss O
cri- O
terion, O
adopting O
an O
early O
stopping O
strategy O
with O
a O
patience B-HyperparameterName
value I-HyperparameterName
of O
5, B-HyperparameterValue
Adam O
(Kingma O
and O
Ba, O
2015) O
optimizer O
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
10−5, B-HyperparameterValue
as O
standard O
when O
ﬁne-tuning O
the O
weights O
of O
a O
pretrained O
lan- O
guage O
model. O
For O
our O
dual-encoder O
discriminator, O
instead, O
we O
use O
mBERT B-MethodName
as O
feature O
extractor O
since O
no O
training O
data O
for O
the O
task O
were O
available.2719Hyperparameter O
name O
Value O
number O
of O
Bi-LSTM O
layers O
2 O
LSTM O
hidden O
size O
256 O
gradient O
accumulation O
steps O
4 O
batch O
size O
32 O
learning O
rate O
0.00001 O
dropout O
0.5 O
gradient O
clipping O
1.0 O
adamβ1 O
0.9 O
adamβ2 O
0.999 O
adam/epsilon1 O
1e-8 O
Table O
3: O
Hyperparameter O
values O
of O
the O
reference O
idiom O
identiﬁcation O
system O
used O
for O
our O
experiments. O
The O
entire O
model O
training O
is O
carried O
out O
on O
a O
NVIDIA O
GeForce O
RTX O
3090. O
Each O
training O
(i.e. O
for O
each O
language) O
requires O
∼8min/epoch O
on O
aver- O
age, O
for O
a O
mean O
of∼20epochs. B-HyperparameterValue
Table O
3 O
shows O
the O
full O
list O
of O
hyperparameters. O
4.2 O
Training, O
Validation O
and O
Test O
Data O
The O
training O
and O
validation O
sets O
that O
we O
use O
in O
our O
experiments O
are O
those O
obtained O
by O
applying O
the O
methodology O
described O
in O
Section O
3.1, O
with O
δ= B-HyperparameterName
0.49. B-HyperparameterValue
Although O
we O
automatically O
produce O
training O
data O
in O
10 O
languages, O
we O
report O
results O
only O
on O
the O
4 O
languages O
for O
which O
manually-curated O
test O
sets O
are O
available O
(see O
Section O
3.2). O
However, O
since O
the O
training O
data O
has O
been O
created O
with O
the O
same O
pro- O
cedure O
for O
each O
of O
the O
10 O
languages, O
similar O
results O
are O
expected O
on O
non-tested O
languages. O
Statistics O
are O
provided O
in O
Table O
1. O
9We O
use O
the O
English O
validation O
set O
to O
manu- O
ally O
search O
for O
the O
best O
value O
of O
δby O
choosing O
from O
the O
following O
set O
of O
possible O
values: O
δ B-HyperparameterName
= O
{0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7} B-HyperparameterValue
Figure O
2: O
Confusion O
matrix O
of O
the O
predictions O
of O
our O
automatic O
system O
(X-axis) O
compared O
to O
the O
correspond- O
ing O
ground O
truth O
values O
(Y-axis). O
Results O
are O
averaged O
over O
the O
4 O
languages O
covered O
by O
the O
test O
sets. O
4.3 O
Results O
In O
what O
follows, O
we O
measure O
both O
the O
quality O
of O
our O
automatic O
annotation O
methodology O
(Section O
3.1) O
and O
of O
our O
idiom B-TaskName
identiﬁcation I-TaskName
system O
(Section O
3.3) O
by O
means O
of O
accuracy B-MetricName
and O
token-level O
macro B-MetricName
F1-score I-MetricName
metrics, O
respectively. O
In O
the O
latter O
case, O
we O
rely O
on O
the O
macro-F B-MetricName
1metric I-MetricName
due O
to O
the O
high O
class O
imbalance O
in O
the O
datasets, O
i.e. O
the O
number O
of O
O O
tags O
is O
much O
higher O
than O
the O
sum O
of O
the O
number O
of O
B O
and O
I O
tags, O
see O
Table O
1. O
Silver-Data O
Quality O
Evaluation O
We O
ﬁrst O
aim O
at O
providing O
an O
empirical O
evaluation O
of O
the O
effective- O
ness O
of O
the O
proposed O
automatic O
strategy O
for O
pro- O
ducing O
idiom-related10sentences. O
To O
do O
so, O
for O
each O
language, O
we O
apply O
our O
dual-encoder B-MethodName
discrim- I-MethodName
inator I-MethodName
Φ(e,c)to I-MethodName
the O
expression-context O
pairs O
/angbracketlefte,c/angbracketright O
available O
in O
our O
manually-curated O
test O
set, O
and O
we O
measure O
the O
accuracy O
score O
by O
comparing O
the O
pre- O
dictions O
produced O
by O
the O
system O
with O
the O
human O
annotations O
in O
the O
gold-standard O
test O
sets. O
The O
ac- B-MetricName
curacy I-MetricName
results O
obtained O
are O
reported O
in O
Table O
4. O
With O
this O
being O
a O
binary-classiﬁcation O
task, O
we O
can O
observe O
that O
the O
performance O
achieved O
by O
our O
dual O
encoder O
is O
much O
higher O
than O
the O
50% O
base- O
line O
of O
a O
random O
classiﬁer, O
hence O
implying O
that O
the O
system O
is O
able O
to O
distinguish O
between O
idiomatic O
and O
literal O
usages O
of O
PIEs O
based O
on O
the O
surrounding O
contexts. O
However, O
the O
accuracy B-MetricName
is O
not O
sufﬁcient O
for O
us O
to O
determine O
the O
strengths O
and O
the O
weaknesses O
of O
our O
system. O
Therefore, O
we O
group O
both O
the O
predic- O
10With O
the O
term O
“idiom-related O
sentences" O
we O
refer O
to O
sen- O
tences O
containing O
potentially O
idiomatic O
expressions. O
tions O
and O
the O
labels O
coming O
from O
the O
4 O
languages, O
and O
construct O
a O
confusion O
matrix O
in O
order O
to O
better O
analyze O
the O
system O
behavior. O
From O
the O
confusion O
matrix O
in O
Figure O
2, O
we O
can O
observe O
that O
the O
sys- O
tem O
is O
able O
to O
(almost O
always) O
identify B-TaskName
idiomatic I-TaskName
expressions I-TaskName
as O
such, O
mainly O
thanks O
to O
their O
seman- O
tic O
distance O
from O
the O
meaning O
of O
the O
surrounding O
words. O
On O
the O
other O
hand, O
when O
dealing O
with O
literal O
expressions, O
the O
system O
again O
correctly O
predicts O
the O
majority O
of O
these, O
but O
it O
makes O
more O
errors. O
We O
attribute O
this O
to O
the O
fact O
that O
the O
context O
is O
often O
not O
sufﬁciently O
rich O
to O
ﬁnd O
a O
strong O
similarity O
(i.e. O
higher O
than O
the O
threshold O
δ) B-HyperparameterName
with O
the O
meaning O
of O
the O
individual O
constituents O
of O
the O
idiomatic O
expression, O
and O
hence O
to O
label O
the O
expression O
as O
literal. O
Indeed, O
the O
lower O
the O
value O
of O
δ, B-HyperparameterName
the O
higher O
the O
number O
of O
literal O
expressions O
discovered, O
but O
the O
system O
inevitably O
classiﬁes O
more O
idiomatic O
expressions O
as O
literal. O
Multilingual O
Idiomatic O
Expression O
Identiﬁca- O
tion O
In O
the O
previous O
paragraph O
we O
evaluated O
the O
performance O
of O
our O
dual-encoder B-MethodName
architecture O
on O
the O
binary O
literal O
or O
idiomatic B-TaskName
classiﬁcation I-TaskName
task, O
where O
the O
PIE O
was O
pre-identiﬁed. O
In O
this O
paragraph, O
instead, O
we O
use O
the O
reﬁned O
silver-data O
produced O
by O
the O
aforementioned O
dual O
encoder, O
and O
measure O
the O
identiﬁcation O
capabilities O
of O
our O
idiom O
identi- O
ﬁcation O
system O
on O
the O
sequence-labeling O
task O
we O
introduced O
(Section O
3.3) O
by O
comparing O
the O
BIO O
tags O
produced O
with O
the O
corresponding O
gold O
labels. O
The O
results O
obtained O
are O
reported O
in O
Table O
5 O
(further O
results O
are O
provided O
in O
Appendix O
A). O
The O
ﬁrst O
thing O
that O
catches O
the O
eye O
is O
that O
the O
performances O
on O
the O
O O
tags O
are O
much O
higher O
than O
those O
on O
the O
B O
and O
I O
tags, O
on O
all O
tested O
languages. O
However, O
this O
is O
not O
surprising, O
owing O
to O
the O
fact O
that O
there O
is O
a O
high O
class O
imbalance. O
An O
interest- O
ing O
result, O
instead, O
is O
that O
the O
system O
achieves O
an O
average O
score O
of O
about O
76 B-MetricValue
F1 B-MetricName
points, O
while O
the O
per- O
centage O
of O
seen O
entities11is O
only O
48.7% B-MetricValue
on O
average. O
This O
implies O
that O
the O
system O
is O
able O
to O
generalize, O
and O
consequently O
also O
to O
correctly O
predict O
unseen O
idioms. O
This O
phenomenon O
is O
particularly O
evident O
on O
English O
and O
Spanish, O
where O
the O
percentage O
of O
seen O
idioms O
is O
very O
low. O
To O
better O
highlight O
the O
capability O
of O
the O
system O
to O
go O
beyond O
idioms O
already O
seen O
during O
training, O
we O
also O
analyze O
the O
system O
performance O
on O
the O
"seen" O
and O
"unseen" O
subsets O
independently, O
and O
re- O
port O
the O
results O
in O
Table O
6. O
As O
we O
can O
observe, O
the O
11Seen O
entities O
are O
entities O
in O
the O
test O
set O
which O
have O
already O
been O
encountered O
in O
the O
training O
set. O
system O
is O
able O
to O
correctly O
predict O
the O
majority O
of O
unseen O
idioms O
on O
all O
tested O
languages, O
achieving O
an O
F1 B-MetricName
score O
of O
59.6 B-MetricValue
points, O
on O
average. O
Moreover, O
on O
seen O
idioms, O
the O
system O
behaves O
almost O
per- O
fectly O
reaching O
an O
average O
score O
of O
91.5 B-MetricValue
points. O
We O
underline O
that O
morphological O
variations O
of O
idioms O
encountered O
in O
the O
training O
sets O
are O
considered O
as O
seen O
idioms. O
Table O
1 O
provides O
dimensions O
of O
the O
"seen" O
and O
"unseen" O
subsets, O
for O
each O
language. O
Then, O
to O
further O
demonstrate O
the O
effectiveness O
of O
our O
dual-encoder B-MethodName
architecture O
(Section O
3.1), O
we O
compare O
the O
results O
obtained O
by O
training O
the O
system O
on O
the O
data O
produced O
with O
and O
without O
the O
valida- O
tion O
performed O
by O
our O
dual B-MethodName
encoder. I-MethodName
The O
results O
reported O
in O
Table O
7 O
highlight O
an O
average O
gap O
of O
4.3 B-MetricValue
F1-score B-MetricName
points O
between O
the O
reﬁned O
version O
of O
the O
data O
and O
the O
original O
one, O
showing O
how O
the O
valida- O
tion O
step O
is O
fundamental O
for O
improving O
the O
quality O
of O
the O
annotations, O
consequently O
leading O
the O
system O
to O
a O
better O
understanding O
of O
idioms. O
Finally, O
the O
high O
results O
in O
Table O
5 O
also O
prove O
that, O
thanks O
to O
our O
renewed O
task O
formulation O
(Section O
3.3), O
common O
sequence-labeling O
architectures O
(e.g. O
those O
used O
for O
NER) O
can O
be O
successfully O
imported O
into O
the O
idiom B-TaskName
identiﬁcation I-TaskName
task, O
thus O
enabling O
knowledge O
transfer O
from O
other O
research O
areas. O
5 O
Qualitative O
Analysis O
Together O
with O
the O
quantitative O
evaluation O
provided O
in O
Section O
4.3, O
we O
now O
perform O
a O
qualitative O
analy- O
sis O
of O
our O
system. O
More O
speciﬁcally, O
in O
Table O
8, O
we O
provide O
4 O
examples O
of O
system O
predictions O
(2 O
correctand O
2 O
wrong) O
for O
each O
tested O
language. O
Although O
our O
system O
proves O
to O
be O
robust O
over O
literal O
PIEs O
(see O
Figure O
2), O
its O
most O
common O
mistake O
consists O
in O
classifying O
a O
PIE O
used O
with O
its O
literal O
meaning O
as O
idiomatic. O
This O
is O
mainly O
due O
to O
the O
system O
bias O
towards O
the O
labels O
associated O
to O
such O
PIEs O
during O
training, O
e.g. O
if O
more O
than O
90% O
of O
occurrences O
of O
a O
certain O
PIE O
are O
labeled O
as O
idiomatic O
in O
the O
train- O
ing O
set, O
then O
the O
system O
will O
tend O
to O
classify O
as O
idiomatic O
any O
other O
of O
its O
occurrences O
in O
the O
test O
set. O
This O
result O
suggests O
that O
improvements O
over O
the O
distribution O
of O
labels O
of O
PIEs O
are O
possible. O
In O
Table O
8 O
we O
provide O
an O
example O
of O
one O
such O
wrongly O
la- O
beled O
PIE O
for O
each O
language. O
Another O
commonly O
observed O
error, O
again O
highlighted O
in O
Table O
8, O
is O
that O
in O
which O
unseen O
idiomatic O
expressions O
are O
not O
identiﬁed O
by O
the O
system. O
However, O
as O
previously O
demonstrated O
in O
Table O
6, O
the O
system O
is O
nevertheless O
able O
to O
correctly O
handle O
the O
majority O
of O
such O
cases. O
On O
the O
other O
hand, O
we O
observe O
that O
the O
system O
is O
able O
to O
correctly O
identify O
both O
lemmatized O
and O
inﬂected O
forms O
of O
idiomatic O
expressions, O
for O
both O
seen O
and O
unseen O
ones. O
6 O
Conclusions O
and O
Future O
work O
In O
this O
work, O
we O
introduced O
ID10M, B-MethodName
an O
innovative O
framework O
for O
idiom B-TaskName
identiﬁcation I-TaskName
consisting O
of O
i) O
a O
new O
multilingual O
Transformer-based O
architec- O
ture, O
ii) O
a O
novel O
automatic O
annotation O
pipeline O
for O
creating O
high-quality O
silver-data O
in O
10 O
languages, O
and O
iii) O
a O
challenging O
manually-curated O
benchmark O
in O
4 O
languages. O
Moreover, O
while O
the O
majority O
of2722current O
approaches O
to O
idiom B-TaskName
identiﬁcation I-TaskName
need O
pre- O
identiﬁed O
potentially O
idiomatic O
expressions, O
we, O
instead, O
dropped O
this O
requirement O
and O
proposed O
a O
new O
formulation O
for O
the O
idiom B-TaskName
identiﬁcation I-TaskName
task O
that O
lets O
systems O
be O
directly O
applicable O
to O
raw O
texts. O
Finally, O
our O
experiments O
showed O
that O
our O
system O
is O
able O
to O
generalize O
beyond O
idioms O
seen O
during O
train- O
ing, O
hence O
achieving O
up O
to O
85.4 B-MetricValue
macro B-MetricName
F1-score I-MetricName
on O
the O
idiom B-TaskName
identiﬁcation I-TaskName
task. O
As O
future O
work, O
we O
plan O
to O
scale O
our O
system O
to O
a O
greater O
number O
of O
languages O
and O
textual O
sources, O
but, O
most O
importantly, O
investigate O
the O
beneﬁts O
de- O
rived O
from O
our O
work O
in O
key O
tasks O
such O
as O
Word O
Sense O
Disambiguation, O
Machine O
Translation O
and O
Question O
Answering. O
Acknowledgments O
The O
authors O
gratefully O
acknowledge O
the O
support O
of O
the O
ERC O
Consolida- O
tor O
Grant O
MOUSSE O
No. O
726487 O
under O
the O
European O
Union’s O
Hori- O
zon O
2020 O
research O
and O
innovation O
programme O
and O
the O
support O
of O
the O
ELEXIS O
project O
No. O
731015 O
under O
the O
European O
Union’s O
Horizon O
2020 O
A O
Additional O
Results O
Since O
we O
reformulated O
the O
idiom O
identiﬁcation O
task O
as O
a O
sequence-labeling O
task O
(Section O
3.3), O
all O
pre- O
vious O
approaches O
(that O
required O
pre-identiﬁed O
po- O
tentially O
idiomatic O
expressions) O
cannot O
be O
com- O
pared O
directly. O
Nonetheless, O
in O
order O
to O
select O
a O
robust O
architecture O
for O
the O
idiom O
identiﬁcation O
task, O
we O
compared O
various O
sequence O
tagging O
architec- O
tures. O
Speciﬁcally, O
we O
evaluated O
the O
performance O
of O
several O
alternative O
systems: O
Bidirectional B-MethodName
LSTM I-MethodName
(Bi-LSTM), I-MethodName
Bi-LSTM I-MethodName
+ I-MethodName
CRF, I-MethodName
Multilingual I-MethodName
BERT I-MethodName
(mBERT), I-MethodName
mBERT I-MethodName
+ I-MethodName
Bi-LSTM, I-MethodName
mBERT I-MethodName
+ I-MethodName
Bi- I-MethodName
LSTM I-MethodName
+ I-MethodName
CRF, I-MethodName
XLM-RoBERTa I-MethodName
(XLM-R, I-MethodName
Conneau I-MethodName
et I-MethodName
al., I-MethodName
2020), I-MethodName
XLM-R I-MethodName
+ I-MethodName
Bi-LSTM, I-MethodName
XLM-R I-MethodName
+ I-MethodName
Bi- I-MethodName
LSTM I-MethodName
+ I-MethodName
CRF. I-MethodName
Results O
are O
reported O
in O
Table O
9. O
Sur- O
prisingly, O
mBERT B-MethodName
achieved O
performance O
slightly O
higher O
than O
XLM-R. B-MethodName
Moreover, O
the O
addition O
of O
Bi- B-MethodName
LSTM I-MethodName
and O
CRF B-MethodName
modules O
provided O
further O
improve- O
ments.2726 O