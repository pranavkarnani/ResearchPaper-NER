Proceedings  O
of  O
the  O
18th  O
International  O
Conference  O
on  O
Spoken  O
Language  O
Translation  O
,  O
pages  O
125–130  O
Bangkok,  O
Thailand  O
(Online),  O
August  O
5–6,  O
2021.  O
©2021  O
Association  O
for  O
Computational  O
Linguistics125KIT’s  O
IWSLT  O
2021  O
Ofﬂine  O
Speech  B-TaskName
Translation  I-TaskName
System  O
Tuan-Nam  O
Nguyen,  O
Thai-Son  O
Nguyen,  O
Christian  O
Huber,  O
Maximilian  O
Awiszus,  O
Ngoc-Quan  O
Pham,  O
Thanh-Le  O
Ha,  O
Felix  O
Schneider,  O
Sebastian  O
St  O
¨uker,  O
Alexander  O
Waibel  O
Karlsruhe  O
Institute  O
of  O
Technology  O
firstname.lastname@kit.edu  O
Abstract  O
This  O
paper  O
describes  O
KIT’submission  O
to  O
the  O
IWSLT  O
2021  O
Ofﬂine  O
Speech  B-TaskName
Translation  I-TaskName
Task.  O
We  O
describe  O
a  O
system  O
in  O
both  O
cascaded  O
con-  O
dition  O
and  O
end-to-end  O
condition.  O
In  O
the  O
cas-  O
caded  O
condition,  O
we  O
investigated  O
different  O
end-  O
to-end  O
architectures  O
for  O
the  O
speech  O
recognition  O
module.  O
For  O
the  O
text  O
segmentation  O
module,  O
we  O
trained  O
a  O
small  O
transformer-based  B-MethodName
model  I-MethodName
on  O
high-quality  O
monolingual  O
data.  O
For  O
the  O
trans-  O
lation  O
module,  O
our  O
last  O
year’s  O
neural  B-MethodName
machine  I-MethodName
translation  I-MethodName
model  I-MethodName
was  O
reused.  O
In  O
the  O
end-to-  O
end  O
condition,  O
we  O
improved  O
our  O
Speech  O
Rela-  O
tive  O
Transformer  O
architecture  O
to  O
reach  O
or  O
even  O
surpass  O
the  O
result  O
of  O
the  O
cascade  O
system.  O
1  O
Introduction  O
As  O
in  O
previous  O
years,  O
the  O
cascade  O
system’s  O
pipeline  O
is  O
constituted  O
by  O
an  O
ASR  O
module,  O
a  O
text  O
segmen-  O
tation  O
module  O
and  O
a  O
machine  O
translation  O
module.  O
In  O
this  O
year’s  O
evaluation  O
campaign,  O
we  O
investigated  O
only  O
sequence-to-sequence  O
ASR  O
models  O
with  O
three  O
architectures.  O
The  O
segmentation  O
module  O
is  O
basi-  O
cally  O
a  O
monolingual  O
system  O
which  O
translates  O
a  O
dis-  O
ﬂuent,  O
broken,  O
uncased  O
text  O
(i.e.  O
ASR  O
outputs)  O
into  O
a  O
more  O
ﬂuent,  O
written-style  O
text  O
with  O
punctuations  O
in  O
order  O
to  O
match  O
the  O
data  O
conditions  O
of  O
the  O
trans-  O
lation  O
system.  O
The  O
machine  O
translation  O
module’s  O
architecture  O
is  O
the  O
same  O
as  O
the  O
previous  O
year’s.  O
For  O
the  O
end-to-end  O
system,  O
we  O
improved  O
from  O
our  O
last  O
year’s  O
Speech  O
Relative  O
Transformer  O
architecture  O
(Pham  O
et  O
al.,  O
2020a).  O
As  O
a  O
result,  O
the  O
end-to-end  O
system  O
can  O
produce  O
better  O
results  O
on  O
certain  O
test  O
sets  O
and  O
approach  O
the  O
performance  O
on  O
some  O
others  O
compared  O
to  O
the  O
cascade  O
system  O
this  O
year,  O
while  O
the  O
end-to-end  O
system  O
was  O
the  O
dominant  O
approach  O
last  O
year.  O
The  O
rest  O
of  O
the  O
paper  O
is  O
organized  O
as  O
followed.  O
Section  O
2  O
describes  O
the  O
data  O
set  O
used  O
to  O
train  O
and  O
test  O
the  O
system.  O
It  O
is  O
then  O
followed  O
by  O
Section  O
3  O
providing  O
the  O
description  O
and  O
experimental  O
resultsof  O
both  O
the  O
cascade  O
and  O
the  O
end-to-end  O
system.  O
In  O
the  O
end,  O
we  O
conclude  O
the  O
paper  O
with  O
Section  O
4.  O
2  O
Data  O
Speech  O
Corpora.  O
For  O
training  O
and  O
evaluation  O
of  O
our  O
ASR  O
models,  O
we  O
used  O
Mozilla  B-DatasetName
Common  I-DatasetName
V  I-DatasetName
oice  I-DatasetName
v6.1  I-DatasetName
(Ardila  O
et  O
al.,  O
2019),  O
Europarl  B-DatasetName
(Koehn,  O
2005),  O
How2  B-DatasetName
(Sanabria  O
et  O
al.,  O
2018),  O
Librispeech  B-DatasetName
(Panayotov  O
et  O
al.,  O
2015),  O
MuST-C  B-DatasetName
v1  I-DatasetName
(Di  O
Gangi  O
et  O
al.,  O
2019),  O
MuST-C  B-DatasetName
v2  I-DatasetName
(Cattoni  O
et  O
al.,  O
2021)  O
and  O
Tedlium  B-DatasetName
v3  I-DatasetName
(Hernandez  O
et  O
al.,  O
2018)  O
dataset.  O
The  O
data  O
split  O
is  O
presented  O
in  O
the  O
following  O
table  O
1.  O
Table  O
1:  O
Summary  O
of  O
the  O
English  O
data-sets  O
used  O
for  O
speech  B-TaskName
recognition  I-TaskName
Corpus  O
Utterances  O
Speech  O
data  O
[h]  O
A:  O
Training  O
Data  O
Mozilla  O
Common  B-DatasetName
V  I-DatasetName
oice  I-DatasetName
1225k  O
1667  O
Europarl  B-DatasetName
33k  O
85  O
How2  B-DatasetName
217k  O
356  O
Librispeech  B-DatasetName
281k  O
963  O
MuST-C  B-DatasetName
v1  O
230k  O
407  O
MuST-C  B-DatasetName
v2  I-DatasetName
251k  O
482  O
Tedlium  B-DatasetName
268k  I-DatasetName
482  O
B:  O
Test  O
Data  O
Tedlium  B-DatasetName
1155  O
2.6  O
Librispeech  B-DatasetName
2620  O
5.4  O
Text  O
Corpora.  O
We  O
collected  O
the  O
text  O
parallel  O
training  O
data  O
as  O
presented  O
in  O
Table  O
2.  O
3  O
Ofﬂine  O
Speech  B-TaskName
Translation  I-TaskName
We  O
address  O
the  O
ofﬂine  O
speech  O
translation  O
task  O
by  O
two  O
main  O
approaches,  O
namely  O
cascade  O
and  O
end-to-  O
end.  O
In  O
the  O
cascade  O
condition,  O
the  O
ASR  O
module  O
(Section  O
3.1)  O
receives  O
audio  O
inputs  O
and  O
generates  O
raw  O
transcripts,  O
which  O
will  O
then  O
pass  O
through  O
a  O
Segmentation  O
module  O
(Section  O
3.2)  O
to  O
formulate  O
well  O
normalized  O
inputs  O
to  O
our  O
Machine  O
Translation  O
module  O
(Section  O
3.3).  O
The  O
MT  O
outputs  O
are  O
the  O
ﬁnal  O
outputs  O
of  O
the  O
cascade  O
system.  O
On  O
the  O
other  O
hand,126Table  O
2:  O
Text  O
Training  O
Data  O
Dataset  O
Sentences  O
TED  B-DatasetName
Talks  I-DatasetName
(TED)  I-DatasetName
220K  O
Europarl  B-DatasetName
(EPPS)  O
2.2MK  B-DatasetName
CommonCrawl  I-DatasetName
2.1M  O
Rapid  B-DatasetName
1.21M  O
ParaCrawl  B-DatasetName
25.1M  O
OpenSubtitles  B-DatasetName
12.6M  O
WikiTitle  B-DatasetName
423K  O
Back-translated  B-DatasetName
News  I-DatasetName
26M  O
the  O
end-to-end  O
architecture  O
is  O
trained  O
to  O
directly  O
translate  O
English  O
audio  O
inputs  O
into  O
German  O
text  O
outputs  O
(Section  O
3.4).  O
3.1  O
Speech  O
Recognition  O
Data  O
preparation  O
and  O
Segmentation  O
tool  O
Af-  O
ter  O
collecting  O
all  O
audios  O
from  O
all  O
data  O
sets  O
men-  O
tioned  O
in  O
Section  O
2,  O
we  O
calculated  O
40  O
features  O
of  O
Mel-ﬁlterbank  O
coefﬁcients  O
for  O
ASR  O
training.  O
To  O
generate  O
labels  O
for  O
the  O
sequence-to-sequence  O
ASR  O
models,  O
we  O
used  O
the  O
Sentence-Piece  O
toolkit  O
(Kudo  O
and  O
Richardson,  O
2018)  O
to  O
train  O
4000  O
different  O
byte-  O
pair-encoding  O
(BPE).  O
The  O
WerRTCV  O
AD  O
toolkit  O
(Wiseman,  O
2016)  O
was  O
used  O
to  O
segment  O
the  O
audio  O
in  O
the  O
testing  O
phase.  O
Model  O
As  O
in  O
previous  O
years  O
(Pham  O
et  O
al.,  O
2019a,  O
2020b),  O
we  O
used  O
only  O
sequence-to-sequence  O
ASR  O
models,  O
which  O
are  O
based  O
on  O
three  O
different  O
net-  O
work  O
architectures:  O
The  O
long  O
short-term  O
mem-  O
ory  O
(LSTM),  O
the  O
Transformer  O
and  O
the  O
Conformer.  O
LSTM-based  O
models  O
(Nguyen  O
et  O
al.,  O
2020)  O
consist  O
of  O
6  O
bidirectional  O
layers  O
for  O
the  O
encoder  O
and  O
2  O
uni-  O
directional  O
layers  O
for  O
the  O
decoder,  O
both  O
encoder  O
and  O
decoder  O
layers  O
have  O
1536  O
units.  O
The  O
Transformer-  O
based  O
models  O
presented  O
in  O
(Pham  O
et  O
al.,  O
2019b)  O
have  O
24  O
layers  O
for  O
the  O
encoder  O
and  O
8  O
layers  O
for  O
the  O
decoder.  O
The  O
Conformer-based  O
models  O
(Gulati  O
et  O
al.,  O
2020)  O
comprise  O
16  O
layers  O
for  O
the  O
encoder  O
and  O
6  O
layers  O
for  O
the  O
decoder.  O
In  O
both  O
the  O
Transformer-  O
based  O
and  O
the  O
Conformer-based  O
models,  O
the  O
size  B-HyperparameterName
of  I-HyperparameterName
each  I-HyperparameterName
layer  I-HyperparameterName
is  O
512  B-HyperparameterValue
and  O
the  O
size  B-HyperparameterName
of  I-HyperparameterName
the  I-HyperparameterName
hidden  I-HyperparameterName
state  I-HyperparameterName
in  O
the  O
feed-forward  O
sublayer  O
is  O
2048.  B-HyperparameterValue
The  O
speech  O
data  O
augmentation  O
technique  O
was  O
used  O
to  O
reduce  O
overﬁt-  O
ting  O
as  O
described  O
in  O
(Nguyen  O
et  O
al.,  O
2020).  O
In  O
order  O
to  O
train  O
a  O
deep  O
network  O
effectively,  O
we  O
also  O
applied  O
Stochastic  B-HyperparameterName
Layers  I-HyperparameterName
(Pham  O
et  O
al.,  O
2019b)  O
with  O
a  O
drop-  B-HyperparameterName
ping  I-HyperparameterName
layer  I-HyperparameterName
rate  I-HyperparameterName
of  O
0.5  B-HyperparameterValue
on  O
both  O
Transformer-based  O
and  O
Conformer-based  O
models.3.2  O
Text  O
Segmentation  O
The  O
text  O
segmentation  O
in  O
the  O
cascaded  O
pipeline  O
serves  O
as  O
a  O
normalization  O
on  O
the  O
ASR  O
output,  O
which  O
usually  O
lacks  O
punctuation  O
marks,  O
proper  O
sentence  O
boundaries  O
and  O
reliable  O
casing.  O
On  O
the  O
other  O
hand,  O
the  O
machine  O
translation  O
system  O
is  O
often  O
trained  O
on  O
well-written,  O
high-quality  O
bilingual  O
data.  O
Follow-  O
ing  O
the  O
idea  O
from  O
(Sperber  O
et  O
al.,  O
2018a),  O
we  O
build  O
the  O
segmentation  O
as  O
a  O
monolingual  O
translation  O
sys-  O
tem,  O
which  O
translates  O
from  O
lower-cased,  O
without-  O
punctuation  O
texts  O
into  O
texts  O
with  O
case  O
information  O
and  O
punctuation,  O
prior  O
to  O
the  O
machine  O
translation  O
module.  O
The  O
monolingual  O
translation  O
for  O
text  O
segmenta-  O
tion  O
is  O
implemented  O
using  O
our  O
neural  O
speech  O
transla-  O
tion  O
framework  O
NMTGMinor1(Pham  B-MethodName
et  O
al.,  O
2020a).  O
It  O
is  O
a  O
small  O
transformer  O
architecture,  O
consisting  O
of  O
a  O
4-layer  O
encoder  O
and  O
4-layer  O
decoder,  O
in  O
which  O
each  O
layer’  O
size  O
is  O
512,  O
while  O
the  O
inner  O
size  O
of  O
feed-forward  O
network  O
inside  O
each  O
layer  O
is  O
2048.  O
The  O
encoder  O
and  O
decode  O
are  O
self-attention  O
blocks,  O
which  O
have  O
4  O
parallel  O
attention  O
heads.  O
The  O
training  O
data  O
for  O
that  O
are  O
the  O
English  O
part  O
extracted  O
from  O
available  O
multilingual  O
corpora:  O
EPPS,  B-DatasetName
NC,  B-DatasetName
Global  B-DatasetName
V  I-DatasetName
oices  I-DatasetName
and  O
TED  B-DatasetName
talks.  I-DatasetName
We  O
trained  O
the  O
model  O
for  O
10  O
epochs,  O
then  O
we  O
ﬁne-tuned  O
it  O
on  O
the  O
TED  O
corpus  O
for  O
30  O
epochs  O
more  O
with  O
stronger  O
drop-out  O
rate.  O
Fur-  O
thermore,  O
to  O
simulate  O
possible  O
errors  O
in  O
the  O
ASR  O
outputs,  O
a  O
similar  O
model  O
is  O
trained  O
on  O
artiﬁcial  O
noisy  O
data  O
and  O
the  O
ﬁnal  O
model  O
is  O
the  O
ensemble  O
of  O
the  O
two  O
models.  O
The  O
trained  O
model  O
is  O
then  O
utilized  O
to  O
translate  O
the  O
ASR  O
outputs  O
in  O
a  O
shifting  O
window  O
manner  O
and  O
the  O
decisions  O
are  O
drawn  O
by  O
a  O
simple  O
voting  O
mechanism.  O
For  O
more  O
details,  O
please  O
refer  O
to  O
(Sperber  O
et  O
al.,  O
2018a).  O
3.3  O
Machine  O
Translation  O
For  O
the  O
machine  O
translation  O
module,  O
we  O
re-use  O
the  O
English  O
!German  O
machine  O
translation  O
model  O
from  O
our  O
last  O
year’  O
submission  O
to  O
IWSLT  O
(Pham  O
et  O
al.,  O
2020b).  O
More  O
than  O
40  O
millions  O
sentence  O
pairs  O
being  O
extracted  O
from  O
TED,  B-DatasetName
EPPS,  B-DatasetName
NC,  B-DatasetName
CommonCrawl,  B-DatasetName
ParaCrawl,  B-DatasetName
Rapid  B-DatasetName
and  O
OpenSubtitles  B-DatasetName
corpora  O
were  O
used  O
for  O
training  O
the  O
model.  O
In  O
addition,  O
26  O
mil-  O
lions  O
sentence  O
pairs  O
are  O
generated  O
from  O
the  O
back-  O
translation  O
technique  O
by  O
a  O
German  O
!English  O
trans-  O
lation  O
system.  O
A  O
large  O
transformer  O
architecture  O
was  O
trained  O
with  O
Relative  O
Attention.  O
We  O
adapted  O
to  O
the  O
in-domain  O
by  O
ﬁne-tuning  O
on  O
TED  O
talk  O
data  O
with  O
1https://github.com/quanpn90/NMTGMinor127stricter  O
regularizations.  O
The  O
same  O
adapted  O
model  O
was  O
trained  O
on  O
noised  O
data  O
synthesized  O
from  O
the  O
same  O
TED  O
data.  O
The  O
ﬁnal  O
model  O
is  O
the  O
ensemble  O
of  O
the  O
two.  O
3.4  O
End-to-End  O
Model  O
Corpora  O
This  O
year,  O
the  O
training  O
data  O
consists  O
of  O
the  O
second  O
version  O
of  O
the  O
MUST-C  B-DatasetName
cor-  O
pus  O
(Di  O
Gangi  O
et  O
al.,  O
2019),  O
the  O
Europarl  B-DatasetName
cor-  I-DatasetName
pus  I-DatasetName
(Iranzo-S  O
´anchez  O
et  O
al.,  O
2020),  O
the  O
Speech  B-DatasetName
Trans-  I-DatasetName
lation  I-DatasetName
corpus  I-DatasetName
and  O
the  O
CoV  B-DatasetName
oST-2  I-DatasetName
(Wang  O
et  O
al.,  O
2020)  O
corpus  O
provided  O
by  O
the  O
organizer.  O
The  O
speech  O
fea-  O
tures  O
are  O
generated  O
with  O
the  O
in-house  O
Janus  O
Recog-  O
nition  O
Toolkit.  O
The  O
ST  O
dataset  O
is  O
handled  O
with  O
an  O
additional  O
ﬁltering  O
step  O
using  O
an  O
English  O
speech  O
recognizer  O
(trained  O
with  O
the  O
its  O
transcripts  O
with  O
the  O
additional  O
Tedlium-3  B-DatasetName
training  O
data).  O
Following  O
the  O
success  O
of  O
generating  O
synthetic  O
audio  O
utterances,  O
the  O
transcripts  O
in  O
the  O
Tedlium-3  B-DatasetName
corpus  O
are  O
translated  O
into  O
German  O
using  O
the  O
cascade  O
built  O
in  O
the  O
previous  O
year’s  O
submission  O
(Pham  O
et  O
al.,  O
2020b).  O
In  O
brief,  O
the  O
translation  O
process  O
required  O
us  O
to  O
preserve  O
the  O
audio-text  O
alignment  O
from  O
the  O
origi-  O
nal  O
data  O
collection  O
and  O
segmentation  O
process.  O
As  O
a  O
results,  O
we  O
used  O
the  O
Transformer-based  O
punctu-  O
ation  O
inserting  O
system  O
from  O
IWSLT2018  O
(Sperber  O
et  O
al.,  O
2018b)  O
to  O
reconstruct  O
the  O
punctuations  O
for  O
the  O
transcripts  O
followed  O
by  O
the  O
translation  O
process  O
that  O
preserves  O
the  O
same  O
segmentation  O
information.  O
Compared  O
to  O
the  O
human  O
translation  O
from  O
the  O
speech  O
translation  O
datasets,  O
this  O
translation  O
is  O
relative  O
nois-  O
ier  O
and  O
incomplete  O
(due  O
to  O
the  O
segmentations  O
are  O
not  O
necessarily  O
aligned  O
with  O
grammatically  O
correct  O
sentences).  O
The  O
end  O
result  O
of  O
the  O
ﬁltering  O
and  O
synthetic  O
cre-  O
ation  O
process  O
is  O
the  O
complete  O
translation  O
set,  O
as  O
summarised  O
in  O
Table  O
3  O
Table  O
3:  O
Training  O
data  O
for  O
E2E  O
translation  O
models.  O
Data  O
Utterances  O
Total  O
time  O
MuST-C  B-DatasetName
229K  O
408h  O
Europarl  O
32K  O
60h  O
Speech  O
Translation  O
142K  O
160h  O
Tedlium-3  O
268K  O
415h  O
CoV  O
oST  O
288K  O
424h  O
During  O
training,  O
the  O
validation  O
data  O
is  O
the  O
Devel-  O
opment  O
set  O
of  O
the  O
MuST-C  O
corpus.  O
The  O
reason  O
is  O
that  O
the  O
SLT  O
testsets  O
often  O
do  O
not  O
have  O
the  O
aligned  O
audio  O
and  O
translation,  O
while  O
training  O
end-to-end  O
models  O
often  O
rely  O
on  O
perplexity  O
for  O
early  O
stopping.Modeling  O
The  O
main  O
architecture  O
is  O
the  O
deep  O
Transformer  O
(Vaswani  O
et  O
al.,  O
2017)  O
with  O
stochas-  O
tic  O
layers  O
(Pham  O
et  O
al.,  O
2019b).  O
The  O
encoder  O
self  O
attention  O
layer  O
uses  O
Bidirectional  O
relative  O
atten-  O
tion  O
(Pham  O
et  O
al.,  O
2020a)  O
which  O
models  O
the  O
relative  O
distance  O
between  O
one  O
position  O
and  O
other  O
positions  O
in  O
the  O
sequence.  O
This  O
modeling  O
is  O
bidirectional  O
because  O
the  O
distance  O
is  O
distinguished  O
for  O
each  O
direc-  O
tion  O
from  O
the  O
perspective  O
of  O
one  O
particular  O
position.  O
The  O
main  O
models  O
use  O
a  O
“Big”  O
conﬁguration  O
with  O
16  O
encoder  O
layers  O
and  O
6decoder  O
layers,  O
and  O
they  O
are  O
randomly  O
dropped  O
in  O
training  O
according  O
to  O
the  O
lin-  O
ear  O
schedule  O
presented  O
in  O
the  O
original  O
work,  O
where  O
the  O
top  O
layer  O
has  O
the  O
highest  O
dropout  O
rate  O
p=  O
0:5.  O
The  O
model  O
size  O
of  O
each  O
layer  O
is  O
1024  O
and  O
the  O
in-  O
ner  O
size  O
is  O
4096  O
.  O
We  O
experimented  O
with  O
different  O
activation  O
functions  O
including  O
GELU  O
(Hendrycks  O
and  O
Gimpel,  O
2016),  O
SiLU  O
(Elfwing  O
et  O
al.,  O
2018)  O
and  O
the  O
gated  O
variants  O
similar  O
to  O
the  O
gated  O
linear  O
units  O
(Dauphin  O
et  O
al.,  O
2017).  O
Also,  O
each  O
transformer  O
block  O
(encoder  O
and  O
decoder)  O
is  O
equipped  O
with  O
an-  O
other  O
feed-forward  O
neural  O
network  O
in  O
the  O
begin-  O
ning  O
(Lu  O
et  O
al.,  O
2019).  O
Our  O
preliminary  O
experiments  O
showed  O
that  O
GeLU  O
and  O
SiLU  O
provided  O
a  O
slightly  O
better  O
performance  O
than  O
ReLU,  O
and  O
our  O
ﬁnal  O
model  O
is  O
the  O
ensemble  O
of  O
the  O
three  O
conﬁgurations  O
that  O
are  O
identical  O
except  O
the  O
activation  O
functions.  O
First,  O
the  O
encoders  O
are  O
pretrained  O
using  O
the  O
data  O
portions  O
containing  O
English  O
texts  O
to  O
make  O
training  O
SLT  O
stable.  O
With  O
the  O
initialized  O
encoder,  O
the  O
net-  O
works  O
can  O
be  O
trained  O
with  O
an  O
aggressive  O
learning  O
rate  O
with  O
4096  O
warm-up  O
steps.  O
Label-smoothing  O
and  O
dropout  O
rates  O
are  O
set  O
at  O
0:1and0:3respectively  O
for  O
all  O
models.  O
Furthermore,  O
all  O
speech  O
inputs  O
are  O
augmented  O
with  O
spectral  O
augmentation  O
(Park  O
et  O
al.,  O
2019;  O
Bahar  O
et  O
al.,  O
2019).  O
All  O
models  O
are  O
trained  O
for200000  O
steps,  O
each  O
consists  O
of  O
accumulated  O
360000  O
audio  O
frames.  O
Using  O
the  O
model  O
setup  O
like  O
above,  O
we  O
managed  O
to  O
ﬁt  O
a  O
batch  O
size  O
of  O
around  O
16000  O
frames  O
to  O
24GB  O
of  O
GPU  O
memory.  O
Speech  O
segmentation  O
As  O
reﬂected  O
from  O
last  O
year’s  O
experiments,  O
audio  O
segmentation  O
plays  O
an  O
important  O
role  O
in  O
the  O
performance  O
of  O
the  O
whole  O
system,  O
and  O
the  O
end-to-end  O
model  O
unfortunately  O
does  O
not  O
have  O
control  O
of  O
segmentation,  O
as  O
it  O
is  O
a  O
prerequisite  O
before  O
training  O
one.  O
During  O
evaluation,  O
we  O
relied  O
on  O
the  O
WerRTCV  O
AD  O
toolkit  O
(Wiseman,  O
2016)  O
to  O
cut  O
the  O
long  O
audio  O
ﬁles  O
into  O
segments  O
of  O
reasonable  O
length,  O
and  O
the  O
tool  O
is  O
also  O
able  O
to  O
rule  O
out  O
silence  O
and  O
events  O
that  O
do  O
not  O
belong  O
to  O
human  O
speech,  O
such  O
as  O
noise  O
and  O
music.128Overall,  O
we  O
improved  O
the  O
submission  O
from  O
last  O
year  O
(Pham  O
et  O
al.,  O
2020b)  O
using  O
stronger  O
models  O
together  O
with  O
a  O
more  O
accurate  O
segmentation  O
tool.  O
3.5  O
Experimental  O
Results  O
3.5.1  O
Cascade  O
Ofﬂine  O
Speech  O
Translation  O
Speech  O
Recognition.  O
We  O
tested  O
our  O
ASR  O
sys-  O
tems  O
on  O
two  O
datasets,  O
Tedlium  O
and  O
Libri  O
test  O
set.  O
The  O
ensemble  O
of  O
LSTM-based  O
and  O
Conformer-  O
based  O
sequence-to-sequence  O
model  O
provide  O
the  O
best  O
results,  O
which  O
are  O
2.4  O
and  O
3.9  O
WERs  O
respectively  O
for  O
two  O
test  O
set  O
Table  O
4.  O
Table  O
4:  O
WER  O
on  O
Libri  O
and  O
Tedlium  O
sets  O
Data  O
Libri  O
Tedlium  O
We  O
do  O
not  O
train  O
any  O
new  O
machine  O
translation  O
module  O
but  O
re-use  O
last  O
year’s  O
model,  O
thus,  O
we  O
do  O
not  O
conduct  O
experiments  O
and  O
comparisons  O
with  O
different  O
machine  O
translation  O
sys-  O
tems.  O
We  O
submitted  O
one  O
cascased  O
model  O
with  O
our  O
audio  O
segmentation.  O
3.5.2  O
End-to-end  O
Ofﬂine  O
Speech  O
Translation  O
Our  O
models  O
are  O
tested  O
on  O
two  O
different  O
setups.  O
On  O
the  O
one  O
hand,  O
we  O
evaluated  O
the  O
model  O
on  O
the  O
tst-  O
COMMON  O
(2nd  O
version)  O
of  O
the  O
MuST-C  O
corpora.  O
Due  O
to  O
the  O
incompatibility  O
between  O
the  O
models  O
and  O
the  O
audio  O
data  O
that  O
requires  O
resegmentation,  O
we  O
rely  O
on  O
the  O
dev  O
and  O
test  O
sets  O
of  O
MuST-C  O
to  O
evaluate  O
the  O
ability  O
to  O
translate  O
on  O
“ideal”  O
conditions.  O
As  O
mentioned  O
above,  O
our  O
ensemble  O
managed  O
to  O
reach  O
32:4BLEU  O
points  O
on  O
this  O
test  O
set2.  O
On  O
the  O
other  O
hand,  O
we  O
used  O
the  O
testsets  O
from  O
2010  O
to  O
2015  O
to  O
measure  O
the  O
progress  O
from  O
last  O
year  O
in  O
the  O
condition  O
requiring  O
audio  O
segmentation.  O
In  O
this  O
particular  O
comparison  O
as  O
shown  O
in  O
Table  O
5,  O
we  O
showed  O
that  O
using  O
a  O
stronger  O
model  O
together  O
with  O
better  O
voice  O
detection  O
not  O
only  O
improves  O
the  O
SLT  O
results  O
by  O
up  O
to  O
1:9BLEU  O
points  O
(in  O
tst2014  O
)  O
but  O
also  O
outperforms  O
the  O
strong  O
cascade  O
in  O
2  O
differ-  O
ent  O
sets:  O
tst2013  O
andtst2014  O
,  O
in  O
which  O
the  O
differ-  O
ence  O
could  O
be  O
even  O
1BLEU  O
point.  O
There  O
is  O
still  O
a  O
performance  O
gap  O
in  O
the  O
last  O
two  O
tests,  O
however,  O
2Unfortunately  O
the  O
comparison  O
to  O
last  O
year  O
tst-COMMON  O
(30:6is  O
not  O
available  O
due  O
to  O
version  O
mismatch.a  O
strong  O
E2E  O
system  O
can  O
now  O
trade  O
blow  O
with  O
a  O
strongly  O
tuned  O
cascade.  O
The  O
deciding  O
factor,  O
in  O
our  O
opinion,  O
is  O
audio  O
segmentation  O
because  O
this  O
is  O
the  O
sole  O
advantage  O
of  O
the  O
cascade  O
which  O
can  O
recover  O
from  O
badly  O
cut  O
segments3.  O
Table  O
5:  O
ST:  O
Translation  O
performance  O
in  O
BLEU  O
"on  O
IWSLT  O
testsets  O
(re-segmentation  O
required).  O
Progres-  O
sive  O
results  O
from  O
this  O
year  O
and  O
last  O
year  O
end-to-end  O
(E2E)  O
and  O
cascades  O
(CD)  O
are  O
provided.  O
Testset  O
!  O
CD  O
2020  O
E2E  O
2020  O
E2E  O
2021  O
tst2010  O
26.68  O
24.27  O
25.28  O
tst2013  O
28.60  O
28.13  O
29.62  O
tst2014  O
25.64  O
25.46  O
27.32  O
tst2015  O
24.95  O
21.82  O
22.13  O
4  O
Conclusion  O
In  O
this  O
year’s  O
evaluation  O
campaign,  O
the  O
end-to-end  O
model  O
proves  O
to  O
be  O
a  O
very  O
promising  O
approach  O
since  O
it  O
can  O
compete  O
or  O
even  O
transcend  O
the  O
best  O
cascade  O
model  O
in  O
ofﬂine  O
speech  O
translation  O
task.  O
As  O
a  O
note  O
for  O
future  O
work,  O
we  O
would  O
like  O
to  O
investi-  O
gate  O
two-stage  O
speech  O
translation  O
models  O
(Sperber  O
et  O
al.,  O
2019)  O
using  O
transformer  O
architectures  O
and  O
compare  O
them  O
with  O
our  O
recent  O
speech  O
translation  O
end-to-end  O
models.  O
Acknowledgments  O
The  O
work  O
leading  O
to  O
these  O
results  O
has  O
received  O
funding  O
from  O
the  O
European  O
Union  O
under  O
grant  O
agreement  O
n825460  O
and  O
the  O
Federal  O
Ministry  O
of  O
Education  O
and  O
Research  O
(Germany)/DLR  O
Projek-  O
ttr¨ager  O
Bereich  O
Gesundheit  O
under  O
grant  O
agreement  O
n01EF1803B.  O