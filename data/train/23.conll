Proceedings  O
of  O
the  O
19th  O
International  O
Conference  O
on  O
Spoken  O
Language  O
Translation  O
(IWSLT  O
2022)  O
,  O
pages  O
369  O
-  O
378  O
May  O
26-27,  O
2022  O
c  O
2022  O
Association  O
for  O
Computational  O
Linguistics  O
AppTek’s  O
Submission  O
to  O
the  O
IWSLT  O
2022  O
Isometric  O
Spoken  O
Language  O
Translation  O
Task  O
Patrick  O
Wilken  O
AppTek  O
Aachen,  O
Germany  O
pwilken@apptek.comEvgeny  O
Matusov  O
AppTek  O
Aachen,  O
Germany  O
ematusov@apptek.com  O
Abstract  O
To  O
participate  O
in  O
the  O
Isometric  B-TaskName
Spoken  I-TaskName
Lan-  I-TaskName
guage  I-TaskName
Translation  I-TaskName
Task  O
of  O
the  O
IWSLT  O
2022  O
evaluation,  O
constrained  O
condition,  O
AppTek  O
de-  O
veloped  O
neural  B-MethodName
Transformer-based  I-MethodName
systems  I-MethodName
for  O
English-to-German  O
with  O
various  O
mechanisms  O
of  O
length  O
control,  O
ranging  O
from  O
source-side  O
and  O
target-side  O
pseudo-tokens  O
to  O
encoding  O
of  O
re-  O
maining  O
length  O
in  O
characters  O
that  O
replaces  O
po-  O
sitional  O
encoding.  O
We  O
further  O
increased  O
trans-  O
lation  O
length  O
compliance  O
by  O
sentence-level  O
se-  O
lection  O
of  O
length-compliant  O
hypotheses  O
from  O
different  O
system  O
variants,  O
as  O
well  O
as  O
rescor-  O
ing  O
of  O
N-best  O
candidates  O
from  O
a  O
single  O
system.  O
Length-compliant  O
back-translated  O
and  O
forward-  O
translated  O
synthetic  O
data,  O
as  O
well  O
as  O
other  O
par-  O
allel  O
data  O
variants  O
derived  O
from  O
the  O
original  O
MuST-C  B-DatasetName
training  O
corpus  O
were  O
important  O
for  O
a  O
good  O
quality/desired  O
length  O
trade-off.  O
Our  O
experimental  O
results  O
show  O
that  O
length  O
compli-  O
ance  O
levels  O
above  O
90%  B-MethodName
can  O
be  O
reached  O
while  O
minimizing  O
losses  O
in  O
MT  O
quality  O
as  O
measured  O
in  O
BERT  O
and  O
BLEU  B-MetricName
scores.  O
1  O
Introduction  O
In  O
this  O
paper,  O
we  O
describe  O
AppTek’s  O
submission  O
to  O
the  O
IWSLT  O
2022  O
Isometric  B-TaskName
Spoken  I-TaskName
Language  I-TaskName
Translation  I-TaskName
evaluation  O
(Anastasopoulos  O
et  O
al.,  O
2022).  O
Our  O
goal  O
was  O
to  O
create  O
a  O
system  O
that  O
pro-  O
duces  O
translations  O
which  O
are  O
within  O
10%  O
of  O
the  O
source  O
sentence  O
length,  O
but  O
have  O
similar  O
levels  O
of  O
quality  O
as  O
a  O
baseline  O
system  O
translations  O
without  O
length  O
control.  O
AppTek  O
participated  O
in  O
the  O
con-  O
strained  O
condition  O
with  O
an  O
English-to-German  O
neu-  O
ral  O
machine  O
translation  O
(NMT)  O
system  O
that  O
we  O
de-  O
scribe  O
in  O
Section  O
2.  O
The  O
system  O
was  O
extended  O
with  O
5  O
different  O
length  O
control  O
methods,  O
which  O
we  O
explain  O
in  O
detail  O
in  O
Section  O
3.  O
We  O
also  O
cre-  O
ated  O
synthetic  O
data  O
with  O
back-translation,  O
forward-  O
translation,  O
as  O
well  O
as  O
a  O
novel  O
data  O
augmentation  O
method  O
of  O
synonym  O
replacement.  O
All  O
three  O
meth-  O
ods  O
are  O
described  O
in  O
Section  O
4.  O
Our  O
experimental  O
results  O
on  O
the  O
MuST-C  B-DatasetName
tst-COMMON  I-DatasetName
test  O
set  O
andthe  O
official  O
evaluation  O
test  O
set  O
are  O
presented  O
in  O
Sec-  O
tion  O
5,  O
including  O
ablation  O
studies  O
that  O
prove  O
the  O
effectiveness  O
of  O
synthetic  O
data  O
and  O
noisy  O
length  O
en-  O
coding  O
for  O
a  O
better  O
trade-off  O
between  O
length  O
compli-  O
ance  O
and  O
MT  O
quality.  O
We  O
summarize  O
our  O
findings  O
in  O
Section  O
6.  O
2  O
Baseline  O
system  O
2.1  O
Data  O
We  O
follow  O
the  O
constrained  O
condition  O
of  O
the  O
IWSLT  O
Isometric  O
SLT  B-DatasetName
task  B-DatasetName
and  O
use  O
only  O
English-to-  B-DatasetName
German  B-DatasetName
TED-talk  B-DatasetName
data  O
from  O
the  O
MuST-C  B-DatasetName
corpus  I-DatasetName
(Di  O
Gangi  O
et  O
al.,  O
2019).  O
The  O
corpus  O
contains  O
251K  O
sentence  O
pairs  O
with  O
4.7M  O
and  O
4.3M  O
English  O
and  O
German  O
words,  O
respectively.  O
We  O
apply  O
minimal  O
text  O
pre-processing,  O
mainly  O
consisting  O
of  O
normalization  O
of  O
quotes  O
and  O
dashes.  O
2K  O
sentences  O
that  O
have  O
mismatching  O
digits  O
or  O
paren-  O
theses  O
in  O
source  O
and  O
target  O
were  O
filtered  O
out.  O
We  O
use  O
a  O
joint  O
English  O
and  O
German  O
Sentence-  O
Piece  O
model  O
(Kudo  O
and  O
Richardson,  O
2018),  O
trained  O
on  O
the  O
whole  O
corpus  O
using  O
a  O
vocabulary  O
size  O
of  O
20K,  O
to  O
split  O
the  O
data  O
into  O
subwords.  O
2.2  O
Neural  O
NMT  O
model  O
In  O
preliminary  O
experiments  O
we  O
tried  O
several  O
Trans-  O
former  O
model  O
configurations,  O
including  O
base  O
and  O
bigfrom  O
the  O
original  O
paper  O
(Vaswani  O
et  O
al.,  O
2017),  O
a  O
12  O
encoder  O
and  O
decoder  O
layer  O
variant  O
of  O
base,  O
and  O
a  O
"deep"  O
20  O
encoder  O
layer  O
version  O
with  O
halved  O
feed-forward  O
layer  O
dimension  O
in  O
the  O
encoder  O
and  O
only  O
4  O
attention  O
heads.  O
These  O
attempts  O
to  O
optimize  O
the  O
model  O
architecture  O
for  O
the  O
given,  O
rather  O
low  O
re-  O
source  O
task  O
did  O
not  O
yield  O
a  O
better  O
architecture  O
than  O
Transformer  O
big,  O
which  O
we  O
end  O
up  O
using  O
in  O
all  O
our  O
experiments.  O
We  O
however  O
find  O
an  O
increased  O
dropout  B-MethodName
rate  O
of  O
0.3  B-HyperparameterValue
and  O
an  O
increased  O
label  B-HyperparameterName
smoothing  I-HyperparameterName
of  O
0.2  B-HyperparameterValue
to  O
be  O
crucial.  O
We  O
further  O
optimize  O
the  O
model  O
by  O
sharing  O
the  O
parameters  O
of  O
the  O
source  O
and  O
target  O
embeddings  O
as  O
well  O
as  O
the  O
softmax  O
projection  O
matrix.369In  O
all  O
experiments  O
we  O
use  O
two  O
translation  O
factors  O
(García-Martínez  O
et  O
al.,  O
2016)  O
on  O
both  O
the  O
source  O
and  O
target  O
side  O
to  O
represent  O
the  O
casing  O
of  O
the  O
sub-  O
words  O
and  O
the  O
binary  O
decision  O
whether  O
a  O
subword  O
is  O
attached  O
to  O
the  O
previous  O
subword  O
(Wilken  O
and  O
Matusov,  O
2019).  O
This  O
allows  O
for  O
explicit  O
sharing  O
of  O
information  O
between  O
closely  O
related  O
variants  O
of  O
a  O
subword  O
and  O
reduces  O
the  O
model  O
vocabulary  O
size.  O
All  O
models  O
are  O
trained  O
on  O
a  O
single  O
GPU  O
for  O
162  O
to  O
198  O
epochs  O
of  O
100K  O
sentence  O
pairs  O
each  O
in  O
less  O
than  O
two  O
days.  O
We  O
use  O
batches  B-HyperparameterName
of  O
1700  B-HyperparameterValue
subwords  I-HyperparameterValue
and  O
accumulate  O
gradients  O
over  O
8  O
subsequent  O
batches.  O
The  O
global  O
learning  B-HyperparameterName
rate  I-HyperparameterName
of  O
the  O
Adam  B-MethodName
optimizer  B-MethodName
is  O
increased  O
linearly  O
from  O
3×10−5to3×10−4in  B-HyperparameterValue
the  O
first  O
10  O
epochs  O
and  O
then  O
decreased  O
dynamically  O
by  O
factor  O
0.9  O
each  O
time  O
perplexity  O
on  O
the  O
MuST-C  O
dev  O
set  O
increases  O
during  O
4  O
epochs.  O
For  O
decoding  O
we  O
use  O
beam  O
search  O
with  O
a  O
beam  B-HyperparameterName
size  I-HyperparameterName
of  O
12.  B-HyperparameterValue
We  O
train  O
the  O
Transformer  O
models  O
using  O
RETURNN  B-MethodName
(Doetsch  O
et  O
al.,  O
2017;  O
Zeyer  O
et  O
al.,  O
2018),  O
which  O
is  O
a  O
flexible  O
neural  O
network  O
toolkit  O
based  O
on  O
Tensorflow  O
(Abadi  O
et  O
al.,  O
2015).  O
Automa-  O
tion  O
of  O
the  O
data  O
processing,  O
training  O
and  O
evalua-  O
tion  O
pipelines  O
is  O
implemented  O
with  O
Sisyphus  B-MethodName
(Peter  O
et  O
al.,  O
2018).  O
3  O
Length  O
control  O
methods  O
In  O
this  O
work  O
we  O
perform  O
an  O
extensive  O
evaluation  O
of  O
different  O
ways  O
to  O
control  O
the  O
length  O
of  O
the  O
transla-  O
tions  O
generated  O
by  O
the  O
NMT  O
model,  O
all  O
applied  O
to  O
the  O
same  O
baseline  O
Transformer  O
bigmodel.  O
3.1  O
N-best  O
rescoring  O
A  O
simple  O
method  O
to  O
achieve  O
length  O
compliant  O
trans-  O
lation  O
is  O
to  O
generate  O
N-best  O
lists  O
and  O
select  O
trans-  O
lation  O
hypotheses  O
from  O
the  O
lists  O
that  O
adhere  O
to  O
the  O
desired  O
length  O
constraints.  O
Saboo  O
and  O
Baumann  O
(2019)  O
and  O
Lakew  O
et  O
al.  O
(2021)  O
compute  O
a  O
linear  O
combination  O
of  O
the  O
original  O
MT  O
model  O
score  O
and  O
a  O
length-related  O
score  O
to  O
reorder  O
the  O
N-best  O
list.  O
In  O
this  O
work,  O
we  O
simply  O
extract  O
the  O
translation  O
from  O
the  O
N-best  O
list  O
with  O
the  O
best  O
MT  O
score  O
that  O
has  O
a  O
character  O
count  O
within  O
a  O
10%  O
margin  O
of  O
the  O
source  O
character  O
count  O
and  O
fall  O
back  O
to  O
the  O
first  O
best  O
hypoth-  O
esis  O
if  O
there  O
is  O
no  O
such  O
translation.  O
This  O
approach  O
is  O
tailored  O
towards  O
the  O
evaluation  O
condition  O
of  O
the  O
IWSLT  O
Isometric  O
SLT  O
task  O
where  O
length  O
compli-  O
ance  O
within  O
a  O
10%  O
margin  O
is  O
a  O
binary  O
decision  O
and  O
the  O
absolute  O
length  O
difference  O
is  O
not  O
considered.  O
While  O
N-best  O
rescoring  O
has  O
the  O
advantage  O
of  O
be-  O
ing  O
applicable  O
to  O
any  O
NMT  O
model  O
that  O
uses  O
beamsearch,  O
it  O
is  O
outperformed  O
by  O
learned  O
length  O
control  O
methods  O
because  O
in  O
many  O
cases  O
there  O
is  O
no  O
length  O
compliant  O
translation  O
in  O
the  O
N-best  O
list,  O
and  O
also  O
because  O
learned  O
methods  O
are  O
able  O
to  O
shorten  O
the  O
translation  O
in  O
a  O
more  O
semantically  O
meaningful  O
way.  O
However,  O
we  O
use  O
N-best  O
rescoring  O
on  O
top  O
of  O
other  O
methods  O
to  O
further  O
improve  O
length  O
compliance,  O
as  O
done  O
by  O
Lakew  O
et  O
al.  O
(2021).  O
3.2  O
Length  O
class  O
token  O
Lakew  O
et  O
al.  O
(2019)  O
introduce  O
a  O
special  O
token  O
at  O
the  O
start  O
of  O
the  O
source  O
sentence  O
to  O
control  O
translation  O
length.  O
For  O
this,  O
the  O
training  O
data  O
is  O
classified  O
into  O
difference  O
length  O
classes  O
based  O
on  O
the  O
target-to-  O
source  O
ratio  O
measured  O
in  O
number  O
of  O
characters.  O
In  O
this  O
work  O
we  O
use  O
two  O
variants  O
of  O
length  O
classes:  O
1.3  O
length  O
bins  O
representing  O
"too  O
short",  O
"length  O
compliant"  O
and  O
"too  O
long".  O
Length  O
compliant  O
here  O
means  O
the  O
number  O
of  O
characters  O
in  O
source  O
and  O
target  O
differs  O
by  O
less  O
than  O
10%;  O
2.7  O
length  O
bins  O
from  O
"extra  O
short"  O
to  O
"extra  O
long",  O
such  O
that  O
an  O
approximately  O
equal  O
num-  O
ber  O
of  O
training  O
sentence  O
pairs  O
falls  O
into  O
each  O
bin.  O
The  O
first  O
option  O
is  O
focused  O
on  O
isometric  O
MT,  O
i.e.  O
equal  O
source  O
and  O
target  O
length,  O
while  O
the  O
second  O
option  O
offers  O
a  O
more  O
fine-grained  O
length  O
control.  O
In  O
addition,  O
we  O
analyze  O
the  O
difference  O
of  O
adding  O
the  O
token  O
to  O
the  O
source  O
versus  O
the  O
target  O
side.  O
Adding  O
the  O
token  O
on  O
the  O
target  O
side  O
has  O
the  O
advan-  O
tage  O
of  O
offering  O
the  O
option  O
to  O
not  O
enforce  O
a  O
length  O
class  O
at  O
inference  O
time  O
and  O
instead  O
let  O
the  O
model  O
perform  O
an  O
unbiased  O
translation.  O
This  O
is  O
especially  O
important  O
in  O
a  O
commercial  O
setting  O
where  O
costs  O
can  O
be  O
saved  O
by  O
deploying  O
a  O
single  O
model  O
for  O
general  O
and  O
isometric  O
MT.  O
3.2.1  O
Length  O
ROVER  B-MethodName
A  O
system  O
that  O
takes  O
a  O
length  O
class  O
as  O
input  O
can  O
produce  O
multiple  O
different  O
translations  O
of  O
a  O
given  O
source  O
sentence.  O
To  O
maximize  O
the  O
chance  O
for  O
length  O
compliant  O
translations,  O
we  O
produce  O
translations  O
of  O
the  O
whole  O
test  O
set  O
for  O
each  O
of  O
the  O
length  O
bins  O
and  O
then,  O
for  O
each  O
sentence,  O
select  O
the  O
hypothesis  O
which  O
adheres  O
to  O
the  O
length  O
constraint.  O
We  O
refer  O
to  O
this  O
as  O
length  O
ROVER,  B-MethodName
in  O
analogy  O
to  O
the  O
automatic  O
speech  O
recognition  O
system  O
combination  O
technique  O
called  O
ROVER  B-MethodName
(Fiscus,  O
1997).  O
If  O
multiple  O
length  O
bins  O
pro-  O
duce  O
a  O
length  O
compliant  O
translation,  O
precedence  O
is  O
determined  O
by  O
the  O
corpus-level  O
translation  O
quality370scores  O
for  O
the  O
different  O
length  O
bins.  O
If  O
no  O
bin  O
pro-  O
duces  O
a  O
length  O
compliant  O
translation  O
the  O
bin  O
with  O
the  O
best  O
corpus-level  O
translation  O
quality  O
is  O
used  O
as  O
fallback.  O
As  O
we  O
use  O
a  O
target-side  O
length  O
token,  O
we  O
can  O
let  O
the  O
model  O
predict  O
the  O
length  O
token  O
instead  O
of  O
forcing  O
one.  O
This  O
usually  O
leads  O
to  O
the  O
best  O
corpus-  O
level  O
translation  O
quality.  O
We  O
include  O
this  O
freely  O
decoded  O
translation  O
in  O
the  O
length  O
ROVER.  B-MethodName
When  O
applying  O
the  O
length  O
ROVER  O
to  O
the  O
7-bin  O
model,  O
we  O
exclude  O
the  O
bins  O
corresponding  O
to  O
the  O
longest  O
and  O
shortest  O
translations  O
as  O
those  O
rarely  O
lead  O
to  O
length  O
compliant  O
translations  O
but  O
generally  O
to  O
degraded  O
translation  O
quality.  O
The  O
same  O
is  O
true  O
for  O
the  O
"too  O
short"  O
and  O
"too  O
long"  O
bins  O
in  O
the  O
3-  O
bin  O
model,  O
which  O
is  O
why  O
we  O
do  O
not  O
use  O
the  O
length  O
ROVER  O
for  O
this  O
model.  O
3.3  O
Length  O
encoding  O
We  O
adopt  O
length-difference  O
positional  O
encoding  O
(LDPE)  O
from  O
Takase  O
and  O
Okazaki  O
(2019).  O
It  O
re-  O
places  O
the  O
positional  O
encoding  O
in  O
the  O
transformer  O
decoder,  O
which  O
usually  O
encodes  O
the  O
absolute  O
target  O
position,  O
with  O
a  O
version  O
that  O
"counts  O
down"  O
from  O
a  O
desired  O
output  O
length  O
Lforced  O
to  O
zero.  O
At  O
each  O
decoding  O
step  O
the  O
available  O
remaining  O
length  O
is  O
an  O
input  O
to  O
the  O
decoder  O
and  O
thus  O
the  O
model  O
learns  O
to  O
stop  O
at  O
the  O
right  O
position.  O
In  O
training,  O
Lforced  O
is  O
usually  O
set  O
to  O
the  O
reference  O
target  O
length  O
Ltarget  O
,  O
while  O
at  O
inference  O
time  O
it  O
can  O
be  O
set  O
as  O
desired.  O
For  O
isometric  O
MT,  O
setting  O
it  O
to  O
the  O
source  O
length  O
Lforced  O
=Lsource  O
is  O
the  O
natural  O
choice.  O
The  O
original  O
work  O
of  O
Takase  O
and  O
Okazaki  O
(2019)  O
uses  O
a  O
character-level  O
decoder,  O
which  O
means  O
that  O
the  O
number  O
of  O
decoding  O
steps  O
equals  O
the  O
translation  O
length,  O
assuming  O
the  O
latter  O
is  O
measured  O
in  O
number  O
of  O
characters.  O
Using  O
subwords  O
(Sennrich  O
et  O
al.,  O
2016)  O
as  O
the  O
output  O
unit  O
of  O
the  O
decoder  O
is  O
more  O
common  O
in  O
state-of-the-art  O
systems  O
(Akhbardeh  O
et  O
al.,  O
2021).  O
In  O
this  O
case,  O
one  O
can  O
either  O
encode  O
the  O
target  O
length  O
in  O
terms  O
of  O
number  O
of  O
subword  O
tokens  O
(Liu  O
et  O
al.,  O
2020;  O
Niehues,  O
2020;  O
Buet  O
and  O
Yvon,  O
2021),  O
or  O
keep  O
the  O
character-level  O
encoding  O
which  O
however  O
requires  O
subtracting  O
the  O
number  O
of  O
characters  O
in  O
the  O
predicted  O
subword  O
token  O
in  O
each  O
decoding  O
step  O
(Lakew  O
et  O
al.,  O
2019).  O
The  O
former  O
has  O
the  O
disadvantage  O
that  O
the  O
number  O
of  O
subword  O
tokens  O
is  O
a  O
less  O
direct  O
measure  O
of  O
translation  O
length,  O
especially  O
for  O
the  O
case  O
of  O
the  O
IWSLT  B-TaskName
Isometric  I-TaskName
SLT  I-TaskName
task  O
where  O
length  O
compliance  O
is  O
measured  O
in  O
terms  O
of  O
number  O
of  O
characters.  O
The  O
second  O
optionis  O
more  O
exact  O
but  O
arguably  O
a  O
bit  O
more  O
complex  O
to  O
implement.  O
In  O
this  O
work  O
we  O
compare  O
results  O
for  O
both  O
methods.  O
In  O
contrast  O
to  O
(Lakew  O
et  O
al.,  O
2019)  O
we  O
do  O
not  O
combine  O
standard  O
token-level  O
positional  O
encoding  O
and  O
character-level  O
length  O
encoding,  O
instead  O
we  O
only  O
use  O
the  O
latter.  O
3.3.1  O
Length  O
perturbation  O
For  O
both  O
the  O
token-level  O
and  O
character-level  O
ver-  O
sion  O
we  O
add  O
random  O
noise  O
to  O
the  O
encoded  O
trans-  O
lation  O
length  O
Lforced  O
during  O
training  O
(Oka  O
et  O
al.,  O
2020).  O
We  O
find  O
that  O
this  O
is  O
necessary  O
to  O
make  O
the  O
model  O
robust  O
to  O
the  O
mismatch  O
between  O
training,  O
where  O
the  O
target  O
length  O
is  O
taken  O
from  O
a  O
natural  O
trans-  O
lation,  O
and  O
inference,  O
where  O
the  O
enforced  O
target  O
length  O
is  O
a  O
free  O
parameter.  O
Especially  O
in  O
the  O
case  O
of  O
character-length  O
encoding  O
one  O
cannot  O
expect  O
that  O
a  O
high-quality  O
translation  O
with  O
a  O
given  O
exact  O
charac-  O
ter  O
count  O
exists.  O
As  O
opposed  O
to  O
Oka  O
et  O
al.  O
(2020),  O
who  O
add  O
a  O
random  O
integer  O
to  O
the  O
token-level  O
target  O
length  O
sampled  O
from  O
a  O
fixed  O
interval,  O
e.g.  O
[−4,4],  O
we  O
chose  O
a  O
relative  O
+/-10%  O
interval:  O
Lforced∼U(⌊0.9·Ltarget⌉,⌊1.1·Ltarget⌉)(1)  O
Here,  O
U(n,  O
m)denotes  O
the  O
discrete  O
uniform  O
dis-  O
tribution  O
in  O
the  O
interval  O
[n,  O
m],  O
and⌊·⌉denotes  O
rounding  O
to  O
the  O
nearest  O
integer.  O
This  O
is  O
in  O
line  O
with  O
the  O
+/-10%  O
length  O
compliance  O
condition  O
used  O
in  O
the  O
evaluation.  O
The  O
length  O
difference  O
subtracted  O
in  O
each  O
decoder  O
step  O
is  O
left  O
unaltered,  O
which  O
means  O
counting  O
down  O
will  O
stop  O
at  O
a  O
value  O
that  O
in  O
general  O
is  O
different  O
from  O
zero.  O
3.3.2  O
Second-pass  O
length  O
correction  O
Length  O
encoding  O
as  O
described  O
above  O
does  O
not  O
result  O
in  O
a  O
length  O
compliant  O
translation  O
in  O
all  O
cases.  O
The  O
reasons  O
for  O
this  O
are:  O
1.  O
general  O
model  O
imperfec-  O
tions,  O
intensified  O
by  O
the  O
small  O
size  O
of  O
the  O
training  O
data  O
in  O
the  O
constrained  O
track;  O
2.  O
the  O
noise  O
added  O
to  O
the  O
target  O
length  O
in  O
training  O
(although  O
it  O
is  O
within  O
the  O
"allowed"  O
10%  O
range);  O
3.  O
for  O
the  O
case  O
of  O
token-  O
level  O
length  O
encoding,  O
an  O
equal  O
number  O
of  O
source  O
and  O
target  O
tokens  O
does  O
not  O
necessarily  O
mean  O
an  O
equal  O
number  O
of  O
characters.  O
We  O
therefore  O
perform  O
a  O
second  O
decoding  O
pass  O
for  O
those  O
sentences  O
where  O
the  O
first  O
pass  O
does  O
not  O
gener-  O
ate  O
a  O
length  O
compliant  O
translation.  O
In  O
this  O
second  O
pass,  O
instead  O
of  O
attempting  O
to  O
enforce  O
Lforced  O
=  O
Lsource  O
,  O
we  O
make  O
a  O
correction  O
by  O
multiplying  O
by  O
the  O
source-to-target  O
ratio  O
observed  O
in  O
the  O
first  O
pass371(measured  O
in  O
tokens  O
or  O
characters,  O
depending  O
on  O
the  O
unit  O
used  O
for  O
length  O
encoding):  O
L2-pass  O
forced=/floorleftigg  O
Lsource·Lsource  O
L1-pass  O
target/ceilingrightigg  O
(2)  O
L1-pass  O
target  O
is  O
the  O
first  O
pass  O
translation  O
length,  O
⌊·⌉de-  O
notes  O
rounding.  O
That  O
way,  O
an  O
over-translation  O
of  O
factor  O
rin  O
the  O
first  O
pass  O
will  O
be  O
counteracted  O
by  O
"aiming"  O
at  O
a  O
translation  O
length  O
of  O
1/rof  O
the  O
source  O
length  O
in  O
the  O
second  O
pass.  O
This  O
procedure  O
could  O
be  O
applied  O
iteratively,  O
one  O
could  O
even  O
run  O
a  O
grid  O
search  O
of  O
many  O
different  O
val-  O
ues  O
for  O
Lforced  O
until  O
a  O
length  O
compliant  O
translation  O
is  O
generated.  O
We  O
refrain  O
from  O
doing  O
so  O
as  O
we  O
find  O
it  O
to  O
be  O
impracticable  O
in  O
real-world  O
applications.  O
4  O
Synthetic  O
data  O
We  O
expand  O
the  O
original  O
MuST-C  B-DatasetName
data  O
with  O
synthetic  O
data  O
of  O
different  O
types,  O
all  O
derived  O
from  O
the  O
given  O
MuST-C  B-DatasetName
corpus.  O
First,  O
we  O
include  O
a  O
copy  O
of  O
the  O
data1in  O
which  O
two  O
consecutive  O
sentences  O
from  O
the  O
same  O
TED  O
talk  O
are  O
concatenated  O
into  O
one.  O
Since  O
many  O
segments  O
in  O
the  O
original  O
data  O
are  O
short,  O
this  O
helps  O
to  O
learn  O
more  O
in-context  O
translations.  O
Then,  O
we  O
also  O
include  O
a  O
copy  O
of  O
the  O
data  O
where  O
the  O
English  O
side  O
is  O
pre-  O
processed  O
by  O
lowercasing,  O
removing  O
punctuation  O
marks  O
and  O
replacing  O
digits,  O
monetary  O
amounts  O
and  O
other  O
entities  O
with  O
their  O
spoken  O
forms.  O
This  O
helps  O
to  O
adjust  O
to  O
the  O
spoken  O
style  O
of  O
TED  O
talks  O
and  O
im-  O
perfections  O
in  O
the  O
(manual)  O
transcriptions  O
of  O
the  O
training  O
and  O
evaluation  O
data.  O
We  O
also  O
use  O
82K  O
bilingual  O
phrase  O
pairs  O
extracted  O
from  O
word-aligned  O
MuST-C  B-DatasetName
data,  O
as  O
described  O
be-  O
low,  O
as  O
training  O
instances.  O
4.1  O
Word  O
synonym  O
replacement  O
To  O
enrich  O
the  O
training  O
data  O
with  O
more  O
examples  O
of  O
length-compliant  O
translations,  O
we  O
experiment  O
with  O
a  O
novel  O
technique  O
of  O
replacing  O
a  O
few  O
randomly  O
se-  O
lected  O
source  O
(English)  O
words  O
in  O
a  O
given  O
sentence  O
pair  O
with  O
their  O
synonyms  O
which  O
are  O
shorter/longer  O
in  O
the  O
number  O
of  O
characters,  O
so  O
that  O
the  O
result-  O
ing  O
modified  O
synthetic  O
sentence  O
is  O
closer  O
to  O
being  O
length  O
compliant.  O
Whereas  O
in  O
an  O
unconstrained  O
conditions  O
the  O
synonyms  O
can  O
come  O
from  O
WordNet  B-DatasetName
or  O
other  O
sources,  O
in  O
the  O
constrained  O
track  O
we  O
rely  O
on  O
synonyms  O
extracted  O
from  O
a  O
bilingual  O
lexicon.  O
The  O
1Including,  O
if  O
applicable,  O
the  O
synthetic  O
data  O
described  O
be-  O
low.replacement  O
of  O
a  O
source  O
word  O
with  O
a  O
synonym  O
in  O
a  O
given  O
sentence  O
pair  O
happens  O
only  O
if  O
it  O
is  O
aligned  O
to  O
a  O
target  O
word,  O
for  O
which  O
another  O
word  O
translation  O
exists  O
in  O
the  O
bilingual  O
lexicon.  O
The  O
word  O
alignment  O
and  O
bilingual  O
word  O
lexicon  O
extraction  O
is  O
performed  O
on  O
the  O
lowercased  O
MuST-C  B-DatasetName
corpus  O
itself  O
using  O
FastAlign  O
(Dyer  O
et  O
al.,  O
2013).  O
The  O
bilingual  O
lexicon  O
is  O
filtered  O
to  O
contain  O
entries  O
with  O
the  O
costs  O
(negative  B-HyperparameterName
log  I-HyperparameterName
of  I-HyperparameterName
the  I-HyperparameterName
word-level  I-HyperparameterName
trans-  I-HyperparameterName
lation  I-HyperparameterName
probability)  I-HyperparameterName
of  O
50  B-HyperparameterValue
or  O
lower.  O
We  O
apply  O
the  O
synonym  O
replacements  O
only  O
to  O
sen-  O
tence  O
pairs  O
for  O
which  O
the  O
target  O
sentence  O
is  O
not  O
length-compliant  O
with  O
the  O
source.  O
We  O
first  O
generate  O
multiple  O
versions  O
of  O
modified  O
source  O
sentences  O
for  O
these  O
data,  O
which  O
all  O
differ  O
in  O
the  O
choice  O
of  O
ran-  O
domly  O
selected  O
words  O
that  O
are  O
to  O
be  O
replaced  O
with  O
synonyms  O
and  O
in  O
the  O
actual  O
synonyms  O
selected  O
for  O
replacement  O
(also  O
at  O
random).  O
Each  O
word  O
in  O
a  O
sen-  O
tence  O
has  O
a  O
0.5  O
chance  O
of  O
being  O
considered  O
for  O
re-  O
placement  O
(regardless  O
of  O
whether  O
it  O
has  O
synonyms  O
as  O
defined  O
above  O
or  O
not),  O
and  O
the  O
replacement  O
is  O
done  O
with  O
(at  O
most)  O
one  O
of  O
3  O
synonym  O
candidates  O
with  O
the  O
highest  O
lexicon  O
probability  O
which  O
have  O
fewer  O
or  O
more  O
characters  O
than  O
the  O
word  O
being  O
re-  O
placed,  O
depending  O
on  O
whether  O
the  O
length  O
of  O
the  O
original  O
sentence  O
was  O
too  O
long  O
or  O
too  O
short.  O
From  O
the  O
resulting  O
data  O
(ca.  O
1M  O
sentences),  O
we  O
keep  O
only  O
those  O
modified  O
source  O
sentences  O
for  O
which  O
the  O
BERT  O
F1  B-MetricName
score  I-MetricName
(Zhang  O
et  O
al.,  O
2020)  O
with  O
respect  O
to  O
the  O
original  O
(unmodified)  O
source  O
sentence  O
is  O
0.94  B-MetricValue
or  O
higher.  O
In  O
this  O
way  O
we  O
try  O
to  O
make  O
sure  O
that  O
the  O
meaning  O
of  O
the  O
modified  O
source  O
sentence  O
stays  O
very  O
close  O
to  O
the  O
original  O
meaning.  O
This  O
way,  O
only  O
192K  O
sentences  O
are  O
kept,  O
which  O
are  O
then  O
paired  O
with  O
the  O
original  O
target  O
(German)  O
sen-  O
tences  O
to  O
form  O
a  O
synthetic  O
synonym  O
replacement  O
parallel  O
corpus.  O
4.2  O
Back-translated  O
data  O
We  O
train  O
the  O
reverse,  O
German-to-English  O
system  O
with  O
7  O
length  O
bins  O
and  O
source  O
length  O
token  O
as  O
de-  O
scribed  O
in  O
Section  O
3  O
using  O
the  O
same  O
architecture  O
and  O
settings  O
as  O
for  O
the  O
English-to-German  O
system.  O
We  O
then  O
use  O
this  O
system  O
to  O
translate  O
the  O
MuST-C  B-DatasetName
corpus  O
from  O
German  O
to  O
English,  O
generating  O
7  O
translations  O
of  O
each  O
sentence  O
for  O
each  O
of  O
the  O
7  O
bins.  O
From  O
these  O
data,  O
we  O
keep  O
all  O
back-translations  O
which  O
make  O
the  O
corresponding  O
German  O
sentence  O
length-compliant.  O
This  O
resulted  O
in  O
a  O
back-translated  O
corpus  O
of  O
172K  O
sentence  O
English  O
→German  O
translation  O
results  O
for  O
MuST-C  B-DatasetName
tst-COMMON  I-DatasetName
and  O
the  O
IWSLT  O
2022  O
Isomtetric  O
SLT  O
blind  O
test.  O
All  O
values  O
in  O
%.  O
LC  B-HyperparameterName
=  I-HyperparameterName
length  I-HyperparameterName
compliance  I-HyperparameterName
within  O
10%  B-HyperparameterValue
in  O
number  O
of  O
characters.  O
All  O
systems  O
are  O
based  O
on  O
the  O
same  O
Transformer  O
bigmodel.  O
Length  O
bins  O
of  O
the  O
7-bin  O
system  O
are  O
referred  O
to  O
as  O
XXS,  O
XS,  O
S,  O
M,  O
L,  O
XL  O
and  O
XXL  O
from  O
short  O
to  O
long.  O
For  O
explanation  O
of  O
N-best  O
rescoring,  O
ROVER,  O
and  O
2-pass  O
length  O
correction  O
refer  O
to  O
Section  O
3.  O
4.3  O
Forward-translated  O
data  O
In  O
addition  O
to  O
back-translated  O
data,  O
we  O
also  O
aug-  O
mented  O
our  O
training  O
corpus  O
with  O
forward-translated  O
data.  O
For  O
this,  O
we  O
generated  O
translations  O
using  O
our  O
English-to-German  O
system  O
with  O
7  O
length  O
bins  O
and  O
a  O
source  O
length  O
token  O
for  O
each  O
of  O
the  O
length  O
classes.  O
Then,  O
we  O
kept  O
only  O
those  O
translations  O
which  O
turned  O
out  O
to  O
be  O
length-compliant  O
with  O
the  O
corresponding  O
source  O
sentence.  O
The  O
resulting  O
synthetic  O
corpus  O
has  O
213K  O
sentence  O
pairs.  O
5  O
Experimental  O
results  O
Table  O
1  O
presents  O
results  O
for  O
all  O
length  O
control  O
meth-  O
ods  O
explored  O
in  O
this  O
work.  O
We  O
evaluate  O
on  O
MuST-C  B-DatasetName
tst-COMMON  I-DatasetName
v22and  O
the  O
blind  O
test  O
set  O
provided  O
by  O
the  O
shared  O
task  O
organizers  O
using  O
the  O
official  O
scor-  O
ing  O
script3.  O
As  O
a  O
measure  O
of  O
MT  O
quality  O
it  O
com-  O
putes  O
BLEU  B-MetricName
(Papineni  O
et  O
al.,  O
2002;  O
Post,  O
2018)  O
and  O
BERT  O
F1  B-MetricName
score  I-MetricName
(Zhang  O
et  O
al.,  O
2020).  O
Length  O
compliance  O
(LC)  O
is  O
calculated  O
as  O
the  O
proportion  O
2The  O
official  O
evaluation  O
uses  O
tst-COMMON  O
v1.  O
Differ-  O
ences  O
in  O
metric  O
scores  O
are  O
minor  O
though.  O
3Blind  O
test  O
set  O
and  O
scoring  O
script  O
are  O
published  O
un-  O
der  O
https://github.com/amazon-research/  O
isometric-slt  O
.of  O
translations  O
that  O
have  O
a  O
character  O
count  O
which  O
differs  O
by  O
10%  O
or  O
less  O
from  O
the  O
number  O
of  O
charac-  O
ters  O
in  O
the  O
source  O
sentence.  O
For  O
this,  O
spaces  O
are  O
not  O
counted  O
and  O
sentences  O
with  O
less  O
than  O
10  O
characters  O
are  O
ignored.  O
References  O
for  O
the  O
blind  O
test  O
set  O
were  O
made  O
available  O
only  O
after  O
development  O
of  O
the  O
sys-  O
tems.  O
Line  O
0  O
in  O
Table  O
1  O
corresponds  O
to  O
a  O
system  O
trained  O
without  O
any  O
of  O
the  O
length  O
control  O
methods  O
from  O
Section  O
3.  O
All  O
systems  O
use  O
all  O
synthetic  O
data  O
as  O
described  O
in  O
Section  O
4  O
if  O
not  O
stated  O
otherwise.  O
5.1  O
Length  O
token  O
systems  O
Rows  O
1  O
to  O
4  O
of  O
Table  O
1  O
show  O
results  O
for  O
the  O
3-bin  O
length  O
token  O
systems.  O
The  O
"length  O
compliant"  O
bin  O
is  O
used  O
for  O
all  O
translations.  O
(When  O
used  O
on  O
the  O
tar-  O
get  O
side  O
it  O
is  O
enforced  O
as  O
the  O
first  O
decoding  O
step.)  O
Overall,  O
we  O
observe  O
no  O
major  O
differences  O
between  O
a  O
source-side  O
and  O
target-side  O
length  O
token  O
in  O
both  O
LC  O
and  O
MT  O
quality  O
scores.  O
Synthetic  O
data  O
and  O
selection  O
of  O
the  O
length  O
bin  O
alone  O
leads  O
to  O
length  O
compliant  O
translations  O
in  O
about  O
50%  O
of  O
cases  O
(rows  O
1  O
and  O
3).  O
This  O
shows  O
that  O
the  O
model  O
has  O
to  O
com-  O
promise  O
between  O
translation  O
quality  O
and  O
length  O
and  O
that  O
a  O
length  O
token  O
is  O
not  O
a  O
strong  O
enough  O
signal  O
to  O
enforce  O
the  O
corresponding  O
length  O
class  O
in  O
all  O
cases.373N-best  O
rescoring,  O
i.e.  O
selection  O
of  O
a  O
length  O
compli-  O
ant  O
translation  O
from  O
the  O
beam  B-HyperparameterName
search  I-HyperparameterName
output  I-HyperparameterName
of  O
size  O
12,  B-HyperparameterValue
can  O
improve  O
LC  O
to  O
78%  O
on  O
tst-COMMON  O
but  O
comes  O
at  O
the  O
cost  O
of  O
a  O
loss  O
in  O
translation  O
quality  O
by  O
0.8%  O
BLEU  O
and  O
0.3%  O
BERTScore  B-MetricName
absolute.  O
The  O
7-bin  O
system  O
shown  O
in  O
rows  O
5  O
to  O
16  O
offers  O
a  O
greater  O
variety  O
of  O
trade-off  O
points.  O
We  O
refer  O
to  O
the  O
7  O
length  O
bins  O
with  O
size  O
labels  O
from  O
"XXS"  O
to  O
"XXL".  O
The  O
target-to-source  O
ratio  O
boundaries  O
for  O
equally  O
sized  O
bins  O
in  O
terms  O
of  O
training  O
examples  O
are  O
computed  O
to  O
be  O
0.90,  O
0.98,  O
1.02,  O
1.06,  O
1.10,  O
and  O
1.23.  O
This  O
means  O
the  O
desired  O
1.0  O
ratio  O
for  O
isometric  O
MT  O
falls  O
into  O
the  O
"S"  O
bin.  O
Row  O
5  O
shows  O
the  O
scores  O
achieved  O
when  O
not  O
forc-  O
ing  O
any  O
length  O
token.  O
This  O
configuration  O
leads  O
to  O
the  O
same  O
quality  O
on  O
tst-COMMON  O
as  O
the  O
base-  O
line  O
system,  O
namely  O
32.0%  B-MetricValue
BLEU  B-MetricName
and  O
84.0%  B-MetricValue
BERTScore.  B-MetricName
This  O
indicates  O
that  O
the  O
model  O
is  O
able  O
to  O
predict  O
the  O
right  O
length  O
class  O
corresponding  O
to  O
an  O
unbiased  O
translation.  O
Setting  O
the  O
length  O
token  O
to  O
either  O
"M",  O
"S"  O
or  O
"XS"  O
offers  O
different  O
trade-offs  O
between  O
translation  O
quality  O
and  O
length  O
compliance.  O
Interestingly,  O
the  O
"XS"  O
class  O
has  O
a  O
higher  O
LC  O
than  O
the  O
class  O
"S"  O
which  O
should  O
represent  O
translations  O
with  O
a  O
target-to-source  O
ratio  O
closer  O
to  O
1.  O
Again,  O
this  O
shows  O
that  O
the  O
effect  O
of  O
length  O
tokens  O
is  O
in  O
conflict  O
with  O
general  O
translation  O
quality,  O
which  O
is  O
optimal  O
when  O
not  O
skipping  O
any  O
information  O
present  O
in  O
the  O
source.  O
A  O
more  O
extreme  O
length  O
class  O
has  O
to  O
be  O
cho-  O
sen  O
to  O
achieve  O
the  O
desired  O
amount  O
of  O
compression.  O
In  O
all  O
cases  O
N-best  O
rescoring  O
has  O
the  O
same  O
effect  O
as  O
observed  O
for  O
the  O
3-bin  O
systems,  O
namely  O
a  O
higher  O
LC  O
at  O
the  O
cost  O
of  O
worse  O
translation  O
quality.  O
All  O
length  O
classes  O
not  O
shown  O
in  O
the  O
table  O
lead  O
to  O
either  O
clearly  O
worse  O
LC  O
or  O
quality  O
scores.  O
The  O
outputs  O
for  O
different  O
length  O
tokens,  O
possi-  O
bly  O
after  O
N-best  O
rescoring,  O
can  O
be  O
combined  O
with  O
the  O
length  O
ROVER.  O
As  O
mentioned  O
in  O
Section  O
3.2.1,  O
we  O
exclude  O
the  O
extreme  O
length  O
classes.  O
We  O
con-  O
sider  O
two  O
variants:  O
excluding  O
the  O
bins  O
with  O
short-  O
est  O
and  O
longest  O
translations,  O
or  O
excluding  O
the  O
two  O
shortest  O
and  O
longest.  O
As  O
expected,  O
both  O
variants  O
lead  O
to  O
more  O
length  O
compliant  O
translations  O
in  O
the  O
combined  O
output.  O
However,  O
they  O
provide  O
different  O
trade-offs:  O
while  O
the  O
first  O
variant  O
(rows  O
13,  O
14)  O
can  O
achieve  O
94%  O
length  O
compliance  O
on  O
tst-COMMON,  O
translation  O
quality  O
drops  O
to  O
similarly  O
low  O
values  O
as  O
observed  O
for  O
the  O
"XS"  O
length  O
class.  O
The  O
second  O
variant  O
is  O
more  O
conservative  O
and  O
achieves  O
only  O
89%  B-MetricValue
length  B-MetricName
compliance,  I-MetricName
but  O
preserves  O
higher  O
BLEU  O
and  O
BERT  O
show  O
the  O
results  O
of  O
sys-  O
tems  O
trained  O
with  O
length  O
encoding  O
as  O
described  O
in  O
Section  O
3.3.  O
They  O
are  O
also  O
trained  O
using  O
3  O
length  O
bins  O
and  O
a  O
"length  O
compliant"  O
token  O
is  O
forced  O
on  O
the  O
target  O
side,  O
we  O
however  O
observe  O
no  O
significant  O
differences  O
to  O
not  O
using  O
the  O
token.  O
Using  O
the  O
source  O
length  O
as  O
input  O
to  O
the  O
decoder  O
(Lforced  O
=Lsource  O
),  O
the  O
token-level  O
length  O
encod-  O
ing  O
model  O
(row  O
17)  O
does  O
not  O
achieve  O
a  O
higher  O
LC  O
value  O
than  O
the  O
length  O
token  O
systems  O
(49%),  O
while  O
the  O
model  O
with  O
character-level  O
length  O
encoding  O
(row  O
21)  O
is  O
able  O
to  O
produce  O
compliant  O
translations  O
in  O
64%  O
of  O
the  O
cases.  O
Doing  O
a  O
length-corrected  O
second  O
decoding  O
pass  O
is  O
very  O
effective  O
for  O
both  O
sys-  O
tems.  O
This  O
shows  O
that  O
the  O
decoder  O
input  O
Lforced  O
has  O
a  O
strong  O
impact  O
on  O
the  O
model  O
output,  O
however  O
has  O
to  O
be  O
adjusted  O
to  O
get  O
the  O
desired  O
output  O
length.  O
In  O
Section  O
3.3.1  O
we  O
give  O
explanations  O
for  O
such  O
imper-  O
fections.  O
In  O
addition,  O
similar  O
to  O
the  O
case  O
of  O
length  O
tokens,  O
we  O
attribute  O
this  O
to  O
the  O
fact  O
that  O
in  O
training  O
the  O
desired  O
length  O
is  O
always  O
conform  O
with  O
the  O
refer-  O
ence  O
translation,  O
while  O
at  O
inference  O
time  O
the  O
model  O
often  O
has  O
to  O
compress  O
its  O
output  O
to  O
fulfill  O
the  O
length  O
constraints,  O
which  O
might  O
require  O
a  O
more  O
extreme  O
value  O
for  O
the  O
targeted  O
length  O
Lforced  O
.  O
N-best  O
rescoring  O
can  O
be  O
applied  O
on  O
top  O
to  O
achieve  O
a  O
further  O
large  O
increase  O
in  O
length  O
compliance4.  O
This  O
indicates  O
that  O
there  O
is  O
length  O
variety  O
in  O
the  O
N-best  O
list  O
that  O
at  O
least  O
in  O
part  O
can  O
be  O
attributed  O
to  O
the  O
noise  O
added  O
through  O
length  O
perturbation  O
(Section  O
3.3.1).  O
The  O
resulting  O
character-level  O
length  O
encoding  O
sys-  O
tem  O
in  O
row  O
24  O
achieves  O
the  O
overall  O
best  O
length  O
com-  O
pliance  O
value  O
of  O
98.14%.  O
5.3  O
System  O
selection  O
To  O
select  O
systems  O
for  O
our  O
submission,  O
in  O
Figure  O
1  O
we  O
visualize  O
the  O
inherent  O
trade-off  O
between  O
length  O
compliance  O
and  O
translation  O
quality  O
for  O
the  O
systems  O
from  O
Table  O
1.  O
We  O
look  O
at  O
BERT  O
scores  O
as  O
they  O
were  O
announced  O
to  O
be  O
the  O
main  O
MT  O
quality  O
metric  O
for  O
the  O
evaluation.  O
We  O
chose  O
system  O
16,  O
the  O
7-bin  O
length  O
token  O
system  O
using  O
the  O
length  O
ROVER,  O
as  O
our  O
primary  O
submission.  O
As  O
contrastive  O
submis-  O
sions  O
we  O
include  O
systems  O
2  O
(3  O
length  O
bins  O
using  O
source-side  O
token),  O
14  O
(ROVER  B-MethodName
variation  O
of  O
the  O
primary  O
submission)  O
and  O
24  O
(character-level  O
length  O
encoding  O
with  O
second-pass  O
length  O
correction).  O
All  O
submissions  O
use  O
N-best  O
rescoring.  O
As  O
it  O
can  O
be  O
4First-best  O
translation  O
length  O
of  O
first  O
pass  O
is  O
used  O
for  O
length  O
correction,  O
N-best  O
rescoring  O
only  O
applied  O
in  O
the  O
Visualization  O
of  O
length  O
compliance  O
(LC)  O
vs.  O
BERTScore  O
trade-offs  O
on  O
MuST-C  B-DatasetName
tst-COMMON  I-DatasetName
for  O
systems  O
taken  O
from  O
Table  O
1.  O
Data  O
point  O
labels  O
are  O
the  O
row  O
numbers  O
(#)  O
from  O
Table  O
1.  O
Submitted  O
systems  O
are  O
labeled  O
in  O
bold  O
blue.  O
seen,  O
the  O
different  O
length  O
control  O
methods  O
are  O
all  O
able  O
to  O
provide  O
useful  O
trade-off  O
points.  O
While  O
only  O
length  O
encoding  O
can  O
achieve  O
a  O
near  O
perfect  O
length  O
compliance,  O
length  O
token-based  O
methods  O
can  O
of-  O
fer  O
a  O
good  O
compromise  O
that  O
preserves  O
more  O
of  O
the  O
baseline  O
MT  O
performance.  O
5.4  O
Ablation  O
study  O
For  O
a  O
selected  O
subset  O
of  O
the  O
systems  O
we  O
show  O
the  O
contribution  O
of  O
the  O
most  O
important  O
types  O
of  O
syn-  O
thetic  O
data  O
used  O
in  O
our  O
systems  O
(Section  O
4),  O
as  O
well  O
as  O
the  O
effect  O
of  O
length  O
perturbation  O
(Section  O
3.3.1).  O
5.4.1  O
Effect  O
of  O
synthetic  O
data  O
Comparison  O
of  O
the  O
first  O
two  O
rows  O
of  O
Table  O
2  O
shows  O
that  O
taking  O
away  O
synthetic  O
data  O
created  O
using  O
word  O
synonym  O
replacement  O
(Section  O
4.1)  O
from  O
the  O
7-bin  O
length  O
token  O
system  O
causes  O
a  O
slight  O
degradation  O
of  O
the  O
BLEU  O
score  O
and  O
no  O
significant  O
change  O
of  O
BERT  O
and  O
length  O
compliance  O
score  O
on  O
tst-COMMON.  O
We  O
consistently  O
observe  O
the  O
same  O
tendencies  O
when  O
tak-  O
ing  O
other  O
configurations  O
of  O
the  O
7-bin  O
system  O
from  O
Table  O
1  O
as  O
baseline  O
(not  O
shown  O
here).  O
This  O
indicates  O
that  O
synonym  O
replacement  O
has  O
some  O
positive  O
effect  O
on  O
MT  O
quality  O
as  O
a  O
data  O
augmentation  O
method,  O
but  O
fails  O
to  O
lead  O
to  O
the  O
desired  O
effect  O
of  O
improved  O
length  O
compliance.  O
This  O
could  O
also  O
in  O
part  O
be  O
explained  O
by  O
the  O
fact  O
that  O
in  O
our  O
experiment  O
setting,  O
remov-  O
ing  O
synonym  O
data  O
resulted  O
in  O
the  O
increased  O
relative  O
proportion  O
of  O
length-compliant  O
back-  O
and  O
forward-  O
translated  O
data.  O
Removing  O
also  O
the  O
back-  O
and  O
forward-translated  O
data  O
from  O
training  O
leads  O
to  O
a  O
consistent  O
drop  O
inall  O
quality  O
metrics  O
on  O
tst-COMMON.  O
In  O
particu-  O
lar,  O
length  O
compliance  O
becomes  O
worse,  O
even  O
in  O
the  O
considered  O
case  O
that  O
uses  O
the  O
length  O
ROVER  O
and  O
N-  O
best  O
rescoring.  O
When  O
training  O
the  O
length-unbiased  O
system  O
of  O
row  O
5,  O
Table  O
1  O
without  O
synthetic  O
data  O
LC  O
even  O
drops  O
from  O
45.27  O
to  O
30.70  O
(not  O
shown  O
in  O
Table  O
2).  O
This  O
shows  O
that  O
length-compliant  O
back-  O
and  O
forward-translated  O
data  O
clearly  O
has  O
the  O
desired  O
effect  O
of  O
learning  O
isometric  O
translation  O
and  O
it  O
is  O
still  O
noticeable  O
when  O
combined  O
with  O
other  O
length  O
con-  O
trol  O
methods.  O
Also  O
for  O
the  O
length  O
encoding  O
model  O
(row  O
8)  O
we  O
observe  O
a  O
similar  O
positive  O
effect  O
of  O
the  O
synthetic  O
data,  O
despite  O
the  O
translation  O
length  O
being  O
predominantly  O
determined  O
by  O
the  O
length  O
value  O
fed  O
into  O
the  O
decoder.  O
On  O
the  O
blind  O
test  O
set  O
we  O
observe  O
contradicting  O
results.  O
For  O
this  O
we  O
can  O
provide  O
no  O
better  O
expla-  O
nation  O
than  O
referring  O
to  O
statistical  O
randomness.  O
In  O
Table  O
1  O
one  O
can  O
see  O
that  O
ranking  O
of  O
independently  O
trained  O
neural  O
models  O
(e.g.  O
rows  O
1,  O
3,  O
5,  O
17  O
and  O
21)  O
disagrees  O
on  O
the  O
two  O
test  O
sets,  O
which  O
we  O
attribute  O
to  O
the  O
small  O
size  O
of  O
200  O
lines  O
of  O
the  O
blind  O
test  O
set.  O
In  O
fact,  O
according  O
to  O
paired  O
bootstrap  O
resampling  O
computed  O
with  O
SacreBLEU  B-MetricName
(Post,  O
2018),  O
the  O
large  O
difference  O
of  O
1.3  O
BLEU  O
between  O
row  O
1  O
and  O
2  O
of  O
Ta-  O
ble  O
2  O
is  O
not  O
statistically  O
significant  O
with  O
p  O
<0.05,  O
and  O
the  O
95%  O
confidence  O
interval  O
of  O
row  O
1  O
is  O
2.8  O
BLEU.  O
5.4.2  O
Effect  O
of  O
length  O
perturbation  O
Without  O
length  O
perturbation  O
the  O
character-level  O
length  O
encoding  O
model  O
is  O
able  O
to  O
produce  O
length  O
compliant  O
translations  O
in  O
almost  O
all  O
cases,  O
as  O
can  O
be  O
seen  O
in  O
Row  O
7  O
of  O
Table  O
2,  O
without  O
the  O
need  O
for  O
sub-  O
sequent  O
steps  O
like  O
N-best  O
rescoring  O
or  O
second-pass  O
length  O
correction.  O
This  O
however  O
comes  O
at  O
the  O
cost  O
of  O
a  O
severe  O
drop  O
in  O
translation  O
quality  O
as  O
measured  O
in  O
both  O
BLEU  O
and  O
BERT  O
score.  O
When  O
comparing  O
to  O
row  O
24  O
of  O
Table  O
1  O
it  O
is  O
apparent  O
that  O
the  O
sys-  O
tem  O
trained  O
with  O
length  O
perturbation  O
and  O
using  O
the  O
above-mentioned  O
methods  O
can  O
achieve  O
a  O
similar  O
high  O
level  O
of  O
length  O
compliance  O
while  O
offering  O
a  O
better  O
translation  O
quality  O
by  O
2.6%  O
BLEU  O
and  O
1.1%  O
BERT  O
F1  O
score  O
absolute.  O
A  O
similar  O
drop  O
in  O
translation  O
quality  O
due  O
to  O
lack  O
of  O
length  O
perturbation  O
can  O
be  O
observed  O
for  O
the  O
case  O
of  O
token-level  O
length  O
encoding  O
comparing  O
rows  O
4  O
and  O
5  O
of  O
Table  O
2.  O
The  O
gain  O
in  O
LC  O
from  O
training  O
with-  O
out  O
noise  O
is  O
outperformed  O
by  O
the  O
combination  O
of  O
N-best  O
rescoring  O
and  O
second-pass  O
length  O
correction  O
applied  O
to  O
the  O
baseline  O
system  O
(row  O
20,  O
Table  O
1).  O
Notably,  O
even  O
without  O
noise  O
in  O
training  O
Ablation  O
study  O
results.  O
All  O
values  O
in  O
%.  O
length  O
encoding  O
does  O
not  O
surpass  O
a  O
length  O
compli-  O
ance  O
value  O
of  O
80%.  O
This  O
shows  O
that  O
the  O
number  O
of  O
subwords  O
is  O
not  O
accurate  O
enough  O
as  O
a  O
measure  O
of  O
length  O
when  O
targeting  O
a  O
precise  O
character  O
count.  O
6  O
Conclusion  O
In  O
this  O
paper,  O
we  O
described  O
AppTek’s  O
neural  O
MT  O
system  O
with  O
length  O
control  O
that  O
we  O
submitted  O
to  O
the  O
IWSLT  O
2022  O
Isometric  O
Spoken  O
Translation  O
Evalu-  O
ation.  O
We  O
showed  O
that  O
by  O
using  O
length-compliant  O
synthetic  O
data,  O
as  O
well  O
as  O
encoding  O
the  O
desired  O
trans-  O
lation  O
length  O
in  O
various  O
ways,  O
we  O
can  O
significantly  O
increase  O
the  O
length  O
compliance  O
score,  O
while  O
at  O
the  O
same  O
time  O
limiting  O
the  O
loss  O
of  O
information  O
as  O
re-  O
flected  O
in  O
only  O
slightly  O
lower  O
BERT  O
scores.  O
As  O
one  O
of  O
the  O
best  O
methods  O
for  O
real-time  O
production  O
set-  O
tings  O
not  O
involving  O
system  O
combination,  O
N-best  O
list  O
rescoring  O
or  O
2-pass  O
search,  O
the  O
modified  O
positional  O
encoding  O
that  O
counts  O
the  O
desired  O
length  O
in  O
charac-  O
ters  O
achieves  O
the  O
best  O
quality/length  O
compliance  O
trade-off  O
in  O
our  O
experiments.  O
We  O
attribute  O
this  O
to  O
more  O
fine-grained  O
length  O
control  O
capabilities  O
of  O
this  O
system  O
as  O
compared  O
to  O
systems  O
that  O
use  O
source-side  O
or  O
target-side  O
length  O
pseudo-tokens.  O