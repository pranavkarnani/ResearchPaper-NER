Proceedings O
of O
the O
2022 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics: O
Human O
Language O
Technologies O
, O
pages O
2551 O
=- O
2568 O
July O
10-15, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
Interactive O
Query-Assisted B-TaskName
Summarization I-TaskName
via I-TaskName
Deep I-TaskName
Reinforcement I-TaskName
Learning I-TaskName
Ori O
Shapira1,3∗, O
Ramakanth O
Pasunuru2, O
Mohit O
Bansal2, O
Ido O
Dagan1, O
and O
Yael O
Amsterdamer1 O
1Bar-Ilan O
University2UNC O
Chapel O
Hill3Amazon O
obspp18@gmail.com O
{ram,mbansal}@cs.unc.edu O
{dagan,amstery}@cs.biu.ac.il O
Abstract B-TaskName
Interactive I-TaskName
summarization I-TaskName
is O
a O
task O
that O
facil- O
itates O
user-guided O
exploration O
of O
information O
within O
a O
document O
set. O
While O
one O
would O
like O
to O
employ O
state O
of O
the O
art O
neural O
models O
to O
im- O
prove O
the O
quality O
of O
interactive B-TaskName
summarization, I-TaskName
many O
such O
technologies O
cannot O
ingest O
the O
full O
document O
set O
or O
cannot O
operate O
at O
sufficient O
speed O
for O
interactivity. O
To O
that O
end, O
we O
propose O
two O
novel O
deep O
reinforcement O
learning O
models O
for O
the O
task O
that O
address, O
respectively, O
the O
sub- O
task O
of O
summarizing O
salient O
information O
that O
adheres O
to O
user O
queries, O
and O
the O
subtask O
of O
list- O
ing O
suggested O
queries O
to O
assist O
users O
throughout O
their O
exploration.1In O
particular, O
our O
models O
allow O
encoding O
the O
interactive O
session O
state O
and O
history O
to O
refrain O
from O
redundancy. O
Together, O
these O
models O
compose O
a O
state O
of O
the O
art O
solu- O
tion O
that O
addresses O
all O
of O
the O
task O
requirements. O
We O
compare O
our O
solution O
to O
a O
recent O
interac- O
tive O
summarization O
system, O
and O
show O
through O
an O
experimental O
study O
involving O
real O
users O
that O
our O
models O
are O
able O
to O
improve O
informativeness O
while O
preserving O
positive O
user O
experience. O
1 O
Introduction O
Integrating O
human O
interaction O
into O
NLP O
tasks O
has O
been O
gaining O
the O
interest O
of O
the O
NLP O
community. O
Human-machine O
cooperation O
can O
improve O
the O
gen- O
eral O
quality O
of O
results, O
as O
well O
as O
provide O
a O
higher O
sense O
of O
control O
for O
the O
targeted O
consumer. O
We O
focus O
on O
the O
task O
of O
interactive B-TaskName
summarization I-TaskName
(INTSUMM B-TaskName
: O
Shapira O
et O
al., O
2021b) O
which O
enables O
information O
exploration O
within O
a O
document O
set O
on O
a O
topic, O
by O
means O
of O
user-guided O
summarization. B-TaskName
As O
illustrated O
in O
Figure O
1, O
a O
user O
can O
incrementally O
expand O
on O
a O
summary O
by O
submitting O
requests O
to O
the O
system, O
in O
order O
to O
expose O
the O
information O
of O
inter- O
est O
within O
the O
topic. O
A O
proper O
exploration O
session O
demands O
access O
to O
allinformation O
within O
the O
docu- O
ment O
set, O
and O
fast O
reaction O
time O
for O
smooth O
human O
∗This O
work O
was O
conducted O
prior O
to O
joining O
Amazon. O
1Code O
and O
trained O
models O
at: O
https://github.com/ O
OriShapira/InterExp_DeepRL O
engagement O
(Anderson, O
"2020;" O
Attig O
et O
al., O
2017). O
In O
addition, O
presented O
information O
must O
consider O
the O
session O
history O
to O
refrain O
from O
repetitiveness. O
While O
it O
is O
worthwhile O
to O
apply O
recent O
NLP O
ad- O
vances O
that O
excel O
at O
extracting O
salient O
and O
query- O
biased O
information, O
those O
advances O
usually O
come O
at O
a O
cost O
of O
rather O
small O
input O
size O
limits O
or O
heavy O
computation O
time. O
Indeed, O
all O
previous O
interactive B-TaskName
summarization B-TaskName
systems O
we O
know O
of O
either O
apply O
traditional O
methods O
or O
are O
inadequate O
for O
real-time O
processing O
due O
to O
high O
latency O
(§2). O
Our O
goal O
is O
to O
overcome O
these O
obstacles, O
and O
leverage O
advanced O
methods O
to O
improve O
information O
exposure O
while O
keeping O
latency O
acceptable O
for O
interaction. O
As O
depicted O
in O
Figure O
1, O
an O
INTSUMM B-TaskName
system O
provides O
an O
initial O
generic O
summary O
as O
an O
overview O
of O
the O
topic, O
after O
which O
a O
user O
can O
iteratively O
is- O
sue O
queries O
to O
the O
system O
for O
summary O
expansions O
on O
subtopics O
of O
interest. O
To O
support O
querying, O
the O
system O
offers O
a O
list O
of O
suggested O
queries O
, O
hinting O
at O
information O
concealed O
within O
the O
document O
set. O
We O
address O
the O
INTSUMM B-TaskName
task O
components O
through O
two O
subtasks: O
-1 O
generating O
the O
initial O
summary O
and O
query O
responses, O
and O
-2 O
generating O
lists O
of O
suggested O
queries. O
For O
each O
of O
the O
sub- O
tasks O
we O
propose O
a O
deep O
reinforcement O
learning O
(RL) O
algorithm O
that O
addresses O
the O
respective O
sub-2551task O
requirements. O
To O
enable O
comprehensive O
topic O
exploration, O
our O
models O
speedily O
process O
the O
full O
document O
set, O
as O
inspired O
by O
Mao O
et O
al. O
(2020). O
Ad- O
ditionally, O
they O
are O
able O
to O
peek O
at O
session O
history O
to O
comply O
to O
the O
current O
state O
of O
the O
interaction. O
The O
model O
for O
the O
query-assisted O
summarization O
subtask, O
MSumm B-MethodName
, O
incorporates O
the O
query O
sequence O
by O
-1 O
encoding O
a O
query O
into O
the O
contextual O
sen- O
tence O
representations, O
-2 O
attending O
the O
represen- O
tations O
using O
a O
new O
query-biased O
variant O
of O
the O
maximal O
marginal O
relevance O
(MMR: O
Carbonell O
and O
Goldstein, O
1998) O
function, O
and O
-3 O
a O
dual O
reward O
mechanism O
for O
policy O
optimization O
(Pasunuru O
and O
Bansal, O
2018) O
which O
we O
adapt O
to O
consider O
both O
ref- O
erence O
summaries O
and O
the O
query O
(§3). O
The O
model O
for O
the O
suggested O
queries O
list O
generation O
subtask, O
MSugg, B-MethodName
works O
at O
the O
phrase O
level, O
as O
opposed O
to O
the O
sentence O
level, O
to O
enable O
extraction O
of O
important O
phrases O
that O
serve O
as O
suggested O
queries. O
Similarly O
toMSumm B-MethodName
, O
the O
model O
learns O
importance O
with O
con- O
sideration O
to O
session O
history, O
but O
without O
an O
input O
query O
– O
as O
its O
role O
is O
to O
suggest O
such O
a O
query O
(§4). O
The O
models O
are O
trained O
on O
the O
DUC22007 B-DatasetName
multi- I-DatasetName
document I-DatasetName
summarization I-DatasetName
(MDS) I-DatasetName
news-domain O
dataset, B-HyperparameterName
with O
adaptions O
for O
our O
task O
setting. O
For O
testing, O
we O
follow O
the O
INTSUMM B-TaskName
evaluation O
frame- O
work O
of O
Shapira O
et O
al. O
(2021b) O
to O
run O
simulations, O
collect O
real O
user O
sessions, O
and O
assess O
the O
results, O
using O
DUC O
2006 O
In O
principle, O
summary O
informa- O
tiveness, O
i.e. O
general O
salience, O
could O
potentially O
come O
at O
the O
expense O
of O
query O
responsiveness, O
but O
importantly, O
our O
results O
show O
that O
our O
RL-based O
so- O
lution O
is O
able O
to O
significantly O
improve O
information O
exposure O
over O
the O
baseline O
of O
Shapira O
et O
al. O
(2021b), O
without O
compromising O
user O
experience O
(§5). O
2 O
Background O
and O
Related O
Work O
Interactive O
summarization O
facilitates O
user-guided O
information O
navigation O
within O
document O
sets. O
The O
task O
suffered O
from O
a O
lack O
of O
a O
methodological O
eval- O
uation, O
until O
Shapira O
et O
al. O
(2021b) O
formalized O
the O
INTSUMM B-TaskName
task O
with O
a O
framework O
consisting O
of O
a O
benchmark, O
evaluation O
metrics, O
a O
session O
collection O
process O
and O
baseline O
systems. O
This O
framework, O
that O
we O
leverage, O
enables O
comparison O
and O
analysis O
of O
systems, O
allowing O
principled O
research O
on O
the O
task O
and O
accelerated O
development O
of O
algorithms. O
To O
the O
best O
of O
our O
knowledge, O
all O
previous O
works O
onINTSUMM B-TaskName
have O
either O
applied O
more O
traditional O
text-processing O
methods O
or O
require O
costly O
prepro- O
2https://duc.nist.gov/cessing O
of O
inputs O
to O
facilitate O
seamless O
interaction. O
Leuski O
et O
al. O
-2003 O
used O
surface-form O
features O
for O
processing O
content, O
and O
Baumel O
et O
al. O
-2014 O
adapted O
classic O
MDS O
algorithms O
like O
LexRank O
(Erkan O
and O
Radev, O
2004) O
and O
KLSum O
(Haghighi O
and O
Vanderwende, O
2009). O
Christensen O
et O
al. O
-2014 O
optimized O
discourse O
graphs O
and O
Shapira O
et O
al. O
-2017 O
relied O
on O
a O
knowledge O
representation, O
both O
expensively O
pre-generating O
hierarchical O
summaries O
that O
limit O
expansions O
to O
pre-prepared O
information O
selections. O
Hirsch O
et O
al. O
-2021 O
applied O
advanced O
coreference O
resolution O
algorithms O
that O
take O
several O
hours O
for O
preprocessing O
a O
document O
set. O
The O
two O
INTSUMM B-TaskName
baseline O
systems O
of O
Shapira O
et O
al. O
(2021b) O
use O
sentence O
clustering O
or O
TextRank O
(Mihalcea O
and O
Tarau, O
2004) O
for O
summarization, O
sen- O
tence O
similarity O
heuristics O
for O
query-responses, O
and O
n-gram O
frequency O
or O
TextRank O
for O
suggested O
query O
extraction. O
Moreover, O
their O
query-response O
gen- O
erators O
strictly O
consider O
a O
given O
query, O
ignoring O
history O
or O
global O
informativeness. O
Our O
proposed O
algorithms O
significantly O
improve O
information O
expo- O
sure O
over O
the O
latter O
baselines, O
using O
advanced O
deep O
RL O
methods, O
working O
in O
real O
time. O
We O
next O
review O
some O
recent O
techniques O
in O
MDS, O
query-focused O
summarization O
and O
multi-document O
keyphrase O
ex- O
traction, O
all O
of O
which O
relate O
to O
the O
INTSUMM B-TaskName
task O
and O
our O
choice O
of O
algorithms. O
The O
subtask O
of O
query-assisted O
summarization. O
Non-interactive O
MDS O
has O
been O
researched O
exten- O
sively, O
with O
few O
recent O
neural-based O
methods O
that O
can O
handle O
relatively O
large O
inputs. O
For O
example, O
Wang O
et O
al. O
-2020 O
use O
graph O
neural O
networks O
to O
globally O
score O
sentence O
salience, O
Xiao O
et O
al. O
-2021 O
summarize O
using O
Longformers O
(Beltagy O
et O
al., O
2020), O
and O
Pasunuru O
et O
al. O
(2021b) O
combine O
a O
Longformer O
with O
BART O
(Lewis O
et O
al., O
2020) O
and O
incorporate O
graphical O
representation O
of O
information. O
Mao O
et O
al. O
-2020 O
apply O
deep O
RL O
for O
autoregressive O
sentence O
selection, O
and, O
in O
contrast O
to O
most O
other O
neural O
methods, O
can O
ingest O
the O
fulldocument O
set. O
In O
the O
query-focused O
summarization O
(QFS) O
task O
summaries O
are O
biased O
on O
a O
query. O
To O
accommo- O
date O
a O
query, O
Xie O
et O
al. O
-2020 O
use O
conditional O
self- O
attention O
to O
enforce O
dependency O
of O
the O
query O
on O
source O
words. O
Pasunuru O
et O
al. O
(2021a) O
and O
Kulka- O
rni O
et O
al. O
-2021 O
hierarchically O
encode O
a O
query O
with O
the O
documents. O
These O
and O
other O
QFS O
methods O
require O
large O
training O
sets, O
and O
limit O
the O
allowed O
input O
size O
(Baumel O
et O
al., O
"2018;" O
Laskar O
et O
al., O
2020). O
Relatedly, O
incremental O
update O
summarization O
(Mc-2552Creadie O
et O
al., O
"2014;" O
Lin O
et O
al., O
2017) O
marks O
query- O
relevant O
information O
as O
reported O
texts O
stream O
in, O
avoiding O
repeating O
information O
marked O
earlier. O
In- O
teractivity O
is O
not O
a O
constraining O
factor O
here, O
yielding O
solutions O
with O
relatively O
high O
computation O
time. O
With O
respect O
to O
the O
above O
related O
work, O
we O
de- O
velop O
a O
model O
inspired O
by O
Mao O
et O
al. O
(2020), O
which O
is O
closest O
to O
our O
requirements. O
To O
facilitate O
an O
inter- O
active O
setting, O
our O
model O
-1 O
enables O
query+history O
injection, O
-2 O
supports O
full O
input O
processing, O
neces- O
sary O
for O
complete O
information O
availability O
during O
exploration, O
-3 O
has O
low O
latency O
at O
inference O
time, O
and O
-4 O
requires O
a O
relatively O
small O
training O
set. O
The O
subtask O
of O
suggested-queries O
list O
genera- O
tion. O
Extracting O
suggested O
queries O
on O
a O
document O
set O
most O
resembles O
the O
multi-document O
keyphrase O
extraction O
(MDKE) O
task O
since O
it O
aims O
to O
identify O
salient O
keyphrases O
(Shapira O
et O
al., O
2021a). O
MDKE O
was O
mostly O
addressed O
using O
traditional O
heuristics O
or O
graph-centrality O
algorithms O
applied O
over O
the O
doc- O
uments O
(e.g. O
Mihalcea O
and O
Tarau, O
"2004;" O
Florescu O
and O
Caragea, O
2017). O
In O
contrast O
to O
MDKE, O
the O
sug- O
gested O
queries O
extraction O
subtask O
is O
a O
new O
paradigm O
that O
updates O
“keyphrases” O
with O
respect O
to O
session O
history. O
While O
previous O
methods O
for O
keyphrase O
extraction O
could O
potentially O
be O
adapted O
for O
our O
dy- O
namic O
setting, O
we O
choose O
to O
focus O
in O
this O
work O
on O
a O
deep O
RL O
architecture O
for O
suggested O
queries O
that O
res- O
onates O
our O
model O
for O
query-assisted O
summarization O
and O
allows O
sharing O
insights O
between O
the O
models. O
3 O
Query-Assisted O
Summarization O
Model O
The O
subtask O
of O
query-assisted O
summarization O
cov- O
ers O
two O
main O
components O
of O
the O
INTSUMM B-TaskName
task: O
the O
generators O
of O
an O
initial O
summary O
and O
of O
query- O
responses. O
The O
initial O
summary O
concisely O
specifies O
some O
central O
issues O
from O
the O
input O
topic O
(not O
biased O
on O
a O
query) O
to O
initiate O
the O
user’s O
understanding O
of O
the O
topic O
and O
to O
motivate O
further O
exploration. O
Then, O
for O
each O
user O
submitted O
query, O
the O
query-response O
generator O
non-redundantly O
expands O
on O
the O
previ- O
ously O
presented O
information O
with O
topically O
salient O
responses O
that O
are O
also O
biased O
around O
the O
query. O
We O
next O
formally O
define O
the O
subtask O
and O
then O
describe O
our O
RL O
model O
for O
it. O
3.1 O
Subtask O
Formulation O
The O
input O
to O
the O
query-assisted O
summarization O
sub- O
task O
is O
tuple O
(D, O
q,Ein, O
m), O
such O
that: O
Dis O
a O
docu- O
ment O
set O
on O
a O
topic O
where O
the O
j-th O
sentence O
in O
the O
concatenation O
of O
D’s O
documents O
is O
denoted O
"sj;qis" O
a O
query, O
and O
can O
be O
empty O
(denoted O
_) O
for O
an O
unbiased O
generic O
"summary;" O
Ein={ein O
1, O
..., O
ein O
k}is O
a O
sequence O
of O
sentences O
from O
Dtermed O
the O
history O
, O
containing O
texts O
previously O
output O
in O
the O
"session;" O
andmis O
the O
number O
of O
sentences O
to O
output. O
The O
output O
is O
sentence O
sequence O
Eout={eout O
1, O
..., O
eout O
m} O
fromD(extractive O
summarization). O
When O
in- O
putting O
(D,_,{}, O
m), O
the O
output O
is O
a O
generic O
sum- O
mary O
of O
msentences, O
that O
can O
serve O
as O
the O
initial O
"summary;" O
and O
when O
qandEinare O
not O
empty, O
the O
output O
is O
an O
expansion O
on O
Einin O
response O
to O
q, O
containing O
new O
salient O
information O
biased O
on O
q. O
Dis O
paired O
with O
a O
set O
of O
generic O
reference O
sum- O
maries O
R, O
which O
is O
used O
for O
training O
or O
as O
a O
part O
of O
the O
evaluation O
effort. O
3.2 O
Model O
Architecture O
Our O
query-assisted O
summarization O
model, O
MSumm B-MethodName
, O
is O
autoregressive, O
outputting O
the O
requested O
number O
of O
summary O
sentences O
one-by-one. O
At O
time O
step O
t, O
a O
sentence O
eout O
tis O
output O
according O
to O
the O
cur- O
rent O
query O
and O
an O
encoding O
of O
the O
summary-so-far O
Et={ein O
1, O
..., O
ein O
k, O
eout O
1, O
..., O
eout O
t−1}to O
prevent O
infor- O
mation O
repetition. O
At O
inference O
time, O
MSumm B-MethodName
out- O
puts O
the O
summary O
sentences O
with O
the O
given O
query O
and O
history O
(possibly O
empty). O
At O
train O
time, O
we O
emulate O
a O
session O
by O
invoking O
MSumm B-MethodName
with O
a O
se- O
quence O
of O
differing O
queries, O
Q={q1, O
q2, O
..., O
q O
m}, O
for O
which O
to O
generate O
the O
corresponding O
sequence O
of O
output O
sentences. O
I.e., O
output O
sentence O
eout O
tis O
biased O
on O
query O
qtand O
the O
summary-so-far O
Etat O
time O
step O
t. O
We O
next O
describe O
the O
architecture3of O
MSumm B-MethodName
, O
also O
illustrated O
in O
Figure O
2 O
Sentence O
encoding. O
The O
first O
step O
of O
the O
model O
is O
hierarchically O
encoding O
the O
sentences O
of O
the O
docu- O
ment O
set O
Dto O
obtain O
contextualized O
representation O
cjfor O
sentence O
sj∀j. O
A O
CNN O
(Kim, O
2014) O
en- O
codes O
sjon O
the O
sentence O
level O
and O
then O
a O
bi-LSTM O
(Huang O
et O
al., O
2015) O
forms O
representation O
cjon O
the O
document O
level, O
given O
the O
CNN O
encodings. O
Query O
encoding. O
Additionally, O
at O
each O
time O
step O
twe O
prepare O
sentence+query O
representations O
ct O
j= O
cj⊕CNN(qt), O
i.e., O
obtained O
by O
concatenating O
a O
sentence O
representation O
and O
the O
CNN-encoding O
of O
the O
current O
query. O
This O
sentence+query O
represen- O
3In O
general, O
the O
implementation O
choices O
weighed O
in O
the O
speed O
at O
which O
the O
full O
input O
document O
set O
can O
be O
processed. O
In O
comparison O
to O
other O
techniques O
(some O
of O
which O
are O
more O
recent), O
these O
choices O
gave O
as O
good O
or O
better O
results O
at O
lower O
latency. O
Alternative O
architectural O
choices O
and O
their O
behavior O
are O
discussed O
in O
Appendix O
B. O
tation O
influences O
the O
relevance O
of O
a O
sentence O
with O
respect O
to O
the O
current O
input O
query. O
Query-MMR O
score O
weighting. O
MMR O
has O
been O
shown O
to O
be O
effective O
in O
MDS, O
where O
information O
repeats O
across O
documents. O
It O
aims O
to O
select O
a O
salient O
sentence O
for O
a O
summary, O
that O
is O
non-redundant O
to O
previous O
summary O
sentences. O
We O
extend O
standard O
MMR O
so O
that O
the O
importance O
of O
the O
sentence O
is O
in O
regards O
to O
both O
the O
document O
set O
and O
the O
query. O
Formally, O
the O
query-focused O
MMR O
function O
defines O
a O
score O
mt O
jfor O
each O
sjat O
time O
step O
tas O
follows: O
mt O
j=λ·BISIM(sj,D, O
qt) O
−(1−λ)·maxe∈EtSIM(sj, O
e)(1) O
BISIM(sj,D, O
qt) O
=β·SIM(sj,D⊕) O
=+ O
(1−β)·SIM(sj, O
qt)(2) O
where O
λ∈[0,1]balances B-HyperparameterName
salience O
and O
redundancy O
andβ∈[0,1]balances B-HyperparameterName
a O
sentence’s O
salience O
within O
its O
document O
set O
and O
its O
resemblance O
to O
the O
current O
query. O
SIM(x, B-HyperparameterName
y)measures I-HyperparameterName
the O
similarity O
of O
texts O
xandy, O
andD⊕is B-HyperparameterName
a O
fully O
concatenated O
version O
of O
document O
set O
D. O
Following O
findings O
of O
Mao O
et O
al. O
(2020), O
SIMcomputes O
cosine O
similarity O
between O
the O
two O
compared O
texts’ O
TF-IDF O
vectors. O
Redun- O
dancy O
to O
previous O
sentences O
is O
computed O
as O
the O
highest O
similarity-score O
against O
any O
of O
the O
previous O
sentences. O
We O
set O
λ= B-HyperparameterName
0.6(following B-HyperparameterValue
Lebanoff O
et O
al., O
2018) O
and O
β= B-HyperparameterName
0.5(see B-HyperparameterValue
Appendix O
B.3).The O
query-focused O
MMR O
scores O
are O
incorpo- O
rated O
into O
MSumm B-MethodName
by O
softly O
attending O
on O
the O
sen- O
tence O
representations O
with O
their O
respective O
trans- O
lated O
query-focused O
MMR O
scores: O
µt=softmax O
(MLP(mt)) O
-3 O
ˆct O
j=µt O
jct O
j O
-4 O
State O
representation. O
At O
time O
t, O
a O
representa- O
tionztof O
the O
summary-so-far O
is O
computed O
by O
ap- O
plying O
an O
LSTM O
encoder O
on O
{cidx(ein O
1), O
...,cidx(ein O
k), O
cidx(eout O
1), O
...,cidx(eout O
t−1)}, O
i.e., O
on O
the O
plain O
sentence O
representations O
of O
Et, O
where O
idx(e)is O
the O
index O
of O
sentence O
e. O
Then, O
a O
state O
representation O
gtcon- O
siders O
ztand O
all O
sentence O
representations O
with O
the O
glimpse O
operation O
(Vinyals O
et O
al., O
2016): O
at O
j=v1tanh(W1ˆct O
j+W2zt) O
-5 O
αt=softmax O
(at) O
-6 O
gt=/summationdisplay O
jαt O
jW1ˆct O
j O
-7 O
where O
v1,W1andW2are O
model O
parameters, O
and O
atrepresents O
the O
vector O
composed O
of O
at O
j. O
Finally, O
a O
sentence O
sjat O
time O
tis O
assigned O
a O
selection O
probability O
softmax O
(pt)jsuch O
that: O
pt O
j=/braceleftigg O
v2tanh(W3ˆct O
j+W4gt)ifsj/∈Et O
−∞ O
otherwise(8) O
where O
v2,W3andW4are O
model O
parameters. O
Reinforcement O
learning. O
AsMSumm B-MethodName
’s O
goal O
is O
to O
incrementally O
generate O
a O
query-assisted2554summary, O
it O
should O
strive O
to O
optimize O
-1 O
non- O
redundant O
salient-sentence O
extraction O
and O
-2 O
query- O
to-sentence O
similarity, O
that O
can O
be O
appraised O
with O
ROUGE B-MetricName
(Lin, O
2004) O
and O
text-similarity O
metrics, O
respectively. O
A O
policy O
gradient-based O
RL O
approach O
(Williams, O
1992) O
allows O
optimizing O
on O
such O
non- O
differentiable O
metrics. O
Specifically, O
we O
adopt O
the O
Advantage O
Actor O
Critic O
method O
(Mnih O
et O
al., O
2016) O
for O
policy O
learning, O
and O
a O
dual-reward O
procedure O
(Pasunuru O
and O
Bansal, O
2018) O
to O
alternate O
between O
the O
summary O
and O
query-similarity O
rewards. O
At O
time O
step O
t, O
for O
selected O
sentence O
eout O
t(based O
onsoftmax O
(pt)), O
reward O
rtis O
computed O
and O
weighted O
into O
MSumm B-MethodName
’s O
loss O
function. O
The O
re- O
ward O
function O
alternates, O
from O
one O
train O
batch O
to O
the O
next, O
between O
ROUGE B-MetricName
∆(eout O
t,Et,R)and O
QSIM(eout B-MetricName
t, O
qt). O
The O
former O
computes O
the O
ROUGE B-MetricName
difference O
before O
adding O
ettoEtand O
after: O
ROUGE B-MetricName
∆(eout O
t,Et,R) O
= O
ROUGE B-MetricName
((Et∪eout O
t)⊕,R)−ROUGE B-MetricName
(E⊕ O
t,R)(9) O
A O
larger O
ROUGE B-MetricName
∆value O
implies O
that O
etconcisely O
adds O
more O
information O
onto O
Et, O
with O
respect O
to O
topic O
reference O
summaries O
R. O
We O
use O
ROUGE- B-MetricName
1 I-MetricName
F1as I-MetricName
the O
ROUGE B-MetricName
function O
here. O
The O
query- O
similarity O
reward O
function O
QSIM(eout B-MetricName
t, O
qt) O
= O
avg(SEMSIM O
(eout O
t, O
qt),LEXSIM O
(eout O
t, O
qt))(10) O
computes O
an O
average O
of O
semantic O
and O
lexical O
sim- O
ilarities O
between O
the O
selected O
sentence O
and O
corre- O
sponding O
query. O
SEMSIM O
computes O
the O
cosine O
sim- O
ilarity O
between O
the O
average O
of O
word O
embeddings O
(spaCy: O
Honnibal O
and O
Montani, O
2021) O
of O
eout O
tand O
that O
of O
qt. O
For O
lexical O
similarity, O
LEXSIM O
(eout O
t, O
qt) O
= O
avg(Rp O
1(eout O
t, O
qt), O
Rp O
2(eout O
t, O
qt), O
Rp O
L(eout O
t, O
qt))(11) O
is O
the O
average O
of O
ROUGE B-MetricName
-1, I-MetricName
2 I-MetricName
and O
L B-MetricName
precision I-MetricName
scores O
between O
sentence O
and O
query. O
By O
alternat- O
ing O
between O
the O
two O
rewards, O
we O
train O
a O
sentence- O
selection O
policy O
in O
MSumm B-MethodName
to O
balance O
summary O
informativeness O
and O
adherence O
to O
queries. O
Overall O
system. O
OurMSumm B-MethodName
model O
adopts O
its O
base O
architecture O
from O
Mao O
et O
al. O
-2020 O
(for O
generic O
MDS). O
Chiefly, O
we O
modify O
their O
model O
for O
handling O
an O
input O
query-sequence O
and O
a O
sentencehistory, O
and O
employ O
a O
different O
summarization O
re- O
ward O
function. O
The O
query O
is O
incorporated O
in O
the O
sentence O
representation, O
in O
the O
new O
query-focused O
MMR O
function O
and O
in O
the O
dual-reward O
mechanism. O
3.3 O
Model O
Training O
Pre-training. O
To O
provide O
a O
warm O
start O
for O
train- O
ingMSumm B-MethodName
, O
a O
reduced O
version O
of O
MSumm B-MethodName
is O
first O
pre-trained O
for O
generic O
extractive O
single-document O
summarization O
using O
the O
large-scale O
CNN/Daily B-DatasetName
Mail B-DatasetName
corpus O
(Hermann O
et O
al., O
2015), O
as O
proposed O
by O
Chen O
and O
Bansal O
(2018). O
The O
reduced O
model O
pre-trains O
the O
full O
model O
for O
contextual O
sentence O
representation O
and O
for O
salient-sentence O
selection O
in O
the O
single-document O
generic O
setting. O
See O
Appendix O
B.1 O
for O
precise O
technical O
details. O
Training O
data. O
After O
pre-training O
the O
reduced O
version O
of O
MSumm B-MethodName
, O
we O
train O
the O
full O
model O
using O
the O
DUC B-DatasetName
2007 I-DatasetName
MDS I-DatasetName
dataset, O
with O
modifications O
for O
our O
query-assisted O
MDS O
task. O
The O
dataset O
includes O
45 O
topics O
(split O
into O
35/10 O
train/val), O
each O
contain- O
ing O
25 O
documents O
and O
4 O
reference O
summaries. O
For O
each O
topic, O
we O
generate O
an O
“oracle” O
extrac- O
tive O
summary O
by O
greedily O
aggregating O
10 O
sentences O
fromD, O
that O
maximizes O
the O
ROUGE B-MetricName
∆-1recall I-MetricName
against O
R. O
Then O
for O
each O
sentence, O
we O
extract O
a O
bi- O
or O
trigram O
that O
is O
most O
lexically-unique O
to O
the O
sentence, O
in O
comparison O
to O
all O
other O
sentences O
in O
D. O
This O
yields O
a O
sequence O
of O
10 O
“queries” O
that O
could O
easily O
render O
the O
corresponding O
oracle O
summary. O
The O
intuition O
for O
this O
approach O
is O
that O
it O
would O
teach O
MSumm B-MethodName
that O
it O
is O
worthwhile O
to O
consider O
a O
given O
query O
when O
selecting O
a O
sentence O
that O
is O
informa- O
tive O
with O
respect O
to O
the O
reference O
summaries. O
This O
further O
assists O
in O
fulfilling O
the O
dual O
requirements O
of O
selecting O
a O
globally O
informative O
sentence O
that O
also O
adheres O
to O
the O
query.4Appendix O
B.3 O
discusses O
usage O
of O
different O
query O
types O
for O
training. O
Validation O
metric. O
As O
the O
interactive O
session O
pro- O
gresses, O
a O
recall O
curve O
emerges, O
that O
maps O
the O
ROUGE B-MetricName
recall I-MetricName
score O
(here O
ROUGE B-MetricName
-1) I-MetricName
versus O
the O
expanding O
summary O
token-length. O
Once O
the O
ses- O
sion O
halts, O
the O
area O
under O
the O
curve O
indicates O
the O
efficacy O
of O
the O
session O
for O
information O
exposure. O
A O
higher O
value O
implies O
faster O
unveiling O
of O
salient O
4Seemingly, O
the O
most O
natural O
approach O
would O
be O
to O
train O
the O
model O
with O
queries O
from O
real O
sessions O
(collected O
using O
a O
different O
system). O
However, O
a O
session’s O
queries O
are O
dependent O
on O
outputs O
previously O
produced O
by O
the O
used O
system. O
Hence, O
these O
do O
not O
benefit O
the O
training O
process O
more O
than O
a O
synthe- O
sized O
sequence O
of O
queries.2555information. O
Normalizing O
by O
the O
final O
summary O
length O
allows O
approximate O
comparability O
between O
different O
length O
sessions. O
We O
hence O
use O
the O
aver- O
age O
(over O
topics) O
length-normalized O
area O
under O
the O
recall O
curve O
for O
validating O
the O
training O
progress. O
4 O
Suggested O
Queries O
Extraction O
Model O
4.1 O
Subtask O
Formulation O
We O
now O
consider O
the O
second O
subtask O
of O
INTSUMM B-TaskName
: O
generating O
lists O
of O
suggested O
queries. O
The O
list O
is O
regenerated O
after O
every O
interaction, O
to O
yield O
queries O
that O
focus O
on O
sub-topics O
that O
were O
not O
yet O
explored. O
Reusing O
the O
notations O
of O
MSumm B-MethodName
in O
§3, O
we O
de- O
fine O
a O
model, O
MSugg, B-MethodName
for O
suggested O
queries O
list O
generation, O
that O
receives O
an O
input O
tuple O
(D,Ein, O
m) O
(notice O
that O
a O
query O
is O
not O
needed O
here). O
Here, O
the O
j- O
thphrase O
inDis O
denoted O
ρj, O
when O
the O
documents O
inDare O
concatenated, O
and O
accordingly, O
history O
Einis O
a O
list O
of O
phrases O
extracted O
from O
the O
session’s O
current O
accumulated O
summary. O
mis O
the O
number O
of O
suggested O
queries O
to O
output. O
The O
model O
outputs O
phrase O
sequence O
Eout={eout O
1, O
eout O
2, O
..., O
eout O
m}from O
D, O
accounting O
for O
history O
Ein. O
As O
in O
MSumm B-MethodName
’s O
setting, O
Dis O
paired O
with O
a O
set O
of O
generic O
reference O
summaries O
R. O
4.2 O
Model O
Architecture O
We O
adopt O
and O
adjust O
the O
architecture O
in O
§3.2 O
for O
this O
subtask. O
Similar O
to O
MSumm B-MethodName
,MSugg B-MethodName
selects O
input O
units O
one-by-one O
considering O
a O
history, O
with O
the O
main O
difference O
being O
the O
absence O
of O
query O
injec- O
tion. O
Additionally, O
inputs O
and O
outputs O
are O
processed O
on O
the O
phrase- O
rather O
than O
the O
sentence O
level. O
Phrase O
and O
state O
representation. O
For O
the O
given O
document O
set, O
all O
noun O
phrases O
are O
extracted O
using O
a O
standard O
part-of-speech O
regular O
expression O
method O
(Mihalcea O
and O
Tarau, O
"2004;" O
Wan O
and O
Xiao, O
2008). O
We O
obtain O
document-level O
contextual O
phrase O
em- O
beddings, O
cjfor O
phrase O
ρj, O
with O
the O
CNN O
and O
bi- O
LSTM O
networks, O
and O
softly O
attend O
the O
embeddings O
with O
a O
standard O
MMR O
score: O
mt O
j=λ·SIM(ρj,D⊕) O
−(1−λ)·maxe∈EtSIM(ρj, O
e)(12) O
The O
MMR-based O
phrase O
representations O
then O
pass O
through O
the O
glimpse O
attention O
procedure, O
which O
culminates O
in O
the O
phrase O
probability O
distribu- O
tion O
for O
selecting O
the O
next O
output O
phrase.Reinforcement O
learning. O
The O
policy O
in O
MSugg B-MethodName
is O
trained O
with O
a O
single O
reward O
function O
that O
measures O
how O
prominent O
the O
selected O
phrase O
is O
within O
the O
reference O
summaries, O
and O
how O
different O
it O
is O
from O
previously O
seen O
phrases. O
Formally, O
at O
time O
step O
t, O
the O
reward O
rtof O
selected O
phrase O
eout O
tis: O
rt=PF(eout O
t,R)−γ1·PFMAX(eout O
t,Ein,R) O
−γ2·PFMAX(eout O
t,Et\Ein,R)(13) O
PF(eout O
t,R) O
#NAME? O
w∈eout O
tTF(w, O
r))(14) O
PFMAX(eout O
t,L,R) O
#NAME? O
t∩e,R) O
-15 O
where O
TF(w, O
r)is O
the O
relative O
frequency O
of O
word O
w O
in O
reference O
summary O
r. O
Namely, O
PFcomputes O
the O
average O
term O
frequency O
of O
a O
phrase O
over O
its O
words O
and O
across O
the O
reference O
summaries, O
as O
an O
estimate O
of O
the O
phrase O
importance O
within O
the O
topic. O
PFMAX O
computes O
the O
highest O
PFagainst O
a O
list O
of O
phrases, O
which O
is O
used O
to O
lower O
the O
reward O
of O
a O
phrase O
that O
is O
redundant O
to O
phrases O
used O
earlier. O
Different O
weights O
are O
given O
to O
the O
PFMAXagainst O
the O
input O
history O
(γ1) O
and O
that O
of O
the O
phrases O
output O
so O
far O
( O
γ2). O
4.3 O
Model O
Training O
Similarly O
to O
MSumm B-MethodName
, O
we O
first O
pre-train O
the O
base O
model O
to O
get O
a O
warm O
start O
on O
embedding O
formation O
and O
salience O
detection. O
The O
reduced O
architecture O
of O
MSumm B-MethodName
andMSugg B-MethodName
for O
pre-training O
are O
identical. O
We O
use O
the O
same O
DUC B-DatasetName
2007 I-DatasetName
training O
data O
, O
with O
document O
sets O
and O
reference O
summaries, O
and O
ad- O
ditionally O
prepare O
three O
“histories” O
per O
topic: O
one O
empty O
and O
two O
non-empty. O
An O
empty O
history O
mim- O
ics O
generating O
a O
session’s O
initial O
list O
of O
suggested O
queries, O
while O
a O
non-empty O
history O
trains O
the O
model O
to O
consider O
previously O
known O
information. O
Train- O
ing O
with O
two O
non-empty O
histories O
per O
topic O
prepares O
a O
model O
for O
varying O
informational O
states. O
These O
are O
curated O
from O
a O
generic O
summary O
(from O
a O
trained O
MSumm B-MethodName
model) O
that O
is O
truncated O
at O
two O
random O
sentence-lengths O
between O
1 O
and O
12 O
Overall, O
the O
model O
is O
trained O
on O
three O
versions O
of O
each O
topic, O
each O
time O
with O
a O
different O
history. O
Similarly O
to O
MSumm B-MethodName
,validation O
is O
guided O
by O
the O
average B-MetricName
normalized I-MetricName
area I-MetricName
under I-MetricName
the I-MetricName
recall I-MetricName
curve. I-MetricName
Here, O
the O
accumulating O
rtscores O
from O
Equation O
13 O
are O
used O
as O
the O
recall O
of O
the O
expanding O
suggested O
queries O
list. O
I.e., O
a O
higher O
reward O
means O
better O
sug- O
gested O
queries O
are O
output O
earlier. O
The O
AUC B-MetricName
is O
nor- O
malized O
with O
the O
total O
token-length O
of O
all O
suggested O
queries O
to O
mitigate O
for O
lengthy O
phrase O
extractions.25565 O
Experiments O
We O
ran O
several O
experiments O
for O
the O
assessment O
of O
our O
MSumm B-MethodName
andMSugg B-MethodName
models, O
applying O
the O
INTSUMM B-TaskName
evaluation O
framework O
of O
Shapira O
et O
al. O
(2021b). O
The O
goals O
of O
the O
experiments O
are O
to O
com- O
pare O
varying O
configurations O
of O
our O
models O
and O
to O
evaluate O
against O
an O
INTSUMM B-TaskName
baseline O
system. O
The O
experiments O
include O
both O
simulations O
and O
in- O
teractive O
sessions O
with O
human O
users. O
5.1 O
Compared O
Algorithms O
TheMSumm B-MethodName
model O
architecture O
(§3.2) O
has O
several O
configurable O
components: O
encoding O
the O
query O
into O
sentences, O
considering O
the O
query O
in O
the O
MMR O
func- O
tion O
(both O
at O
train O
and O
inference O
time), O
and O
the O
dual O
reward O
mechanism. O
We O
compared O
several O
varia- O
tions O
of O
these O
using O
simulations, O
presented O
in O
§5.2. O
In O
addition, O
we O
compare, O
both O
via O
simulations O
(§5.2) O
and O
real O
sessions O
(§5.3), O
against O
the O
(better- O
performing) O
baseline O
system O
in O
(Shapira O
et O
al., O
2021b), O
named O
S2.S2’s O
initial O
summary O
algorithm O
is O
TextRank, O
and O
the O
query-response O
generator O
ex- O
tracts O
sentences O
via O
lexical+semantic O
similarity O
to O
the O
query, O
somewhat O
resembling O
QSIMin B-MetricName
Equation O
10, O
fully O
neglecting O
the O
summary-so-far, O
in O
contrast O
toMSumm B-MethodName
.S2’s O
suggested O
queries O
list O
contains O
TextRank’s O
top O
salient O
topic O
phrases. O
Since O
these O
too O
do O
not O
account O
for O
the O
summary-so-far, O
they O
are O
computed O
at O
the O
session O
beginning O
and O
are O
not O
updated O
along O
the O
session, O
in O
contrast O
to O
MSugg. B-MethodName
5.2 O
Simulated O
Experiments O
TheINTSUMM B-TaskName
task O
involves O
human O
users O
by O
defi- O
nition. O
Nevertheless, O
running O
on O
simulated O
query O
lists O
and O
session O
histories O
is O
pertinent O
for O
efficient O
system O
evaluation O
and O
comparison O
of O
methods. B-HyperparameterName
To O
simulate O
the O
query-assisted O
summarization O
algorithms, O
we O
utilize O
the O
real O
sessions O
recorded O
by O
Shapira O
et O
al. O
(2021b): O
03-Apr O
user O
sessions O
on O
20 O
topics O
from O
DUC B-DatasetName
2006 I-DatasetName
collected O
with O
S2. O
In O
our O
simulation, O
each O
summary-so-far O
from O
a O
recorded O
session O
is O
fed O
as O
input O
to O
the O
system O
together O
with O
the O
following O
recorded O
user O
query. O
We O
then O
measure O
Rrecall B-MetricName
1∆(difference I-MetricName
of O
ROUGE- B-MetricName
1recall I-MetricName
in- O
curred O
by O
the O
query O
response O
compared O
with O
the O
input O
summary-so-far). O
Additionally, O
we O
use O
RF1 B-MetricName
1 O
(ROUGE- B-MetricName
1F1) I-MetricName
for O
initial O
summary O
informative- O
ness. O
Both O
are O
measured O
w.r.t. O
the O
reference O
sum- O
maries, O
normalized O
by O
the O
output O
length, O
and O
aver- O
aged O
per O
session O
recording, O
and O
then O
over O
all O
ses- O
sions O
and O
topics, O
to O
get O
an O
overall O
system O
infor-mativeness O
score. O
We O
also O
measure O
system O
query- O
responsiveness O
using O
the O
QSIMmetric. B-MetricName
Table O
1 O
presents O
a O
representative O
partial O
ablation O
of O
the O
MSumm B-MethodName
model. O
All O
variants O
were O
config- O
ured O
to O
output O
sentences O
of O
up O
to O
30 B-HyperparameterValue
tokens, B-HyperparameterName
initial B-HyperparameterName
summaries I-HyperparameterName
are O
75 B-HyperparameterValue
tokens, O
and O
query B-HyperparameterName
responses I-HyperparameterName
are O
2 B-HyperparameterValue
sentences. O
Configurations O
i-ivuse O
the O
query O
in O
training, O
while O
vandvido O
not. O
Each O
configuration O
is O
measured O
for O
informativeness O
(columns O
marked O
with†), O
and O
for O
query-responsiveness O
( O
QSIMcol- B-MetricName
umn). O
Out O
of O
configurations O
i-iv, O
config. O
i, O
where O
we O
employ O
all O
mechanisms O
for O
query O
inclusion, O
yields O
the O
best O
overall O
scores O
in O
both O
informative- O
ness O
and O
query-responsiveness, O
despite O
the O
inher- O
ent O
tradeoff O
between O
the O
two. O
In O
the O
second O
set O
of O
configurations O
( O
v-vi), O
we O
observe O
that O
ignoring O
the O
query O
at O
train O
time O
substantially O
degrades O
query- O
responsiveness, O
and O
this O
is O
expectedly O
further O
exac- O
erbated O
when O
also O
ignoring O
the O
query O
at O
inference O
time. O
However, O
disregarding O
the O
query O
gives O
more O
informative O
expansions O
with O
respect O
to O
reference O
summaries, O
since O
the O
model O
was O
trained O
only O
to O
optimize O
content O
informativeness, O
and O
is O
less O
likely O
to O
sidetrack O
to O
the O
query-related O
information. O
Compared O
to O
S2(last O
row), O
our O
model O
sig- O
nificantly O
improves O
informativeness. O
Query- O
responsiveness O
is O
better O
in O
the O
S2baseline O
since O
its O
query-response O
generator O
simply O
invokes O
a O
func- O
tion O
similar O
to O
QSIM, B-MetricName
but O
for O
the O
price O
of O
lower O
informativeness. O
Still, O
this O
does O
not O
lead O
to O
inferior O
overall O
user O
experience, O
see O
§ O
5.3. O
5.3 O
Real O
Session O
Collection O
and O
Evaluation O
We O
collect O
real O
user O
sessions O
via O
controlled O
crowd- O
sourcing O
(which O
provides O
high O
quality O
work, O
see O
Appendix O
D) O
with O
the O
use O
of O
an O
INTSUMM B-TaskName
web O
application5running O
either O
our O
MSumm B-MethodName
#NAME? B-MethodName
models O
or O
the O
S2baseline O
algorithms, O
enabling O
a O
comparative O
assessment O
of O
the O
two O
systems. O
No- O
tably, O
our O
algorithms O
have O
the O
low O
latency O
required O
for O
the O
interactive O
setting O
(Attig O
et O
al., O
2017), O
i.e., O
responding O
almost O
immediately.6 O
Using O
the O
DUC B-DatasetName
2006 I-DatasetName
INTSUMM B-TaskName
test O
set, O
we O
pre- O
pared O
two O
complementing O
user O
sets O
of O
20 O
topics, O
each O
with O
10 O
of O
the O
topics O
to O
be O
run O
on O
our O
system O
and O
the O
other O
10 O
on O
the O
baseline. O
We O
apply O
the O
eval- O
uation O
metrics O
of O
Shapira O
et O
al. O
(2021b): O
-1 O
The O
5Minimally O
modified O
from O
(Shapira O
et O
al., O
2021b) O
to O
sup- O
port O
updating O
the O
suggested O
queries O
list O
after O
each O
interaction. O
6MSumm B-MethodName
generates O
summaries O
in O
under O
a O
second O
and O
MSugg B-MethodName
prepares O
the O
list O
of O
suggested O
queries O
in O
a O
few O
sec- O
onds. O
See O
Appendix O
E.2 O
for O
more O
details. B-MethodName
area O
under O
the O
sessions’ O
ROUGE B-MetricName
recall O
curves, O
in O
a O
common O
word-length O
interval O
across O
all O
sessions O
and O
topics, O
which O
demonstrates O
how O
fast O
salient O
in- O
formation O
is O
exposed O
in O
sessions. O
-2 O
ROUGE B-MetricName
F1at I-MetricName
the O
initial O
summary O
and O
at O
250 O
tokens, O
that O
indicate O
how O
effectively O
the O
interactive O
system O
can O
gener- O
ate O
summaries O
at O
pre-specified, O
comparable O
lengths. O
-3 O
Manually O
assigned O
query-responsiveness O
score O
(1 O
to O
5 O
scale), O
which O
expresses O
how O
well O
users O
think O
the O
system O
responded O
to O
their O
requests. O
And O
-4 O
manual O
UMUX-Lite B-MetricName
(Lewis O
et O
al., O
2013) O
score O
for O
system O
usability O
(effectiveness O
and O
ease O
of O
use), O
where O
68is O
considered O
“acceptable” O
and O
80.3is O
considered O
“excellent”. O
We O
also O
measure O
automatic O
query-responsiveness O
with O
QSIM.7 B-MetricName
We O
conducted O
two O
such O
comparative O
collec- O
tion O
and O
assessment O
experiments, O
either O
employing O
MSumm B-MethodName
configuration O
vori, O
namely O
the O
best O
of O
the O
two O
configuration O
sets. O
In O
both O
cases, O
the O
MSugg O
model O
used O
was O
set O
with O
γ1= B-HyperparameterName
0.5andγ2= B-HyperparameterValue
0.9 B-HyperparameterValue
after O
some O
hyperparameter O
tuning O
(Appendix O
B.4). O
The O
first O
experiment O
(with O
configuration O
v) O
is O
de- O
scribed O
here, O
and O
the O
other O
in O
Appendix O
E.1. O
We O
hired O
6 O
qualified O
workers O
using O
the O
controlled O
crowdsourcing O
procedure, O
and O
collected O
02-Mar O
ses- O
sions O
per O
topic O
per O
system O
(111 O
total O
sessions). O
In O
7While O
QSIMis B-MetricName
a O
reasonable O
automatic O
measure O
for O
esti- O
mation O
of O
query-responsiveness, O
it O
is O
left O
for O
future O
work O
to O
assess O
its O
TRUE O
reliability O
for O
such O
use.the O
sessions, O
users O
explore O
their O
given O
topic O
by O
sub- O
mitting O
queries O
with O
a O
common O
generic O
informa- O
tional O
goal O
in O
mind O
(Appendix O
D). O
Overall O
system O
assessment. O
Table O
2, O
presenting O
average O
scores O
over O
the O
collected O
sessions, O
shows O
that O
our O
system O
is O
significantly O
more O
effective O
for O
exposing O
salient O
information, O
as O
depicted O
in O
the O
first O
three O
rows. O
Users O
indicate O
a O
slight O
degradation O
in O
query-responsiveness O
of O
our O
system, O
consistent O
with O
QSIMscores B-MetricName
(row O
4-5). O
Note O
that O
the O
observed O
difference O
in O
QSIMscores, B-MetricName
between O
simulations O
and O
user O
sessions, O
partly O
stems O
from O
the O
fact O
that O
they O
were O
computed O
over O
different O
sets O
of O
queries. O
The O
varying O
queries O
issued O
by O
the O
users O
in O
user O
sessions O
form O
a O
less O
stable O
query O
responsiveness O
comparison O
than O
the O
one O
in O
Table O
1, O
where O
QSIM B-MetricName
scores I-MetricName
are O
computed O
using O
consistent O
queries O
for O
all O
systems. O
Despite O
the O
gap O
in O
QSIMscores B-MetricName
between O
our O
system O
and O
S2in O
Table O
2, O
the O
overall O
usability O
scores O
are O
slightly O
better O
(last O
row). O
This O
may O
sug- O
gest O
that O
users O
appreciate O
the O
informativeness O
of O
the O
produced O
summary O
even O
when O
they O
are O
aware O
that O
the O
summary O
is O
less O
biased O
on O
their O
"queries;" O
thus O
our O
system O
improves O
informativeness O
while O
still O
providing O
a O
favorable O
user O
experience. O
Assessment O
of O
suggested O
queries O
functionality. O
We O
analyzed O
the O
types O
of O
queries O
users O
submitted O
throughout O
their O
sessions, O
to O
assess O
the O
utility O
of O
up- O
dating O
suggested O
queries, O
with O
MSugg, B-MethodName
as O
opposed O
to O
a O
static O
list O
of O
suggestions, O
with O
S2. O
To O
that O
end, O
we O
tallied O
suggested O
query O
clicks O
and O
query O
submissions O
via O
other O
modes, O
binning O
the O
tallies O
to O
three O
sequential O
temporal O
segments O
within O
their O
respective O
sessions O
(Appendix O
E.3). O
We O
found O
that, O
on O
average, O
the O
usage O
of O
suggested O
query B-MetricName
clicks I-MetricName
in- O
creased O
by O
~13% B-MetricValue
when O
nearing O
the O
end O
of O
a O
session O
withMSugg, B-MethodName
and O
conversely O
decreased O
by O
~24% B-MetricValue
withS2. O
While O
the O
decrease O
in O
use O
of O
the O
static O
list O
is O
expected, O
since O
appealing O
queries O
are O
likely O
exhausted O
earlier O
in O
a O
session, O
it O
is O
encouraging O
to2558witness O
the O
usefulness O
of O
updated O
queries O
as O
the O
session O
progresses. O
This O
behavior O
suggests O
that O
the O
updated O
list O
contains O
suggested O
queries O
that O
are O
indeed O
engaging O
for O
learning O
more O
about O
the O
topic. O
6 O
Conclusion O
Interactive O
summarization O
for O
information O
explo- O
ration O
is O
a O
task O
that O
requires O
compliance O
to O
user O
requests O
and O
session O
history, O
while O
comprehen- O
sively O
handling O
a O
large O
input O
document O
set. O
These O
requirements O
pose O
a O
challenge O
for O
advanced O
text O
processing O
methods O
due O
to O
the O
need O
for O
fast O
reac- O
tion O
time. O
We O
present O
novel O
deep O
reinforcement O
learning O
based O
algorithms O
that O
answer O
to O
the O
task O
requirements, O
improving O
salient O
information O
expo- O
sure O
while O
satisfying O
user O
queries O
and O
keeping O
user O
experience O
positive. O
We O
note O
that O
while O
MSumm B-MethodName
is O
designed O
for O
the O
INTSUMM B-TaskName
task, O
it O
may O
potentially O
be O
serviceable O
for O
standard O
MDS, O
QFS, O
update O
summarization O
and O
combinations O
thereof. O
This O
can O
be O
accommodated O
by O
a O
proper O
choice O
of O
input, O
e.g., O
QFS O
can O
be O
ad- O
dressed O
by O
giving O
MSumm B-MethodName
as O
input O
a O
query, O
an O
empty O
history O
and O
target O
summary O
length. O
In O
fu- O
ture O
work, O
we O
may O
study O
the O
performance O
of O
our O
solutions O
for O
such O
tasks, O
as O
well O
as O
strive O
to O
fur- O
ther O
improve O
their O
performance O
on O
both O
ends O
of O
the O
INTSUMM B-TaskName
task O
– O
selecting O
topically O
salient O
infor- O
mation O
and O
responding O
to O
user O
queries. O
Acknowledgements O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
con- O
structive O
comments O
and O
suggestions. O
This O
work O
was O
supported O
in O
part O
by O
Intel O
"Labs;" O
by O
the O
Is- O
rael O
Science O
Foundation O
(grants O
no. O
2827/21 O
and O
"2015/21);" O
by O
a O
grant O
from O
the O
Israel O
Ministry O
of O
Science O
and O
"Technology;" O
by O
the O
NSF-CAREER O
Award O
"#1846185;" O
and O
by O
a O
Microsoft O
PhD O
Fellow- O
ship. O
Ethical O
Considerations O
Datasets. O
The O
DUC B-DatasetName
2006 I-DatasetName
and O
2007 O
datasets O
were O
obtained O
according O
to O
the O
DUC O
website O
( O
duc. O
nist.gov O
) O
requirements. O
It O
was O
not O
possible O
for O
others O
to O
reconstruct O
the O
document O
sets O
and O
refer- O
ence O
summaries O
of O
the O
dataset O
from O
the O
crowdsourc- O
ing O
tasks. O
The O
datasets O
are O
composed O
of O
new O
articles O
mainly O
from O
the O
late O
1990s O
from O
large O
news O
out- O
lets, O
compiled O
by O
NIST. O
All O
data O
exposed O
by O
our O
systems O
are O
directly O
extracted O
from O
those O
articles. O
For O
extraction, O
we O
do O
not O
intentionally O
add O
in O
any O
rules O
for O
ignoring O
or O
boosting O
certain O
information O
due O
to O
an O
opinion. O
Crowdsourcing. O
Due O
to O
the O
need O
for O
English O
speaking O
workers, O
a O
location O
filter O
was O
set O
on O
the O
Amazon O
Mechanical O
Turk O
( O
https://www. O
mturk.com O
) O
tasks O
for O
the O
US, O
UK O
and O
Australia. O
All O
tasks O
paid O
according O
to O
a O
$10 O
per O
hour O
wage, O
according O
to O
the O
estimated O
required O
time O
of O
each O
task. O
The O
payment O
was O
either O
paid O
per O
assignment, O
or O
as O
a O
combination O
with O
a O
bonus. O
Compute O
resources. O
Our O
MSumm B-MethodName
and O
MSugg B-MethodName
models O
required O
between O
2 O
and O
20 O
hours O
of O
training O
(usually O
around O
4 O
hours), O
depending O
on O
the O
configuration. O
We O
trained O
on O
one O
NVIDIA O
GeForce O
GTX O
1080 O
Ti O
GPU O
with O
11GB O
memory. O
The O
pretrained O
base O
model O
was O
trained O
once O
and O
reused O
in O
all O
subsequent O
training. O
Outputting O
at O
inference O
time O
is O
computationally O
cheap: O
MSumm B-MethodName
runs O
upto O
about O
1 O
second, O
but O
mostly O
in O
a O
few O
hundred O
milliseconds, O
and O
MSugg B-MethodName
runs O
upto O
about O
7 O
seconds, O
but O
mostly O
in O
under O
4 O
seconds. O
Training O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
8 B-HyperparameterValue
used O
about O
3GB O
GPU O
memory O
for O
MSumm B-MethodName
, O
and O
about O
9GB O
memory O
for O
MSugg B-MethodName
(since O
there O
are O
many O
more O
input O
units O
per O
document O
set, O
i.e., O
all O
noun O
phrases O
versus O
sentences). O
B O
Implementation O
Details O
B.1 O
Pre-training O
Technicalities O
To O
provide O
a O
warm O
start O
for O
training O
MSumm B-MethodName
and O
MSugg, B-MethodName
a O
reduced O
version O
of O
the O
models, O
which O
is O
the O
same O
for O
both, O
is O
first O
pre-trained O
for O
generic O
extractive O
single-document O
summarization O
using O
the O
CNN/Daily B-DatasetName
Mail B-DatasetName
corpus I-DatasetName
(Hermann O
et O
al., O
2015) O
with O
about O
287k O
samples, O
as O
proposed O
by O
Chen O
and O
Bansal O
(2018). O
In O
this O
reduced O
model, O
ˆct O
jis O
replaced O
by O
cjin O
Equations O
5, O
7 O
and O
8 O
Further-more, O
there O
is O
a O
single O
reward O
function O
for O
learn- O
ing O
the O
policy, O
computed O
per O
selected O
sentence O
eout O
t O
asROUGE B-MetricName
#NAME? I-MetricName
F1w.r.t. I-MetricName
the O
(single) O
reference O
sum- O
mary’s O
sentence O
at O
index O
t. O
The O
reduced O
model O
pre-trains O
the O
full O
model O
for O
contextual O
sentence O
representation O
and O
for O
salient-sentence O
selection O
in O
the O
single-document O
generic O
setting. O
This O
allows O
training O
MSumm B-MethodName
andMSugg B-MethodName
with O
a O
relatively O
small O
dataset O
for O
their O
final O
purposes. O
B.2 O
Training O
Technicalities O
Following O
(Mao O
et O
al., O
2020), O
the O
pre-trained O
base O
model O
is O
the O
rnn-ext O
=+ O
RL O
model O
from O
Chen O
and O
Bansal O
(2018), O
and O
is O
trained O
like O
in O
Lebanoff O
et O
al. O
(2018). O
Both O
MSumm B-MethodName
andMSugg B-MethodName
are O
further O
trained O
on O
our O
adjusted O
DUC B-DatasetName
2007 I-DatasetName
data O
using O
an O
Adam O
optimizer O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5.00E-04 B-HyperparameterValue
and O
no O
weight B-HyperparameterName
decay. I-HyperparameterName
A O
discount B-HyperparameterName
factor I-HyperparameterName
of O
0.99 B-HyperparameterValue
is O
used O
for O
the O
reinforcement O
learning O
rewards. O
The O
batch B-HyperparameterName
size I-HyperparameterName
was O
8 B-HyperparameterValue
Training O
was O
halted O
once O
30 B-HyperparameterValue
consecutive O
epochs B-HyperparameterName
did O
not O
improve O
the O
validation O
score. O
The O
MMR O
function O
within O
our O
models O
uses O
TF- O
IDF O
vector O
cosine O
similarity O
for O
all O
SIMinstances O
(in O
Equations O
1 O
and O
12). O
The O
TF-IDF O
vectorizer O
is O
initialized O
with O
the O
document O
set O
on O
which O
the O
MMR O
score O
is O
computed. O
As O
is O
commonly O
practiced, O
selection O
of O
an O
out- O
put O
sentence/phrase O
eout O
tis O
done O
by O
sampling O
prob- O
ability O
distribution O
pt(in O
Equation O
8) O
at O
train O
time, O
and O
by O
extracting O
the O
maximum O
scoring O
sen- O
tence/phrase O
at O
inference O
time. O
The O
MLP O
in O
Equation O
3 O
transforms O
the O
MMR O
score O
with O
a O
feed-forward O
network O
with O
one-hidden O
layer O
of O
dimension O
80 O
following O
(Mao O
et O
al., O
2020). O
B.3 O
Query-Assisted O
Summarization O
Model O
Model O
configurations. O
The O
architecture O
of O
the O
MSumm B-MethodName
model O
and O
its O
training O
allowed O
for O
much O
creativity O
in O
the O
configuration O
process. O
Other O
than O
the O
combinations O
mentioned O
in O
the O
paper O
in O
Table O
1, O
we O
also O
experimented O
with O
other O
components. O
We O
list O
here O
many O
of O
the O
experiments, O
without O
for- O
mal O
results. O
Anecdotes O
are O
taken O
by O
looking O
at O
validation O
scores O
and O
some O
eyeballing. O
-1 O
The O
βvalue B-HyperparameterName
in O
the O
query-focused O
MMR O
function O
in O
Equation O
2, O
that O
impacts O
the O
weight O
of O
the O
query O
on O
a O
sentence O
versus O
the O
document O
set O
on O
the O
sentence. O
We O
tried O
out O
a O
few O
βvalues B-HyperparameterName
and O
mainly O
noticed O
that O
a O
value O
of O
0.5kept B-HyperparameterValue
validation O
results O
more O
stable O
across O
configurations, O
or O
kept O
training O
time O
shorter. O
In O
our O
experiments, O
to O
cancel O
out O
this O
component O
(both O
at O
training O
and O
inference2562time), O
we O
simply O
set O
β= B-HyperparameterName
1so B-HyperparameterValue
that O
the O
query O
is O
not O
considered. O
-2 O
Different O
summary O
reward O
functions O
. O
ROUGE B-MetricName
∆recall I-MetricName
(instead O
of O
F1) O
was O
also O
a O
good O
alternative, O
but O
gave O
somewhat O
less O
stable O
results O
across O
configurations. O
ROUGE B-MetricName
(not O
as O
∆) O
was O
also O
less O
stable O
with O
recall O
and O
F1, O
and O
gave O
too O
short O
and O
irrelevant O
sentences O
with O
precision. O
We O
also O
tried O
sentence O
level O
ROUGE B-MetricName
-L, I-MetricName
like O
in O
(Mao O
et O
al., O
2020), O
eventually O
outputting O
sentences O
that O
were O
much O
less O
compliant O
to O
queries. O
-3 O
Using O
only O
the O
query O
similarity O
reward O
in- O
stead O
of O
the O
dual O
reward O
mechanism O
worked O
sur- O
prisingly O
well. O
This O
may O
be O
due O
to O
the O
queries O
on O
which O
the O
model O
was O
trained O
on. O
These O
queries O
were O
very O
relevant O
to O
the O
gold O
reference O
summaries, O
hence O
possibly O
implicitly O
providing O
a O
strong O
signal O
to O
salient O
sentences O
within O
the O
document O
set. O
Still, O
this O
was O
less O
productive O
than O
our O
final O
choice O
of O
reward. O
(4)Adding O
training O
data O
(additional O
DUC B-DatasetName
MDS I-DatasetName
datasets) O
did O
not O
impact O
the O
results. O
Impor- O
tantly, O
since O
DUC B-DatasetName
2007 I-DatasetName
is O
most O
similar O
to O
the O
test O
DUC B-DatasetName
2006 I-DatasetName
set, O
it O
seems O
to O
be O
more O
beneficial O
to O
include O
DUC B-DatasetName
2007 I-DatasetName
in O
the O
training O
set. O
-5 O
We O
also O
tried O
representing O
the O
query O
in O
the O
input O
by O
concatenating O
it’s O
raw O
text O
to O
each O
input O
sentence O
before O
get O
the O
sentence O
representations. O
-6 O
To O
represent O
the O
sentences O
, O
we O
also O
tried O
using O
average O
w2v O
vectors O
(Honnibal O
and O
Montani, O
2021) O
and O
Sentence-BERT O
(Reimers O
and O
Gurevych, O
2019) O
instead O
of O
the O
CNN O
network. O
These O
did O
not O
show O
any O
apparent O
improvements, O
and O
were O
notably O
expensive O
in O
terms O
of O
execution O
time. O
-7 O
For O
the O
sentence O
similarity O
in O
the O
query- O
MMR O
component O
, O
we O
tried O
w2v O
and O
Sentence- O
BERT O
representations O
instead O
of O
TF-IDF O
vectors. O
Similarly O
to O
(6), O
they O
did O
not O
show O
improvements O
over O
using O
TF-IDF, O
and O
were O
very O
time-costly. O
-8 O
Instead O
of O
the O
dual-reward O
mechanism O
that O
alternates O
between O
the O
two O
rewards O
from O
batch O
to O
batch, O
we O
also O
considered O
using O
a O
weighted O
average O
of O
the O
two O
rewards, O
consistently O
over O
all O
batches. O
Further O
experimentation O
is O
required O
on O
this O
tech- O
nique O
for O
a O
more O
conclusive O
judgment. O
Queries O
used O
for O
training. O
The O
queries O
used O
for O
training O
the O
MSumm B-MethodName
model O
can O
affect O
the O
way O
it O
learns O
to O
respond O
to O
a O
query. O
Seemingly, O
the O
most O
natural O
approach O
would O
be O
to O
train O
the O
model O
as O
close O
as O
possible O
to O
the O
model’s O
use O
at O
inference O
time. O
This O
would O
mean O
training O
MSumm B-MethodName
withqueries O
from O
real O
sessions. O
However, O
a O
session’s O
queries O
are O
dependent O
on O
outputs O
previously O
pro- O
duced O
by O
the O
used O
system. O
It O
is O
therefore O
not O
certain O
that O
the O
sequence O
of O
queries O
from O
a O
different O
sys- O
tem’s O
usage O
would O
necessarily O
benefit O
the O
training O
process O
when O
compared O
to O
a O
synthesized O
sequence O
of O
queries. O
I.e., O
it’s O
not O
actually O
possible O
to O
train O
with O
“real O
sessions” O
in O
a O
conventional O
way. O
Also, O
as O
stated O
in O
§3.3, O
the O
synthetic O
queries O
we O
eventually O
used O
direct O
the O
model O
to O
select O
salient O
sentences, O
which O
can O
support O
our O
dual-objectives: O
to O
get O
a O
sentence O
that O
is O
both O
globally O
salient O
to O
the O
topic, O
as O
well O
as O
responsive O
to O
the O
query. O
We O
tried O
training O
on O
other O
query O
types, O
synthesized O
with O
various O
keyphrase O
extraction O
techniques, O
and O
found O
that O
our O
final O
choice O
of O
queries O
more O
consistently O
gave O
good O
results O
overall. O
Sentence O
length. O
We O
segmented O
the O
sentences O
in O
the O
document O
sets O
with O
the O
NLTK8sentence O
tok- O
enizer, O
and O
removed O
sentences O
that O
contain O
quotes O
in O
them O
or O
do O
not O
end O
with O
a O
period. O
During O
training O
we O
did O
not O
constrain O
the O
input O
sentences O
in O
any O
way. O
Some O
of O
the O
configuration O
ex- O
periments O
described O
above O
were O
done O
to O
check O
how O
the O
configuration O
might O
influence O
the O
length O
of O
the O
selected O
sentences. O
The O
best O
configurations, O
includ- O
ing O
the O
one O
we O
eventually O
used O
in O
our O
tests, O
tended O
to O
output O
somewhat O
longer O
sentences. O
Very O
long O
sentences O
are O
usually O
tedious O
for O
human O
readers, O
and O
we O
hence O
limited O
the O
sentences O
to O
30 O
tokens O
at O
inference O
time. O
We O
found O
that O
this O
length O
constraint O
caused O
a O
slight O
degradation O
in O
simulation O
score O
re- O
sults O
of O
our O
models, O
however O
still O
gave O
superior O
informativeness O
results O
compared O
to O
the O
baseline O
system. O
Initial O
summary O
length. O
Sentences O
are O
accumu- O
lated O
until O
surpassing O
75 O
tokens. O
Therefore O
sum- O
maries O
are O
not O
shorter O
than O
75 O
tokens, O
but O
mostly O
not O
much O
longer O
than O
that. O
B.4 O
Suggested O
Queries O
Extraction O
Model O
Model O
configurations. O
We O
experimented O
with O
different O
configurations O
and O
hyper-parameter O
fine- O
tuning O
in O
the O
MSugg O
model O
as O
well. O
Tuning O
was O
performed O
in O
accordance O
to O
the O
validation O
scores O
and O
generic O
keyphrase O
extraction O
scores O
on O
the O
MK-DUC-01 O
multi-document O
keyphrase O
extraction O
dataset O
of O
Shapira O
et O
al. O
(2021a). O
8https://www.nltk.org2563(1) O
In O
the O
reward O
function O
in O
Equation O
13, O
we O
setγ1= O
0.5andγ2= O
0.9, O
i.e., O
the O
preceding O
output O
phrases O
are O
more O
strongly O
accounted O
for O
than O
the O
phrases O
in O
the O
session O
history. O
We O
tested O
several O
values O
between O
0 O
and O
1 O
for O
both O
hyper-parameters. O
-2 O
We O
implemented O
altered O
versions O
of O
the O
re- O
ward O
function O
in O
Equation O
13 O
Instead O
of O
phrase O
unigram-level O
frequency, O
we O
tried O
computing O
the O
full O
phrase O
frequency O
and O
computing O
partial O
phrase O
frequency, O
i.e., O
a O
maximal O
phrase O
template O
match O
within O
a O
reference O
summary. O
All O
functions O
tested O
were O
adequate O
overall, O
though O
our O
final O
choice O
of O
reward O
function O
was O
closest O
to O
the O
keyphrase O
ex- O
traction O
task O
unigram O
overlap O
metric, O
and O
gave O
best O
results O
overall. O
-3 O
We O
also O
attempted O
noun O
phrase O
extraction O
with O
the O
spaCy9noun O
chunker O
and O
named O
entity O
recognizer. O
This O
combined O
approach O
misses O
some O
noun O
phrases O
within O
the O
text, O
but O
mainly O
is O
also O
more O
computationally O
heavy O
than O
the O
simple O
POS O
regex O
search O
that O
we O
use. O
Extracting O
phrases O
with O
regular-expression. O
We O
extracted O
all O
noun-phrases O
from O
the O
docu- O
ment O
set O
by O
first O
mapping O
all O
tokens O
to O
their O
part-of-speech O
tags, O
and O
then O
applying O
a O
regular- O
expression O
chunker O
with O
regex: O
{(<JJ> O
* O
<NN. O
*>+ O
<IN>)? O
<JJ> O
*<NN. O
*>+}. O
These O
steps O
were O
accomplished O
with O
NLTK. O
Phrase O
length. O
There O
is O
no O
limit O
set O
on O
the O
phrase O
length. O
We O
tried O
training O
and O
inferring O
with O
a O
phrase O
length O
constraint O
of O
4 O
words, O
but O
found O
that O
this O
gave O
worse O
results O
overall. O
History O
sentences O
to O
phrases. O
MSugg B-MethodName
works O
on O
thephrase O
level. O
Meanwhile, O
in O
our O
extractive O
in- O
teractive O
setting, O
the O
history O
is O
a O
set O
of O
sentences O
already O
presented O
to O
the O
reader. O
Therefore, O
when O
extracting O
phrases O
from O
D, O
we O
also O
link O
each O
phrase O
to O
its O
source O
sentence, O
and O
obtain O
Einby O
compiling O
the O
phrases O
linked O
from O
the O
history O
sentences. O
C O
Dataset O
Notes O
While O
DUC B-DatasetName
2006 I-DatasetName
(our O
test O
set) O
and O
2007 O
(our O
train/validation O
set) O
were O
originally O
designed O
for O
the O
query-focused O
summarization O
task, O
they O
con- O
tain O
excessive O
topic O
concentration O
due O
to O
their O
long O
and O
descriptive O
topic O
queries O
(Baumel O
et O
al., O
2016). O
Hence, O
their O
reference O
summaries O
can O
practically O
be O
considered O
generic. O
9https://spacy.io/D O
Session O
Collection O
Controlled O
crowdsourcing O
protocol. O
We O
fol- O
lowed O
the O
controlled O
crowdsourcing O
protocol O
of O
Shapira O
et O
al. O
(2021b), O
which O
includes O
three O
steps: O
-1 O
a O
trap O
task O
for O
finding O
qualified O
"workers;" O
-2 O
practice O
tasks O
for O
explaining O
the O
interface O
and O
the O
purpose, O
as O
well O
as O
reiterating O
the O
generic O
infor- O
mation O
goal O
(see O
below) O
during O
"exploration;" O
-3 O
the O
session O
collection O
tasks. O
We O
used O
the O
Amazon O
Mechanical O
Turk O
HITs O
prepared O
by O
Shapira O
et O
al. O
(2021b). O
Process O
cost. O
We O
paid O
$0.40 O
for O
a O
trap O
task O
assign- O
ment, O
with O
400 O
assignments O
released, O
and O
$0.90 O
for O
a O
practice O
task O
assignment, O
with O
28 O
assignments O
completed. O
The O
session O
collection O
assignment O
paid O
$0.70, O
and O
a O
bonus O
mainly O
according O
to O
the O
length O
of O
interaction O
and O
additional O
comments O
provided. O
The O
bonus O
was O
between O
$0.15 O
and O
$0.35. O
A O
total O
of O
111 B-MethodName
sessions O
were O
recorded O
from O
6 O
high O
qual- O
ity O
workers. O
The O
full O
process O
cost O
about O
$385 O
in O
total O
(including O
the O
Mechanical O
Turk O
fees) O
for O
the O
experiment O
including O
configuration O
vin O
Table O
1 O
The O
second O
round O
of O
experiments O
done O
on O
an- O
other O
variant O
of O
our O
system O
(configuration O
i) O
also O
included O
28 O
practice O
tasks O
and O
compiled O
10 O
fi- O
nal O
workers O
for O
a O
total O
of O
180 O
collected O
sessions. O
Bonuses O
ranged O
from O
$0.10 O
and O
$0.40 O
on O
the O
ses- O
sion O
collection O
task. O
The O
full O
process O
cost O
of O
the O
second O
experiment O
was O
about O
$475 O
in O
total O
(includ- O
ing O
the O
Mechanical O
Turk O
fees). O
Session O
collection O
data O
preparation. O
We O
used O
the O
same O
20 O
test O
topics O
as O
Shapira O
et O
al. O
(2021b), O
and O
created O
2 O
batches O
of O
tasks. O
For O
the O
first O
batch, O
in O
alternating O
order O
of O
topics, O
10 O
topics O
were O
paired O
with O
our O
system, O
and O
the O
other O
10 O
were O
paired O
with O
theS2baseline. O
The O
other O
batch O
consisted O
of O
the O
complementing O
topic-system O
pairings. O
The O
work- O
ers O
were O
assigned O
a O
batch O
to O
work O
on O
such O
that O
half O
of O
the O
workers O
would O
work O
on O
each O
batch. O
User O
informational O
goal. O
Since O
all O
sessions O
on O
a O
topic O
are O
evaluated O
against O
the O
same O
reference O
summaries, O
it O
is O
important O
that O
users O
aim O
to O
ex- O
plore O
similar O
information. O
Following O
Shapira O
et O
al. O
(2021b), O
during O
practice O
tasks O
all O
users O
received O
a O
common O
informational O
goal O
to O
follow, O
so O
that O
the O
sessions O
are O
comparable. O
The O
emphasized O
descrip- O
tion O
was: O
“produce O
an O
informative O
summary O
draft O
text O
which O
a O
journalist O
could O
use O
to O
best O
produce O
an O
overview O
of O
the O
topic”.2564Sessions O
filtering. O
In O
the O
first O
experiment, O
we O
filtered O
out O
7 O
sessions O
that O
accumulated O
less O
than O
250 O
tokens O
(from O
2 O
different O
workers). O
In O
the O
second O
experiment, O
9 O
of O
the O
10 O
workers O
completed O
at O
least O
19 O
of O
the O
20 O
topics O
One O
worker O
completed O
only O
3 O
tasks O
and O
we O
disregarded O
those O
sessions. O
We O
also O
threw O
away O
9 O
sessions O
that O
accu- O
mulated O
less O
than O
250 O
tokens. O
INTSUMM B-TaskName
user O
interface. O
We O
used O
the O
same O
user O
interface O
developed O
by O
Shapira O
et O
al. O
(2021b) O
with O
a O
small O
change O
to O
enable O
suggested O
query O
list O
updates O
after O
each O
interaction O
(the O
interface O
was O
designed O
for O
the O
baselines, O
where O
the O
suggested- O
query O
list O
is O
static). O
To O
refrain O
from O
any O
possible O
user O
experience O
bias, O
we O
made O
the O
UI O
change O
as O
least O
apparent O
as O
possible. O
System O
response O
time. O
MSumm B-MethodName
is O
able O
to O
gen- O
erate O
summaries O
mostly O
in O
under O
a O
second, O
and O
MSugg B-MethodName
prepares O
the O
list O
in O
a O
few O
seconds. O
The O
summary O
expansion O
is O
hence O
presented O
to O
the O
user O
almost O
immediately O
after O
query O
submission, O
and O
the O
suggested O
queries O
list O
is O
shown O
shortly O
afterwords, O
before O
the O
user O
finishes O
reading O
the O
expansion. O
The O
small O
delay O
in O
suggested O
query O
updating O
is O
hence O
al- O
most O
unnoticed. O
The O
baseline O
summarizer O
responds O
similarly O
fast O
to O
MSumm B-MethodName
, O
making O
response-time O
difference O
unperceivable O
between O
the O
systems. O
User O
feedback. O
Many O
of O
the O
users O
provided O
feed- O
back O
about O
the O
session O
collection O
tasks O
after O
finish- O
ing O
their O
assignment O
batch. O
The O
overall O
impression O
was O
that O
there O
was O
no O
strong O
preference O
for O
either O
system. O
For O
example, O
one O
user O
wrote: O
“I O
did O
not O
discern O
a O
consistent O
difference O
between O
the O
two O
systems O
that O
would O
result O
in O
having O
a O
clear O
pref- O
erence. O
” O
This O
kind O
of O
comment O
was O
repeated O
by O
several O
users. O
Generally, O
there O
were O
no O
explicit O
comments O
about O
the O
difference O
in O
quality O
of O
the O
summary O
outputs, O
and O
topics O
were O
mostly O
scored O
or O
commented O
on O
similarly O
between O
the O
two O
sys- O
tems O
since O
the O
complexity O
of O
the O
topic O
influenced O
the O
ability O
of O
the O
systems O
to O
comply O
to O
the O
user. O
A O
comment O
in O
favor O
of O
updating O
suggested O
queries O
during O
interaction O
said: O
“It O
was O
nice O
to O
have O
a O
new O
list O
as O
you O
progressed O
through O
the O
task, O
it O
helped O
me O
think O
of O
where O
to O
go O
next O
if O
I O
got O
stuck... O
” O
This O
specific O
comment O
was O
written O
by O
a O
user O
that O
explored O
topics O
quite O
deeply. O
On O
the O
other O
hand, O
a O
user O
that O
explored O
more O
shallow O
liked O
that O
used O
suggested O
queries O
in O
the O
static O
list O
were O
marked: O
“I O
did O
notice...the O
red O
font O
color O
on O
the O
used O
queries.That O
was O
helpful. O
” O
It O
therefore O
seems O
that O
updating O
suggested O
queries O
are O
more O
useful O
for O
lengthy O
ex- O
ploration, O
but O
for O
quick O
navigation, O
the O
static O
list O
might O
naturally O
be O
enough. O
E O
More O
Results O
E.1 O
Overall O
System O
Assessment O
We O
conducted O
two O
comparative O
session O
collec- O
tion O
and O
analysis O
experiments, O
one O
using O
MSumm B-MethodName
model O
configuration O
v(from O
Table O
1), O
as O
presented O
in O
§5.3 O
and O
Table O
2, O
and O
another O
with O
MSumm B-MethodName
model O
configuration O
i. O
As O
explained O
in O
§5.2, O
these O
two O
configurations O
performed O
best, O
on O
simulations, O
out O
of O
their O
respective O
configuration O
sets. O
We O
show O
here O
results O
of O
the O
second O
experiment, O
where O
we O
used O
MSumm B-MethodName
model O
configuration O
i, O
with O
the O
same O
MSugg B-MethodName
model O
as O
in O
the O
first O
experiment. O
TheS2baseline O
was O
similarly O
used O
for O
compari- O
son. O
We O
also O
kept O
the O
same O
AUC O
length O
limits O
(106 O
to O
250 O
tokens) O
for O
easy O
comparability O
to O
Table O
2 O
Table O
3 O
shows O
the O
results. O
Here O
too, O
while O
less O
substantially, O
informativeness O
is O
improved O
with O
our O
system O
without O
significantly O
harming O
the O
user O
expe- O
rience. O
Overall, O
it O
seems O
that O
users O
were O
somewhat O
more O
satisfied O
with O
the O
INTSUMM B-TaskName
system O
that O
uses O
MSumm B-MethodName
configuration O
vthan O
configuration O
i. O
Inter- O
estingly, O
it O
seems O
the O
users O
may O
have O
appreciated O
the O
slightly O
better O
informativeness O
of O
configuration O
veven O
if O
the O
query-responsiveness O
was O
not O
as O
good O
as O
in O
configuration O
i, O
as O
shown O
through O
the O
QSIM B-MetricName
score. I-MetricName
In O
addition, O
we O
see O
that O
absolute O
manual O
scores O
in O
Table O
3 O
are O
lower O
than O
in O
Table O
2, O
but O
trends O
are O
generally O
similar. O
It O
is O
common O
that O
scal- O
ing O
of O
manually O
supplied O
scores O
can O
fluctuate O
(e.g. O
Gillick O
and O
Liu, O
2010). O
Figures O
3 O
and O
4 O
show O
the O
averaged O
(per O
topic O
and O
then O
over O
all O
topics) O
recall O
curves O
of O
the O
collected2565sessions O
in O
the O
experiment O
described O
in O
§5.3 O
and O
above, O
respectively. O
The O
x-axis O
is O
the O
accumulat- O
ing O
token-length O
of O
the O
session, O
and O
the O
y-axis O
is O
theROUGE B-MetricName
-1 I-MetricName
recall. I-MetricName
The O
points O
on O
the O
curve O
are O
the O
average O
interpolated O
values O
from O
all O
the O
ses- O
sions. O
The O
vertical O
dashed O
lines O
are O
the O
intersecting O
bounds O
of O
the O
sessions, O
from O
106 O
tokens O
to O
250 O
The O
area B-MetricName
under I-MetricName
the I-MetricName
curve I-MetricName
(AUC) I-MetricName
is O
computed O
for O
each O
of O
the O
curves, O
and O
reported O
in O
the O
first O
row O
of O
Tables O
2 O
and O
3 O
The O
higher O
AUC B-MetricName
scores O
obtained O
from O
the O
recall O
curves O
of O
our O
models, O
compared O
to O
those O
of O
the O
S2baseline, O
highlight O
the O
ability O
to O
expose O
more O
salient O
information O
earlier O
in O
the O
session. O
E.2 O
Execution O
Time O
of O
Systems O
Systems O
that O
are O
made O
for O
interacting O
with O
humans O
must O
respond O
quickly O
in O
order O
to O
keep O
the O
user’s O
engagement. O
The O
exact O
amount O
of O
time O
does O
not O
affect O
the O
user O
experience O
as O
long O
as O
it O
does O
not O
surpass O
some O
limit, O
after O
which O
the O
user O
starts O
los- O
ing O
interest O
or O
feeling O
irritated O
(Attig O
et O
al., O
"2017;" O
Anderson, O
2020). O
As O
mentioned O
in O
Appendix O
D, O
MSumm B-MethodName
generates O
summaries O
in O
under O
a O
second O
and O
MSugg B-MethodName
prepares O
the O
list O
in O
a O
few O
seconds. O
The O
baseline O
summarizer O
also O
responds O
in O
under O
a O
second. O
The O
difference O
between O
the O
systems O
is O
virtually O
unperceivable O
dur- O
ing O
interaction. O
There O
were O
no O
comments O
from O
the O
users O
in O
our O
experiments O
that O
stated O
any O
issue O
with O
execution O
time. O
E.3 O
Assessment O
of O
Suggested O
Queries O
Functionality O
In O
this O
analysis, O
we O
assessed O
what O
modes O
of O
query O
submission O
users O
relied O
on O
over O
the O
course O
of O
a O
session. O
To O
that O
end, O
-1 O
we O
divided O
each O
session O
to O
three O
segments O
(first, O
second O
and O
third O
part O
of O
the O
session), O
and O
counted O
the O
types O
of O
queries. O
The O
types O
are O
“suggested O
query”, O
“free-text”, O
“highlight” O
(a O
span O
from O
the O
summary O
text) O
and O
“repeat” O
(re- O
peating O
the O
last O
submitted O
query). O
-2 O
We O
then O
com- O
puted O
the O
percentage O
of O
each O
mode O
in O
each O
segment. O
-3 O
The O
percentages O
over O
all O
sessions O
and O
all O
topics O
were O
computed O
for O
each O
of O
the O
three O
segments. O
This O
process O
was O
conducted O
only O
for O
sessions O
between O
4 O
and O
20 O
interactions, O
as O
the O
few O
long O
and O
short O
sessions O
often O
show O
different O
behavior. O
For O
the O
first O
experiment, O
this O
left O
43 O
sessions O
with O
avg. O
8.63 O
(std. O
2.32) O
interactions O
for O
our O
system, O
and O
50 O
sessions O
with O
8.44 O
-2.48 O
interaction O
for O
S2. O
For O
the O
second O
experiment, O
it O
left O
72 O
sessions O
with O
10.24 O
-4.82 O
interactions O
for O
our O
system, O
and O
74 O
sessions O
with O
9.59 O
-4.42 O
interactions O
for O
S2. O
We O
focus O
here O
on O
the O
use O
of O
suggested O
queries O
versus O
all O
other O
query O
types. O
In O
the O
first O
experiment O
we O
observe O
a O
change O
of O
0.09 O
from O
the O
first O
to O
the O
third O
segment O
in O
our O
system, O
and O
-0.2 O
in O
S2. O
In O
the O
second O
experiment O
we O
see O
0.18 O
and O
-0.28 O
in O
S2. O
As O
discussed O
in O
§5.3, O
this O
suggests O
the O
effectiveness O
of O
updated O
suggested O
queries, O
especially O
by O
the O
end O
of O
a O
session. O
Further O
Explanations O
on O
Evaluation O
Metrics O
The O
normalized O
AUC B-MetricName
score O
for O
the O
validation O
metric O
(explained O
in O
§3.3) O
is O
computed O
over O
the O
recall O
curve O
produced O
from O
the O
accumulating O
sum- O
mary O
expansions. O
Each O
point O
on O
the O
curve O
marks O
an O
accumulating O
token-length O
(x-axis) O
and O
an O
accu- O
mulating O
recall O
score O
(y-axis) O
of O
an O
interactive O
state, O
as O
depicted O
in O
Figures O
3 O
and O
4 O
(although O
these O
fig- O
ures O
show O
the O
averaged O
session O
recall O
curves O
with O
bounds, O
whereas O
during O
validation O
the O
curve O
is O
for O
a O
single O
session O
and O
there O
are O
no O
bounds O
set). O
By O
computing O
the O
area O
under O
the O
full O
curve, O
and O
divid- O
ing O
by O
the O
full O
length, O
the O
normalized O
AUC B-MetricName
score O
is O
obtained. O
The O
normalization O
gives O
an O
approximate O
absolute O
value O
that O
can O
be O
compared O
at O
different O
lengths O
(although O
at O
large O
length O
differences O
this O
is O
not O
comparable O
due O
to O
the O
decaying O
slope O
of O
the O
curve). O
The O
manual O
query-responsiveness O
score O
, O
re- O
ported O
in O
Tables O
2 O
and O
3, O
is O
obtained O
by O
asking O
users, O
at O
the O
end O
of O
a O
session, O
“During O
the O
inter- O
active O
stage, O
how O
well O
did O
the O
responses O
respond O
to O
your O
queries?”, O
for O
which O
they O
rate O
on O
a O
1-to-5 O
scale. O
The O
scores O
are O
averaged O
over O
the O
topic O
and O
then O
over O
all O
topics. O
This O
follows O
the O
evaluation O
defined O
in O
Shapira O
et O
al. O
(2021b). O
TheUMUX-Lite O
score O
(Lewis O
et O
al., O
2013), O
re- O
ported O
in O
Tables O
2 O
and O
3, O
is O
obtained O
by O
asking O
users O
to O
rate O
(1-to-5) O
two O
statements O
at O
the O
end O
of O
a O
session: O
-1 O
“The O
system’s O
capabilities O
meet O
the O
need O
to O
efficiently O
collect O
useful O
information O
for O
a O
journalistic O
overview” O
and O
-2 O
“The O
system O
is O
easy O
to O
use”. O
The O
first O
question O
refers O
to O
the O
users’ O
informational O
goal O
that O
they O
received, O
in O
order O
to O
follow O
a O
consistent O
objective O
goal O
during O
their O
ex- O
ploration. O
The O
final O
score O
is O
a O
function O
of O
these O
two O
scores, O
and O
is O
used O
as O
a O
replacement O
for O
the O
popular O
SUS O
metric O
(Brooke, O
1996) O
(with O
a O
much O
longer O
questionnaire), O
to O
which O
it O
shows O
very O
high O
cor- O
relation, O
thus O
offering O
a O
cheaper O
alternative. O
This O
also O
follows O
the O
evaluation O
defined O
in O
Shapira O
et O
al. O
(2021b). O
Allconfidence O
intervals O
in O
Tables O
1, O
2 O
and O
3 O
are O
computed O
as O
margins-of-error, O
on O
the O
topic- O
level, O
over O
the O
standard O
error O
of O
the O
mean O
with O
95% O
confidence.10 O
Thetoken-length O
values O
in O
Table O
1 O
are O
averages O
with O
standard O
deviations. O
10E.g., O
see O
https://www.calculator.net/ O
standard-deviation-calculator.htmlG O
A2C O
Policy O
Learning O
A O
policy O
gradient-based O
reinforcement O
learning O
ap- O
proach O
(Williams, O
1992) O
allows O
optimizing O
on O
non- O
differentiable O
metrics, O
and O
eliminates O
the O
exposure O
bias O
that O
occurs O
with O
traditional O
training O
methods, O
like O
cross-entropy, O
on O
generation O
tasks O
(Ranzato O
et O
al., O
2016). O
Specifically, O
we O
use O
the O
Advantage O
Actor O
Critic O
(A2C) O
policy O
gradient O
training O
method. O
See O
tech- O
nical O
explanations O
in O
the O
appendix O
of O
(Chen O
and O
Bansal, O
2018). O
At O
a O
high O
level, O
an O
output O
reward O
(subtracted O
by O
a O
baseline O
reward O
– O
computed O
on O
a O
version O
of O
the O
model O
without O
MMR O
attention) O
is O
used O
to O
weight O
the O
output O
selection O
in O
the O
loss O
func- O
tion. O
In O
so, O
outputs O
with O
higher O
rewards O
increase O
the O
likelihood O
of O
those O
outputs O
and O
lower O
rewards O
decrease O
the O
likelihood. O
Since O
the O
reward O
function O
is O
not O
differentiable, O
it O
is O
used O
as O
a O
weight O
on O
the O
probability O
of O
the O
selected O
output, O
which O
is O
then O
given O
to O
the O
loss O
function. O
H O
I O
NTSUMM O
Example O
We O
show O
in O
Figure O
5 O
an O
example O
of O
an O
INTSUMM B-TaskName
system O
using O
the O
web O
application O
of O
Shapira O
et O
al. O
(2021b) O
and O
our O
our O
MSumm B-MethodName
(configuration O
ifrom O
Table O
1) O
and O
MSugg B-MethodName
models O
in O
the O
backend. O
