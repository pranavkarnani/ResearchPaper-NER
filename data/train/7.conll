Proceedings O
of O
NAACL O
HLT O
2021: O
IndustryTrack O
Papers O
, O
pages O
154–162 O
June O
6–11, O
2021. O
©2021 O
Association O
for O
Computational O
Linguistics154Query2Prod2Vec B-MethodName
Grounded B-TaskName
Word I-TaskName
Embeddings I-TaskName
for I-TaskName
eCommerce I-TaskName
Federico O
Bianchi O
Bocconi O
University O
Milano, O
Italy O
f.bianchi@unibocconi.itJacopo O
Tagliabue O
Coveo O
Labs O
New O
York, O
USA O
jtagliabue@coveo.comBingqing O
Yu O
Coveo O
Montreal, O
Canada O
cyu2@coveo.com O
Abstract O
We O
present O
Query2Prod2Vec B-MethodName
, O
a O
model O
that O
grounds O
lexical B-TaskName
representations I-TaskName
for I-TaskName
product I-TaskName
search I-TaskName
in I-TaskName
product I-TaskName
embeddings: I-TaskName
in O
our O
model, O
meaning O
is O
a O
mapping O
between O
words O
and O
a O
latent O
space O
of O
products O
in O
a O
digital O
shop. O
We O
leverage O
shopping O
sessions O
to O
learn O
the O
un- O
derlying O
space O
and O
use O
merchandising O
anno- O
tations O
to O
build O
lexical O
analogies O
for O
evalua- O
tion: O
our O
experiments O
show O
that O
our O
model O
is O
more O
accurate O
than O
known O
techniques O
from O
the O
NLP O
and O
IR O
literature. O
Finally, O
we O
stress O
the O
importance O
of O
data O
efﬁciency O
for O
product O
search O
outside O
of O
retail O
giants, O
and O
highlight O
how O
Query2Prod2Vec B-MethodName
ﬁts O
with O
practical O
con- O
straints O
faced O
by O
most O
practitioners. O
1 O
Introduction O
The O
eCommerce O
market O
reached O
in O
recent O
years O
an O
unprecedented O
scale: O
in O
2020, O
3.9 O
trillion O
dol- O
lars O
were O
spent O
globally O
in O
online O
retail O
(Cramer- O
Flood, O
2020). O
While O
shoppers O
make O
signiﬁcant O
use O
of O
search O
functionalities, O
improving O
their O
ex- O
perience O
is O
a O
never-ending O
quest O
(Econsultancy, O
2020), O
as O
outside O
of O
few O
retail O
giants O
users O
complain O
about O
sub-optimal O
performances O
(Baymard O
Insti- O
tute, O
2020). O
As O
the O
technology O
behind O
the O
indus- O
try O
increases O
in O
sophistication, O
neural O
architectures O
are O
gradually O
becoming O
more O
common O
(Tsagkias O
et O
al., O
2020) O
and, O
with O
them, O
the O
need O
for O
accu- O
rate O
word O
embeddings O
for O
Information O
Retrieval O
(IR) O
and O
downstream O
Natural O
Language O
Processing O
(NLP) O
tasks O
(Yu O
and O
Tagliabue, O
2020; O
Tagliabue O
et O
al., O
2020a). O
Unfortunately, O
the O
success O
of O
standard O
and O
contextual O
embeddings O
from O
the O
NLP O
litera- O
ture O
(Mikolov O
et O
al., O
2013a; O
Devlin O
et O
al., O
2019) O
could O
not O
be O
immediately O
translated O
to O
the O
prod- O
uct O
search O
scenario, O
due O
to O
some O
peculiar O
chal- O
lenges O
(Bianchi O
et O
al., O
2020b), O
such O
as O
short O
text, O
Corresponding O
author. O
All O
authors O
contributed O
equally O
and O
are O
listed O
alphabetically.industry-speciﬁc O
jargon O
(Bai O
et O
al., O
2018), O
low- O
resource O
languages; O
moreover, O
speciﬁc O
embedding O
strategies O
have O
often O
been O
developed O
in O
the O
con- O
text O
of O
high-trafﬁc O
websites O
(Grbovic O
et O
al., O
2016), O
which O
limit O
their O
applicability O
in O
many O
practical O
sce- O
narios. O
In O
thiswork, O
we O
propose O
a O
sample O
efﬁcient O
word B-TaskName
embedding I-TaskName
method I-TaskName
for I-TaskName
IR I-TaskName
in I-TaskName
eCommerce, I-TaskName
and O
benchmark O
it O
against O
SOTA O
models O
over O
industry O
data O
provided O
by O
partnering O
shops. O
We O
summarize O
our O
contributions O
as O
follows: O
1.we O
propose O
a O
method O
to O
learn O
dense O
represen- O
tations O
of O
words O
for O
eCommerce: O
we O
name O
our O
method O
Query2Prod2Vec B-MethodName
, O
as O
the O
map- O
ping O
between O
words O
and O
the O
latent O
space O
is O
mediated O
by O
the O
product O
domain; O
2.we O
evaluate O
the O
lexical O
representations O
learned O
byQuery2Prod2Vec B-MethodName
on O
an O
analogy O
task O
against O
SOTA O
models O
in O
NLP O
and O
IR; O
bench- O
marks O
are O
run O
on O
two O
independent O
shops, O
dif- O
fering O
in O
trafﬁc, O
industry O
and O
catalog O
size; O
3.we O
detail O
a O
procedure O
to O
generate O
synthetic O
em- O
beddings, O
which O
allow O
us O
to O
tackle O
the O
“cold O
start” O
challenge; O
4.we O
release O
our O
implementations, O
to O
help O
the O
community O
with O
the O
replication O
of O
our O
ﬁnd- O
ings O
on O
other O
shops1. O
While O
perhaps O
not O
fundamental O
to O
its O
industry O
signiﬁcance, O
it O
is O
important O
to O
remark O
that O
grounded O
lexical O
learning O
is O
well O
aligned O
with O
theoretical O
con- O
siderations O
on O
meaning O
in O
recent O
(and O
less O
recent) O
literature O
(Bender O
and O
Koller, O
2020; O
Bisk O
et O
al., O
2020; O
Montague, O
1974). O
1Public O
repository O
available O
at: O
https://github. O
com/coveooss/ecommerce-query-embeddings O
.1552 O
Embeddings B-TaskName
for I-TaskName
Product I-TaskName
Search: I-TaskName
an O
Industry O
Perspective O
In O
product O
search, O
when O
the O
shopper O
issues O
a O
query O
(e.g. O
“sneakers”) O
on O
a O
shop, O
the O
shop O
search O
engine O
returns O
a O
list O
of O
Kproducts O
matching O
the O
query O
intent O
and O
possibly O
some O
contextual O
factor O
– O
the O
shopper O
at O
that O
point O
may O
either O
leave O
the O
website, O
or O
click O
on O
nproducts O
to O
further O
explore O
the O
offer- O
ing O
and O
eventually O
make O
a O
purchase. O
Unlike O
web O
search, O
which O
is O
exclusively O
per- O
formed O
at O
massive O
scale, O
product B-TaskName
search I-TaskName
is O
a O
prob- O
lem O
that O
both O
big O
and O
small O
retailers O
have O
to O
solve: O
while O
word O
embeddings O
have O
revolutionized O
many O
areas O
of O
NLP O
(Mikolov O
et O
al., O
2013a), O
word B-TaskName
embed- I-TaskName
dings I-TaskName
for I-TaskName
product I-TaskName
queries I-TaskName
are O
especially O
challenging O
to O
obtain O
at O
scale, O
when O
considering O
the O
huge O
vari- O
ety O
of O
use O
cases O
in O
the O
overall O
eCommerce O
industry. O
In O
particular, O
based O
on O
industry O
data O
and O
ﬁrst-hand O
experience O
with O
dozens O
of O
shops O
in O
our O
network, O
we O
identify O
four O
constraints O
for O
effective B-TaskName
word I-TaskName
em- I-TaskName
beddings I-TaskName
in I-TaskName
eCommerce: I-TaskName
1.Short O
text O
. O
Most O
product O
queries O
are O
very O
short O
– O
60% O
of O
all O
queries O
in O
our O
dataset O
are O
one-word O
queries, O
> O
80% O
are O
two O
words O
or O
less; O
the O
advantage O
of O
contextualized O
embed- O
dings O
may O
therefore O
be O
limited, O
while O
lexical O
vectors O
are O
fundamental O
for O
downstream O
NLP O
tasks O
(Yu O
and O
Tagliabue, O
2020; O
Bianchi O
et O
al., O
2020a). O
For O
this O
reason, O
the O
current O
work O
speciﬁcally O
addresses O
the O
quality O
of O
word O
em- O
beddings2. O
2.Low-resource O
languages O
. O
Even O
shops O
that O
have O
the O
majority O
of O
their O
trafﬁc O
on O
English O
domain O
typically O
have O
smaller O
shops O
in O
low- O
resource O
languages. O
3.Data O
sparsity O
. O
InShop O
X O
below, O
only O
9% O
of O
all O
shopping O
sessions O
have O
a O
search O
interac- O
tion3. O
Search O
sparsity, O
coupled O
with O
vertical- O
speciﬁc O
jargon O
and O
the O
usual O
long O
tail O
of O
search O
queries, O
makes O
data-hungry O
models O
unlikely O
to O
succeed O
for O
most O
shops. O
2Irrespectively O
of O
how O
the O
lexical O
vectors O
are O
computed, O
query O
embeddings O
can O
be O
easily O
recovered O
with O
the O
usual O
techniques O
(e.g. O
sum O
or O
average O
word O
embeddings O
(Yu O
et O
al., O
2020)): O
as O
we O
mention O
in O
the O
concluding O
remarks, O
investi- O
gating O
compositionality O
is O
an O
important O
part O
of O
our O
overall O
research O
agenda. O
3This O
is O
a O
common O
trait O
veriﬁed O
across O
industries O
and O
sizes: O
among O
dozens O
of O
shops O
in O
our O
network, O
30% O
is O
the O
highest O
search O
vs O
no-search O
session O
ratio; O
Shop O
Y O
below O
is O
around O
29%.4.Computational O
capacity O
. O
The O
majority O
of O
the O
market O
has O
the O
necessity O
to O
strike O
a O
good O
trade-off O
between O
quality O
of O
lexical O
represen- O
tations O
and O
the O
cost O
of O
training O
and O
deploying O
models, O
both O
as O
hardware O
expenses O
and O
as O
ad- O
ditional O
maintenance/training O
costs. O
The O
embedding O
strategy O
we O
propose O
– O
Query2Prod2Vec B-MethodName
– O
has O
been O
designed O
to O
allow O
efﬁcient B-TaskName
learning I-TaskName
of I-TaskName
word I-TaskName
embeddings I-TaskName
for I-TaskName
product I-TaskName
queries. I-TaskName
Our O
ﬁndings O
are O
useful O
to O
a O
wide O
range O
of O
practitioners: O
large O
shops O
launching O
in O
new O
languages/countries, O
mid-and-small O
shops O
transitioning O
to O
dense O
IR O
architectures O
and O
the O
raising O
wave O
of O
multi-tenant O
players4: O
as O
A.I. O
providers O
grow O
by O
deploying O
their O
solutions O
on O
multiple O
shops, O
“cold O
start” O
scenarios O
are O
an O
important O
challenge O
to O
the O
viability O
of O
their O
business O
model. O
3 O
Related O
Work O
The O
literature O
on O
learning B-TaskName
representations I-TaskName
for I-TaskName
lex- I-TaskName
ical I-TaskName
items I-TaskName
in O
NLP O
is O
vast O
and O
growing O
fast; O
as O
an O
overview O
of O
classical O
methods, O
Baroni O
et O
al. O
(2014) O
benchmarks O
several O
count-based O
and O
neural O
techniques O
(Landauer O
and O
Dumais, O
1997; O
Mikolov O
et O
al., O
2013b); O
recently, O
context-aware O
embed- O
dings O
(Peters O
et O
al., O
2018; O
Devlin O
et O
al., O
2019) O
have O
demonstrated O
state-of-the-art O
performances O
in O
several O
semantic O
tasks O
(Rogers O
et O
al., O
2020; O
Nozza O
et O
al., O
2020), O
including O
document-based O
search O
(Nogueira O
et O
al., O
2020), O
in O
which O
target O
entities O
are O
long O
documents, O
instead O
of O
prod- O
uct O
(Craswell O
et O
al., O
2020). O
To O
address O
IR-speciﬁc O
challenges, O
other O
embedding O
strategies O
have O
been O
proposed: O
Search2Vec O
(Grbovic O
et O
al., O
2016) O
uses O
interactions O
with O
ads O
and O
pages O
as O
context O
in O
the O
typical O
context-target O
setting O
of O
skip-gram O
mod- O
els O
(Mikolov O
et O
al., O
2013b); O
QueryNGram2Vec O
(Bai O
et O
al., O
2018) O
additionally O
learns O
embeddings O
for O
n- O
grams O
of O
word O
appearing O
in O
queries O
to O
better O
cover O
the O
long O
tail. O
The O
idea O
of O
using O
vectors O
(from O
im- O
ages) O
as O
an O
aid O
to O
query O
representation O
has O
also O
been O
suggested O
as O
a O
heuristic O
device O
by O
Yu O
et O
al. O
(2020), O
in O
the O
context O
of O
personalized O
language O
models; O
thiswork O
is O
the O
ﬁrst O
to O
our O
knowledge O
to O
benchmark O
embeddings O
on O
lexical O
semantics O
(not O
4As O
an O
indication O
of O
the O
market O
opportunity, O
only O
in O
2019 O
and O
only O
in O
the O
space O
of O
AI-powered O
search O
and O
recommenda- O
tions, O
we O
witnessed O
Coveo O
(Techcrunch), O
Algolia O
(Techcrunch, O
2019a) O
and O
Lucidworks O
(Techcrunch, O
2019b) O
raising O
more O
than O
100M O
USD O
each O
from O
venture O
funds.156tuned O
for O
domain-speciﬁc O
tasks), O
andinvestigate O
sample O
efﬁciency O
for O
small-data O
contexts. O
4 O
Query2Prod2Vec B-MethodName
InQuery2Prod2Vec B-MethodName
, O
the O
representation O
for O
a O
query O
qis O
built O
through O
the O
representation O
of O
the O
objects O
that O
qrefers O
to. O
Consider O
a O
typical O
shopper- O
engine O
interaction O
in O
the O
context O
of O
product O
search: O
the O
shopper O
issues O
a O
query, O
e.g. O
“shoes”, O
the O
en- O
gine O
replies O
with O
a O
noisy O
set O
of O
potential O
refer- O
ents, O
e.g. O
pairs O
of O
shoes O
from O
the O
shop O
inventory, O
among O
which O
the O
shopper O
may O
select O
relevant O
items. O
Hence, O
this O
dynamics O
is O
reminiscent O
of O
a O
coopera- O
tive O
language O
game O
(Lewis, O
1969), O
in O
which O
shop- O
pers O
give O
noisy O
feedback O
to O
the O
search O
engine O
on O
the O
meaning O
of O
the O
queries. O
A O
full O
speciﬁcation O
ofQuery2Prod2Vec B-MethodName
therefore O
involves O
a O
represen- O
tation O
of O
the O
target O
domain O
of O
reference O
(i.e. O
prod- O
ucts O
in O
a O
digital O
shop) O
and O
a O
denotation O
function. O
4.1 O
Building O
a O
Target O
Domain O
We O
represent O
products O
in O
a O
target O
shop O
through O
aprod2vec O
model O
built O
with O
anonymized O
shopping O
sessions O
containing O
user-product O
interactions. O
Em- O
beddings O
are O
trained O
by O
solving O
the O
same O
optimiza- O
tion O
problem O
as O
in O
classical O
word2vec O
(Mikolov O
et O
al., O
2013a): O
word2vec O
becomes O
prod2vec O
by O
sub- O
stituting O
words O
in O
asentence O
with O
products O
viewed O
in O
ashopping O
session O
(Mu O
et O
al., O
2018). O
The O
util- O
ity O
of O
prod2vec O
is O
independently O
justiﬁed O
(Grbovic O
et O
al., O
2015; O
Tagliabue O
and O
Yu, O
2020) O
and, O
more O
importantly, O
the O
referential O
approach O
leverages O
the O
abundance O
of O
browsing-based O
interactions, O
as O
com- O
pared O
to O
search-based O
interactions: O
by O
learning B-TaskName
product I-TaskName
embeddings I-TaskName
from I-TaskName
abundant I-TaskName
behavioral I-TaskName
data I-TaskName
ﬁrst, O
we O
sidestep O
a O
major O
obstacle O
to O
reliable O
word O
representation O
in O
eCommerce. O
Hyperparameter O
op- O
timization O
follows O
the O
guidelines O
in O
Bianchi O
et O
al. O
(2020a), O
with O
a O
total O
of O
26;057(Shop O
X O
) O
and O
84;575 O
(Shop O
Y O
) O
product O
embeddings O
available O
for O
down- O
stream O
processing5. O
4.2 O
Learning O
Embeddings O
The O
fundamental O
intuition O
of O
Query2Prod2Vec B-MethodName
is O
treating O
clicks O
after O
qas O
a O
noisy O
feedback O
map- O
ping O
qto O
a O
portion O
of O
the O
latent O
product O
space. O
In O
particular, O
we O
compute O
the O
embedding O
for O
qby O
averaging O
the O
product O
embeddings O
of O
all O
products O
5Final O
parameters O
for O
prod2vec O
are:dimension B-HyperparameterName
= O
50 B-HyperparameterValue
, O
win_size B-HyperparameterName
= O
10 B-HyperparameterValue
,iterations B-HyperparameterName
= O
30 B-HyperparameterValue
,ns_exponent B-HyperparameterName
= O
0:75.clicked B-HyperparameterValue
after O
it, O
using O
frequency O
as O
a O
weighting O
fac- O
tor O
(i.e. O
products O
clicked O
often O
contribute O
more). O
The O
model O
has O
one O
free O
parameter, O
rank, B-HyperparameterName
which O
controls O
how O
many O
embeddings O
are O
used O
to O
build O
the O
representation O
for O
q: O
ifrank=k B-HyperparameterName
, O
only O
the O
kmost B-HyperparameterValue
clicked O
products O
after O
qare O
used. O
The O
results O
in O
Table O
1 O
are O
obtained O
with O
rank=5 B-HyperparameterName
, O
as O
we O
leave O
to O
future O
work O
to O
investigate O
the O
role O
of O
this O
parameter. O
The O
lack O
of O
large-scale O
search O
logs O
in O
the O
case O
of O
new O
deployments O
is O
a O
severe O
issue O
for O
successful O
training. O
The O
referential O
nature O
ofQuery2Prod2Vec B-MethodName
provides O
a O
fundamental O
com- O
petitive O
advantage O
over O
models O
building O
embed- O
dings O
from O
past O
linguistic O
behavior O
only, O
as O
syn- O
thetic O
embeddings O
can O
be O
generated O
as O
long O
as O
cheap O
session O
data O
is O
available O
to O
obtain O
an O
ini- O
tialprod2vec O
model. O
As O
detailed O
in O
the O
ensuing O
section, O
the O
process O
happens O
in O
two O
stages, O
event O
generation O
and O
embeddings O
creation. O
4.3 O
Creating O
Synthetic O
Embeddings O
The O
procedure O
to O
create O
synthetic O
embeddings O
is O
detailed O
in O
Algorithm O
1: O
it O
takes O
as O
input O
a O
list O
of O
words, O
a O
pre-deﬁned O
number O
of O
sampling O
it- O
erations, O
a O
popularity O
distribution O
over O
products6, O
and O
it O
returns O
a O
list O
of O
synthetic O
search O
events, O
that O
is, O
a O
mapping O
between O
words O
and O
lists O
of O
prod- O
ucts O
“clicked”. O
Simulating O
the O
search O
event O
can O
be O
achieved O
through O
the O
existing O
search O
engine, O
as, O
from O
a O
practical O
standpoint, O
some O
IR O
system O
must O
already O
be O
in O
place O
given O
the O
use O
case O
under O
con- O
sideration. O
To O
avoid O
over-relying O
on O
the O
quality O
of O
IR O
and O
prove O
the O
robustness O
of O
the O
method, O
all O
the O
simulations O
below O
are O
not O
performed O
with O
the O
actual O
production O
API, O
but O
with O
a O
custom-built O
in- O
verted O
index O
over O
product O
meta-data, O
with O
a O
simple O
TF-IDF O
weighting O
and O
Boolean O
search. O
For O
the O
second O
stage, O
we O
can O
treat O
the O
synthetic O
click O
events O
produced O
by O
Algorithm O
1 O
as O
a O
drop- O
in O
replacement O
for O
user-generated O
events O
– O
that O
is, O
for O
any O
query O
q, O
we O
calculate O
an O
embedding O
by O
averaging O
the O
product B-TaskName
embeddings I-TaskName
of I-TaskName
the I-TaskName
rel- I-TaskName
evant I-TaskName
products, I-TaskName
weighted O
by O
frequency7. O
Putting O
the O
two O
stages O
together, O
Query2Prod2Vec B-MethodName
can O
not O
only O
produce O
reliable O
query O
embeddings O
based O
on O
historical O
data, O
but O
also O
learn O
approximate O
embed- O
dings O
for O
a O
large O
vocabulary O
before O
being O
exposed O
6Please O
note O
that O
data O
on O
product O
popularity O
can O
be O
easily O
obtained O
through O
marketing O
tools, O
such O
as O
Google B-DatasetName
Analytics. I-DatasetName
7Please O
note O
that O
while O
thiswork O
focuses O
on O
lexical O
quality, O
the O
same O
strategy O
can O
be O
applied O
to O
complex O
queries O
in O
a O
“cold O
start” O
scenario.157Algorithm O
1: O
Generation O
of O
synthetic O
click O
events. O
Data: O
a O
list O
of O
words O
W, O
a O
pre-deﬁned O
number O
Nof O
simulations O
per O
word, O
a O
distribution O
Dover O
products. O
Result: O
A O
dataset O
of O
synthetic O
clicked O
events: O
E O
E O
empty O
mapping O
; O
foreach O
wordwinWdo O
product_list O
Search( O
w); O
fori= O
1toNdo O
p O
Sample O
(product_list, O
D); O
append O
the O
entry O
( O
w,p) O
toE; O
end O
end O
return O
E O
to O
any O
search O
interaction: O
in O
Section O
7 O
we O
report O
the O
performance O
of O
Query2Prod2Vec B-MethodName
when O
using O
only O
synthetic O
embeddings8. O
5 O
Dataset O
and O
Baselines O
5.1 O
Dataset O
Following O
best O
practices O
in O
the O
multi-tenant O
liter- O
ature O
(Tagliabue O
et O
al., O
2020b), O
we O
benchmark O
all O
models O
on O
different O
shops O
to O
test O
their O
robustness. O
In O
particular, O
we O
obtained O
catalog O
data, O
search O
logs O
and O
anonymized O
shopping O
sessions O
from O
two O
part- O
nering O
shops, O
Shop O
X O
andShop O
Y O
:Shop O
X O
is O
a O
sport O
apparel O
shop O
with O
Alexa O
ranking O
of O
approx- O
imately O
200k O
, O
representing O
a O
prototypical O
shop O
in O
the O
middle O
of O
the O
long O
tail; O
Shop O
Y O
is O
a O
home O
im- O
provement O
shop O
with O
Alexa O
ranking O
of O
approxi- O
mately O
10k, O
representing O
an O
intermediate O
size O
be- O
tween O
Shop O
X O
and O
public O
companies O
in O
the O
space. O
Linguistic O
data O
is O
in O
Italian O
for O
both O
shops, O
and O
training O
is O
done O
on O
random O
sessions O
from O
the O
pe- O
riod O
June-October O
2019: O
after O
sampling, O
removal O
of O
bot-like O
sessions O
and O
pre-processing, O
we O
are O
left O
with O
722;479sessions O
for O
Shop O
X O
, O
and O
1;986;452 O
sessions O
for O
Shop O
Y O
. O
5.2 O
Baselines O
We O
leverage O
the O
unique O
opportunity O
to O
join O
cata- O
log O
data, O
search O
logs O
and O
shopping O
sessions O
to O
ex- O
tensively O
benchmark O
Query2Prod2Vec B-MethodName
against O
a O
variety O
of O
methods O
from O
NLP O
and O
IR. O
8All O
the O
experiments O
are O
performed O
with O
N= O
500 O
simu- O
lated O
search O
events O
per O
query.•Word2Vec O
and O
FastText O
. O
We O
train O
a O
CBOW B-MethodName
(Mikolov O
et O
al., O
2013a) O
and O
a O
FastText B-MethodName
model O
(Bojanowski O
et O
al., O
2017) O
over O
product O
descriptions O
in O
the O
catalog; O
•UmBERTo B-MethodName
. O
We O
use O
RoBERTa B-MethodName
trained O
on O
Ital- O
ian O
data O
– O
UmBERTo9. B-MethodName
Thehsiembedding O
of O
the O
last O
layer O
of O
the O
architecture O
is O
the O
query O
embedding; O
•Search2Vec B-MethodName
. O
We O
implement O
the O
skip-gram O
model O
from O
Grbovic O
et O
al. O
(2016), O
by O
feeding O
the O
model O
with O
sessions O
composed O
of O
search O
queries O
and O
user O
clicks. O
Following O
the O
origi- O
nal O
model, O
we O
also O
train O
a O
time-sensitive O
vari- O
ant, O
in O
which O
time O
between O
actions O
is O
used O
to O
weight O
query-click O
pairs O
differently; O
•Query2Vec B-MethodName
. O
We O
implement O
a O
different O
context-target O
model, O
inspired O
by O
Egg O
(2019): O
embeddings O
are O
learned O
by O
the O
model O
when O
it O
tries O
to O
predict O
a O
(purchased O
or O
clicked) O
item O
starting O
from O
a O
query; O
•QueryNGram2Vec B-MethodName
. O
We O
implement O
the O
model O
from O
Bai O
et O
al. O
(2018). O
Besides O
learning O
representations O
through O
a O
skip-gram O
model O
as O
in O
Grbovic O
et O
al. O
(2016), O
the O
model O
learns O
the O
embeddings O
of O
unigrams O
to O
help O
cover O
the O
long O
tail O
for O
which O
no O
direct O
embedding O
is O
available. O
To O
guarantee O
a O
fair O
comparison, O
all O
models O
are O
trained O
on O
the O
same O
sessions. O
For O
all O
baselines, O
we O
follow O
the O
same O
hyperparameters O
found O
in O
the O
cited O
works: O
the O
dimension O
of O
query O
embedding O
vectors O
is O
set O
to O
50, O
except O
that O
768-dimensional O
vectors O
are O
used O
for O
UmBERTo B-MethodName
, O
as O
provided O
by O
the O
pre-trained O
model. O
As O
discussed O
in O
Section O
1, O
a O
distinguishing O
fea- O
ture O
of O
Query2Prod2Vec B-MethodName
isgrounding O
, O
that O
is, O
the O
relation O
between O
words O
and O
an O
external O
domain O
– O
in O
this O
case, O
products. O
It O
is O
therefore O
interesting O
not O
only O
to O
assess O
a O
possible O
quantitative O
gap O
in O
the O
quality O
of O
the O
representations O
produced O
by O
the O
baseline O
models, O
but O
also O
to O
remark O
the O
qualita- O
tivedifference O
at O
the O
core O
of O
the O
proposed O
method: O
if O
words O
are O
about O
something, O
pure O
co-occurrence O
patterns O
may O
be O
capturing O
only O
fragments O
of O
lexical O
meaning O
(Bianchi O
et O
al., O
2021). O
9https://huggingface.co/Musixmatch/ O
umberto-commoncrawl-cased-v11586 O
Solving O
Analogies O
in O
eCommerce O
As O
discussed O
in O
Section O
2, O
we O
consider O
evalua- O
tion O
tasks O
focused O
on O
word O
meaning O
, O
without O
us- O
ing O
product-based O
similarity O
(as O
that O
would O
im- O
plicitly O
and O
unfairly O
favor O
referential O
embeddings). O
Analogy-based O
tasks O
(Mikolov O
et O
al., O
2013a) O
are O
a O
popular O
choice O
to O
measure O
semantic B-MetricName
accuracy I-MetricName
of O
embeddings, O
where O
a O
model O
is O
asked O
to O
ﬁll O
templates O
like O
man O
: O
king O
= O
woman O
: O
? O
; O
how- O
ever, O
preparing O
analogies O
for O
digital O
shops O
presents O
non O
trivial O
challenges O
for O
human O
annotators: O
these O
would O
in O
fact O
need O
to O
know O
both O
the O
language O
and O
the O
underlying O
space O
(“air O
max” O
is O
closer O
to O
“nike” O
than O
to O
“adidas”), O
with O
the O
additional O
complication O
that O
many O
candidates O
may O
not O
have O
“determinate” O
answers O
(e.g. O
if O
Adidas O
is O
toGazelle O
, O
then O
Nike O
is O
to O
what O
exactly?). O
In O
building O
our O
testing O
framework, O
we O
keep O
the O
intuition O
that O
analogies O
are O
an O
effective O
way O
to O
test O
for O
lexical O
meaning O
and O
the O
assumption O
that O
human-level O
concepts O
should O
be O
our O
ground O
truth: O
in O
particular, O
we O
programmatically O
produce O
analogies O
by O
leveraging O
existing O
human O
labelling, O
as O
indirectly O
provided O
by O
the O
merchandisers O
who O
built O
product O
catalogs10. O
6.1 O
Test O
Set O
Preparation O
We O
extract O
words O
from O
the O
merchandising O
taxon- O
omy O
of O
the O
target O
shops, O
focusing O
on O
three O
most O
frequent O
ﬁelds O
in O
query O
logs: O
product O
type O
,brand O
andsport O
activity O
forShop O
X O
;product O
type O
,brand O
andpart O
of O
the O
house O
forShop O
Y O
. O
Our O
goal O
is O
to O
go O
from O
taxonomy O
to O
analogies, O
that O
is, O
showing O
how O
for O
each O
pair O
of O
taxonomy O
types O
(e.g. O
brand O
: O
sport O
), O
we O
can O
produce O
two O
pairs O
of O
tokens O
(Wil- O
son O
: O
tennis O
,Cressi O
: O
scubadiving O
), O
and O
create O
two O
analogies: O
b1 O
: O
s1 O
= O
b2 O
: O
? O
(target: O
s2) O
andb2: O
s2 O
= O
b1 O
: O
? O
(target: O
s1) O
for O
testing O
purposes. O
For O
each O
type O
in O
a O
pair O
(e.g. O
brand O
: O
sport O
), O
we O
repeat O
the O
following O
for O
all O
possible O
values O
of O
brand O
(e.g. O
“Wilson”, O
“Nike”) O
– O
given O
a O
brand O
B: O
1.we O
loop O
over O
the O
catalog O
and O
record O
all O
values O
ofsport O
, O
along O
with O
their O
frequency, O
for O
the O
products O
made O
by O
B. O
For O
example, O
for O
B= O
Nike O
, O
the O
distribution O
may O
be: O
{“soccer”: O
10, O
“basketball”: O
8, O
“scubadiving”: O
0 O
}; O
for O
B= O
Wilson O
, O
it O
may O
be: O
{“tennis”: O
8}; O
10It O
is O
important O
to O
note O
that O
this O
categorization O
is O
done O
by O
product O
experts O
for O
navigation O
and O
inventory O
purposes: O
all O
product O
labels O
are O
produced O
independently O
from O
any O
NLP O
consideration.2.we O
calculate O
the O
Gini B-HyperparameterName
coefﬁcient I-HyperparameterName
(Catalano O
et O
al., O
2009) O
over O
the O
distribution O
on O
the O
val- O
ues O
of O
sport O
and O
choose O
a O
conservative O
Gini B-HyperparameterName
threshold, I-HyperparameterName
i.e. B-HyperparameterValue
75thpercentile: I-HyperparameterValue
the O
goal O
of O
this O
threshold O
is O
to O
avoid O
“undetermined” O
analo- O
gies, O
such O
as O
Adidas O
: O
Gazelle O
= O
Nike O
: O
? O
. O
The O
intuition O
behind O
the O
use O
of O
a O
dispersion O
measure O
is O
that O
product O
analogies O
are O
harder O
if O
the O
relevant O
label O
is O
found O
across O
a O
variety O
of O
products11. O
With O
all O
the O
Gini B-HyperparameterName
coefﬁcients I-HyperparameterName
and O
a O
chosen O
thresh- O
old, O
we O
are O
now O
ready O
to O
generate O
the O
analogies, O
by O
repeating O
the O
following O
for O
all O
values O
of O
brand O
– O
given O
a O
brand O
Bwe O
can O
repeat O
the O
following O
sam- O
pling O
process O
Ktimes O
( O
K= B-HyperparameterName
10 B-HyperparameterValue
for O
our O
experi- O
ments): O
1.ifB’s O
Gini B-HyperparameterName
value I-HyperparameterName
for O
its O
distribution O
of O
sport O
la- O
bels O
is O
below O
our O
chosen O
threshold, O
we O
skip O
B; O
ifB’s O
value O
is O
above O
, O
we O
associate O
to O
Bits O
most O
frequent O
sport O
value, O
e.g. O
Wilson O
: O
ten- O
nis. O
This O
is O
the O
source O
pair O
of O
the O
analogy; O
to O
generate O
a O
target O
pair, O
we O
sample O
randomly O
a O
brand O
Cwith O
high O
Gini B-HyperparameterName
together O
with O
its O
most O
frequent O
value, O
e.g. O
Atomic O
: O
skiing O
; O
2.we O
add O
to O
the O
ﬁnal O
test O
set O
two O
analogies: O
Wil- O
son O
: O
tennis O
= O
Atomic O
: O
? O
, O
and O
Atomic O
: O
skiing O
= O
Wilson O
: O
? O
. O
The O
procedure O
is O
designed O
to O
generate O
test O
exam- O
ples O
conservatively, O
but O
of O
fairly O
high O
quality, O
as O
for O
example O
Garmin O
: O
watches O
= O
Arena O
: O
bathing O
cap O
(the O
analogy O
relates O
two O
brands O
which O
sell O
only O
one O
type O
of O
items), O
or O
racket O
: O
tennis O
= O
bathing O
cap O
: O
in- O
door O
swimming O
(the O
analogy O
relates O
“tools” O
that O
are O
needed O
in O
two O
activities). O
A O
total O
of O
1208 O
and606 O
test O
analogies O
are O
used O
for O
the O
analogy O
task O
( O
AT) O
for, O
respectively, O
Shop O
X O
andShop O
Y O
: O
we O
bench- O
mark O
all O
models O
by O
reporting O
Hit O
Rate O
at O
different O
cutoffs O
(Vasile O
et O
al., O
2016), O
and O
we O
also O
report O
how O
many O
analogies O
are O
covered O
by O
the O
lexicon O
learned O
by O
the O
models O
( O
coverage O
is O
the O
ratio O
of O
analogies O
for O
which O
all O
embeddings O
are O
available O
in O
the O
relevant O
space). O
7 O
Results O
Table O
1 O
reports O
model O
performance O
for O
the O
cho- O
sen O
cutoffs. O
Query2Prod2Vec B-MethodName
(as O
trained O
on O
real O
11In O
other O
words, O
Wilson O
: O
tennis O
= O
Atomic O
: O
? O
(skiing) O
is O
a O
better O
analogy O
than O
Adidas O
: O
Gazelle O
= O
Nike O
: O
? O
data) O
has O
the O
best O
performance12, O
while O
maintain- O
ing O
a O
very O
competitive O
coverage. O
More O
impor- O
tantly, O
following O
our O
considerations O
in O
Section O
2, O
results O
conﬁrm O
that O
producing O
competitive B-TaskName
embed- I-TaskName
dings I-TaskName
on I-TaskName
shops I-TaskName
with O
different O
constraints O
is O
a O
chal- O
lenging O
task O
for O
existing O
techniques, O
as O
models O
tend O
to O
either O
rely O
on O
speciﬁc O
query O
distribution O
(e.g. O
Search2Vec B-MethodName
(time) O
), O
or O
the O
availability O
of O
lin- O
guistic O
and O
catalog O
resources O
with O
good O
coverage O
(e.g. O
Word2Vec B-MethodName
).Query2Prod2Vec B-MethodName
is O
the O
only O
model O
performing O
with O
comparable O
quality O
in O
the O
two O
scenarios, O
further O
strengthening O
the O
method- O
ological O
importance O
of O
running O
benchmarks O
on O
more O
than O
one O
shop O
if O
ﬁndings O
are O
to O
be O
trusted O
by O
a O
large O
group O
of O
practitioners. O
7.1 O
Sample O
Efﬁciency O
and O
User O
Studies O
To O
investigate O
sample O
efﬁciency, O
we O
run O
two O
further O
experiments O
on O
Shop O
X O
: O
ﬁrst, O
we O
run O
ATgiving O
only O
1/3 O
of O
the O
original O
data O
toQuery2Prod2Vec B-MethodName
(both O
for O
the O
prod2vec O
space, O
and O
for O
the O
denotation). O
The O
small-dataset O
version O
ofQuery2Prod2Vec B-MethodName
still O
outperforms O
all O
other O
full-dataset O
models O
in O
Table O
1 O
( O
HR@5,10 O
=0.276 O
/ O
0.380 O
). O
Second, O
we O
train O
a O
Query2Prod2Vec B-MethodName
model O
only O
with O
simulated O
data O
produced O
as O
explained O
in O
Section O
4 O
– O
that O
is, O
with O
zero O
data O
from O
real O
search O
logs. O
The O
entirely O
simu- O
lated O
Query2Prod2Vec B-MethodName
shows O
performance O
com- O
petitive O
with O
the O
small-dataset O
version O
( O
HR@5,10 O
=0.259 O
/ O
0.363 O
)13, O
outperforming O
all O
baselines. O
As O
a O
further O
independent O
check, O
we O
supple- O
ment O
ATwith O
a O
small O
semantic O
similarity O
task O
( O
ST) O
12HR@20 O
was O
also O
computed, O
but O
omitted O
for O
brevity O
as O
it O
conﬁrmed O
the O
general O
trend. O
13A O
similar O
result O
was O
obtained O
on O
Shop O
Y O
, O
and O
it O
is O
omitted O
for O
brevity.onShop O
X14: O
two O
native O
speakers O
are O
asked O
to O
solve O
a O
small O
set O
( O
46) O
of O
manually O
curated O
ques- O
tions O
in O
the O
form: O
“Given O
the O
word O
Nike, O
which O
is O
the O
most O
similar, O
Adidas O
orWilson O
?”.STis O
meant O
to O
(partially) O
capture O
how O
much O
the O
em- B-TaskName
bedding I-TaskName
spaces I-TaskName
align I-TaskName
with I-TaskName
lexical I-TaskName
intuitions I-TaskName
of O
generic O
speakers, O
independently O
of O
the O
product O
search O
dynamics. O
Table O
1 O
reports O
results O
treat- O
ing O
human O
ratings O
as O
ground O
truth O
and O
using O
co- O
sine O
similarity O
on O
the O
learned O
embeddings O
for O
all O
models15.Query2Prod2Vec B-MethodName
outperforms O
all O
other O
methods, O
further O
suggesting O
that O
the O
representa- O
tions O
learned O
through O
referential O
information O
cap- O
ture O
some O
aspects O
of O
lexical O
knowledge. O
7.2 O
Computational O
Requirements O
As O
stressed O
in O
Section O
2, O
accuracy O
and O
re- O
sources O
form O
a O
natural O
trade-off O
for O
industry O
prac- O
titioners. O
Therefore, O
it O
is O
important O
to O
high- O
light O
that, O
our O
model O
is O
not O
just O
more O
accu- O
rate, O
but O
signiﬁcantly O
more O
efﬁcient O
to O
train: O
the O
best O
performing O
Query2Prod2Vec B-MethodName
takes O
30 O
minutes O
(CPU O
only) O
to O
be O
completed O
for O
the O
larger O
Shop O
Y O
, O
while O
other O
competitive O
models O
such O
asSearch2Vec(time) B-MethodName
andQueryNGram2Vec B-MethodName
re- O
quire O
2 O
to O
4 O
hours16. O
Being O
able O
to O
quickly O
generate O
many O
models O
allows O
for O
cost-effective O
analysis O
and O
optimization; O
moreover, O
infrastructure O
cost O
is O
heav- O
ily O
related O
to O
ethical O
and O
social O
issues O
on O
energy O
consumption O
in O
NLP O
(Strubell O
et O
al., O
2019). O
14Shop O
X O
is O
chosen O
since O
it O
is O
easier O
to O
ﬁnd O
speakers O
familiar O
with O
sport O
apparel O
than O
DIY O
items. O
15Inter-rater O
agreement O
was O
substantial, O
with O
Cohen O
Kappa B-MetricName
Score I-MetricName
=0.67 B-MetricValue
(McHugh, O
2012). O
16Training O
is O
performed O
on O
a O
Tesla O
V100 O
16GB O
GPU. O
As O
a O
back O
of O
the O
envelope O
calculation, O
training O
QueryNGram2Vec B-MethodName
on O
a O
AWS O
p3 O
large O
instance O
costs O
around O
12 O
USD, O
while O
a O
standard O
CPU O
container O
for O
Query2Prod2Vec B-MethodName
costs O
less O
than O
1 O
USD.1608 O
Conclusion O
and O
Future O
Work O
Inthis O
work, O
we O
learned O
reference-based B-TaskName
word I-TaskName
embeddings I-TaskName
for I-TaskName
product I-TaskName
search: I-TaskName
Query2Prod2Vec B-MethodName
signiﬁcantly O
outperforms O
other O
embedding O
strate- O
gies O
on O
lexical O
tasks, O
and O
consistently O
provides O
good O
performance O
in O
small-data O
and O
zero-data O
sce- O
narios, O
with O
the O
help O
of O
synthetic O
embeddings. O
In O
future O
work, O
we O
will O
extend O
our O
analysis O
to O
i) O
spe- O
ciﬁc O
IR O
tasks, O
within O
the O
recent O
paradigm O
of O
the O
dual O
encoder O
model O
(Karpukhin O
et O
al., O
2020), O
and O
ii) O
compositional O
tasks, O
trying O
a O
systematic O
replication O
of O
the O
practical O
success O
obtained O
by O
Yu O
et O
al. O
(2020) O
through O
image-based O
heuristics. O
When O
looking O
at O
models O
like O
Query2Prod2Vec B-MethodName
in O
the O
larger O
industry O
landscape, O
we O
hope O
our O
methodology O
can O
help O
the O
ﬁeld O
broaden O
its O
hori- O
zons: O
while O
retail O
giants O
indubitably O
played O
a O
major O
role O
in O
moving O
eCommerce O
use O
cases O
to O
the O
center O
of O
NLP O
research, O
ﬁnding O
solutions O
that O
address O
a O
larger O
portion O
of O
the O
market O
is O
not O
just O
practically O
important, O
but O
also O
an O
exciting O
agenda O
of O
its O
own17. O
9 O
Ethical O
Considerations O
Coveo O
collects O
anonymized O
user O
data O
when O
provid- O
ing O
its O
business O
services O
in O
full O
compliance O
with O
ex- O
isting O
legislation O
(e.g. O
GDPR). O
The O
training O
dataset O
used O
for O
all O
models O
employs O
anonymous O
UUIDs O
to O
label O
events O
and O
sessions O
and, O
as O
such, O
it O
does O
not O
contain O
any O
information O
that O
can O
be O
linked O
to O
shoppers O
or O
physical O
entities; O
in O
particular, O
data O
is O
ingested O
through O
a O
standardized O
client-side O
integra- O
tion, O
as O
speciﬁed O
in O
our O
public O
protocol. O
Acknowledgements O
We O
wish O
to O
thank O
Nadia O
Labai, O
Patrick O
John O
Chia, O
Andrea O
Polonioli, O
Ciro O
Greco O
and O
three O
anony- O
mous O
reviewers O
for O
helpful O
comments O
on O
previ- O
ous O
versions O
of O
this O
article. O
The O
authors O
wish O
to O
thank O
Coveo O
for O
the O
support O
and O
the O
computational O
resources O
used O
for O
the O
project. O
Federico O
Bianchi O
is O
a O
member O
of O
the O
Bocconi O
Institute O
for O
Data O
Science O
and O
Analytics O
(BIDSA) O
and O
the O
Data O
and O
Market- O
ing O
Insights O
(DMI) O
unit. O
17Please O
note O
that O
a O
previous O
draft O
of O
this O
article O
appeared O
onarxiv O
–https://arxiv.org/abs/2104.02061 O
– O
after O
the O
review O
process, O
but O
before O
the O
camera-ready O
submis- O
sion. O