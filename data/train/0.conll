Proceedings O
of O
the O
11th O
Joint O
Conference O
on O
Lexical O
and O
Computational O
Semantics O
, O
pages O
341 O
=- O
347 O
July O
14-15, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
Online B-TaskName
Coreference I-TaskName
Resolution I-TaskName
for O
Dialogue B-TaskName
Processing: I-TaskName
Improving O
Mention-Linking O
on O
Real-Time O
Conversations O
Liyan O
Xu O
Jinho O
D. O
Choi O
Department O
of O
Computer O
Science O
Emory O
University, O
Atlanta, O
USA O
{liyan.xu,jinho.choi O
}@emory.edu O
Abstract O
This O
paper O
suggests O
a O
direction O
of O
corefer- B-TaskName
ence I-TaskName
resolution I-TaskName
for O
online O
decoding O
on O
actively O
generated O
input O
such O
as O
dialogue, O
where O
the O
model O
accepts O
an O
utterance O
and O
its O
past O
con- O
text, O
then O
ﬁnds O
mentions O
in O
the O
current O
utter- O
ance O
as O
well O
as O
their O
referents, O
upon O
each O
dia- O
logue O
turn. O
A O
baseline O
and O
four O
incremental- O
updated O
models O
adapted O
from O
the O
mention- O
linking O
paradigm O
are O
proposed O
for O
this O
new O
set- O
ting, O
which O
address O
different O
aspects O
includ- O
ing O
the O
singletons, O
speaker-grounded O
encod- O
ing O
and O
cross-turn O
mention O
contextualization. O
Our O
approach O
is O
assessed O
on O
three O
datasets: O
Friends B-DatasetName
,OntoNotes B-DatasetName
, O
and O
BOLT B-DatasetName
. O
Results O
show O
that O
each O
aspect O
brings O
out O
steady O
improve- O
ment, O
and O
our O
best O
models O
outperform O
the O
baseline O
by O
over O
10%, B-MetricValue
presenting O
an O
effective O
system O
for O
this O
setting. O
Further O
analysis O
high- O
lights O
the O
task O
characteristics, O
such O
as O
the O
sig- O
niﬁcance O
of O
addressing O
the O
mention B-MetricName
recall. I-MetricName
1 O
Introduction O
It O
has O
been O
made O
practical O
recently O
to O
apply O
coref- O
erence O
resolution O
to O
assist O
a O
broad O
scope O
of O
NLP O
tasks O
(Peng O
et O
al., O
"2017;" O
Sahu O
et O
al., O
"2019;" O
Gao O
et O
al., O
2019), O
especially O
with O
the O
advent O
of O
neural O
end-to-end O
decoding O
and O
contextualized O
encoding O
(Lee O
et O
al., O
2017, O
"2018;" O
Joshi O
et O
al., O
2019, O
"2020;" O
Wu O
et O
al., O
2020). O
However, O
it O
is O
quite O
limited O
to O
use O
existing O
coreference B-TaskName
models I-TaskName
in O
real-time O
dialogue B-TaskName
processing I-TaskName
systems, O
as O
most O
of O
them O
are O
not O
trained O
to O
handle O
an O
online O
decoding O
environment. O
In O
the O
dialogue O
domain, O
recent O
efforts O
have O
focused O
on O
ellipsis O
recovery O
and O
query O
rewriting O
(Quan O
et O
al., O
"2019;" O
Tseng O
et O
al., O
"2021);" O
in O
this O
work, O
we O
target O
to O
address O
a O
new O
perspective O
speciﬁcally O
for O
the O
online O
decoding, O
where O
the O
model O
sequentially O
ac- O
cepts O
utterances O
in O
a O
dialogue O
and O
spits O
out O
valid O
mentions O
as O
well O
as O
their O
referent O
links O
for O
each O
latest O
utterance O
turn O
upon O
arrival, O
to O
be O
consumed O
by O
the O
downstream O
dialogue B-TaskName
processing I-TaskName
More O
formally, O
let O
uibe O
the O
current O
( O
i’th) O
utter- O
ance O
in O
a O
dialogue O
( O
"u1,..,ui,..);Mibe" O
the O
men- O
tions O
"inui;Mi−1be" O
the O
mentions O
from O
previously O
predicted O
clusters O
till O
ui−1. O
The O
objective O
upon O
i’th O
turn O
is O
to: O
-1 O
identify O
Mi(2) O
identify O
conference O
links O
amongMi, O
as O
well O
as O
fromMitoMi−1. O
We O
do O
not O
allow O
updates O
on O
Milater, O
since O
that O
would O
be O
equivalent O
to O
general O
coreference B-TaskName
"resolution;" I-TaskName
in O
this O
work, O
we O
speciﬁcally O
target O
this O
underexplored O
online O
scenario O
under O
this O
setting, O
which O
requires O
accurate O
predictions O
upon O
each O
turn O
that O
could O
be O
directly O
consumed O
by O
downstream O
applications. O
Several O
quasi-online O
coreference O
models O
have O
been O
proposed O
that O
maintain O
and O
update O
referents O
sequentially O
(Clark O
and O
Manning, O
2015, O
"2016;" O
Liu O
et O
al., O
"2019;" O
Toshniwal O
et O
al., O
"2020;" O
Xia O
et O
al., O
2020). O
However, O
these O
models O
differ O
from O
our O
real O
online O
setting O
in O
two O
ways. O
First, O
only O
the O
latest O
utter- O
ance O
and O
its O
past O
sequence O
are O
visible O
in O
our O
set- O
ting, O
so O
that O
decisions O
need O
to O
be O
made O
without O
knowing O
the O
unseen O
future. O
Second, O
the O
decision O
of O
whether O
a O
span O
should O
be O
extracted O
or O
linked O
to O
others O
needs O
to O
be O
made O
immediately O
at O
each O
utter- O
ance O
turn, O
while O
quasi-online O
models O
can O
maintain O
an O
internal O
pool O
of O
candidates O
and O
make O
one O
ﬁnal O
prediction O
after O
the O
entire O
document O
is O
processed. O
For O
this O
task, O
we O
ﬁrst O
introduce O
our O
baseline O
adapted O
from O
the O
classic O
mention-linking O
(ML) O
ap-341proach O
(Wiseman O
et O
al., O
"2015;" O
Lee O
et O
al., O
2017), O
and O
then O
propose O
four O
models O
where O
each O
one O
does O
an O
incremental O
update O
upon O
the O
previous O
model O
and O
addresses O
a O
speciﬁc O
perspective O
of O
this O
task, O
including O
the O
online O
inference, O
singletons, O
speaker- O
grounded O
encoding, O
and O
mention O
contextualization O
across O
utterance O
turns. O
For O
our O
approach, O
we O
do O
not O
use O
models O
that O
maintain O
explicit O
entities, O
be- O
cause: O
-1 O
it O
has O
been O
shown O
that O
higher-order O
fea- O
tures O
from O
entity O
representation O
provide O
negative O
to O
marginal O
positive O
impact O
over O
ML O
counterparts O
despite O
their O
complexities O
(Xu O
and O
Choi, O
"2020;" O
Xia O
et O
al., O
"2020;" O
Toshniwal O
et O
al., O
"2020);" O
-2 O
ML O
models O
are O
“stateless” O
so O
that O
they O
do O
not O
need O
to O
maintain O
decision O
states O
for O
previous O
mentions, O
which O
makes O
it O
more O
adaptable O
to O
applications O
in O
practice. O
All O
models O
are O
evaluated O
on O
three O
datasets O
to O
test O
the O
generalizability O
of O
our O
approach, O
and O
the O
best O
model O
obtains O
over O
10% B-MetricValue
improvement O
over O
the O
baseline O
on O
all O
datasets. O
Results O
and O
further O
analysis O
suggest O
that O
each O
aforementioned O
aspect O
can O
bring O
out O
steady O
improvement O
under O
the O
online O
setting, O
and O
highlight O
the O
singleton O
recovery O
to O
be O
the O
most O
critical O
component. O
2 O
Approach O
End-to-End O
Resolution O
Our O
model O
backbone O
is O
based O
on O
the O
end-to-end O
coreference B-TaskName
resolution I-TaskName
(Lee O
et O
al., O
2018) O
with O
a O
Transformers O
encoder O
(Joshi O
et O
al., O
2020). O
It O
scores O
every O
span O
for O
being O
a O
mention, O
and O
extracts O
top O
spans O
as O
mention O
candi- O
dates. O
Pairwise B-HyperparameterName
scoring I-HyperparameterName
is O
then O
performed O
among O
all O
candidates O
to O
determine O
the O
coreference O
links. O
Details O
of O
the O
model O
architecture O
can O
be O
referred O
by O
the O
paper O
from O
Lee O
et O
al. O
(2018), O
and O
we O
denote O
the O
original O
coreference B-MetricName
loss I-MetricName
asLc. O
Baseline O
( O
BL)We O
ﬁrst O
present O
our O
baseline O
that O
takes O
the O
end-to-end O
model O
and O
trains O
in O
the O
exact O
same O
non-online O
way O
as O
prior O
work, O
but O
adapts O
the O
decoding O
to O
ﬁt O
in O
our O
online O
inference O
setting. O
Letuibe O
thei’th O
utterance O
in O
the O
dialogue, O
and O
|ui|be O
its O
length O
(number O
of O
tokens). O
During O
online O
decoding O
upon O
ui, O
this O
model O
takes O
an O
utterance O
sequence O
with O
past O
context O
as O
input, O
denoted O
by O
Ui O
k= O
"(uk,..,ui);k∈[1,i)is" O
dynamically O
de- O
termined O
by/summationtexti O
j=k|uj|≤Υwhere O
Υis O
the O
max O
number O
of O
tokens O
that O
the O
encoder O
accepts. O
Differ- O
ent O
from O
Lee O
et O
al. O
(2018), O
the O
mention O
candidates O
now O
consist O
of O
two O
parts: O
-1 O
the O
extracted O
top O
can- O
didates O
solely O
from O
ui, O
denoted O
"asXi;" O
-2 O
mentions O
from O
previously O
predicted O
clusters O
from O
Ui−1 O
k, O
de-noted O
asMi−1 O
k. O
Thereby O
the O
ﬁnal O
candidate O
set O
Xcan O
be O
denoted O
as O
Xi∪Mi−1 O
k. O
The O
same O
pair- O
wise O
scoring O
as O
prior O
work O
is O
then O
performed O
on O
all O
candidatesX. O
Since O
we O
do O
not O
modify O
previous O
decisions O
in O
our O
setting, O
we O
keep O
coreference O
links O
amongXi, O
or O
fromXitoMi−1 O
k, O
but O
not O
among O
Mi−1 O
k. O
The O
predicted O
clusters O
after O
uiwill O
be O
up- O
dated O
in O
the O
same O
way O
by O
picking O
the O
referent O
an- O
tecedents O
according O
to O
coreference O
links. O
Singleton O
Recovery O
( O
SR)SRis O
built O
upon O
BLto O
address O
the O
singleton O
problem. O
In O
BL, O
after O
pro- O
cessing O
each O
utterance O
sequence O
Ui O
k, O
the O
model O
ﬁl- O
ters O
out O
mention O
candidates O
from O
Xithat O
are O
not O
referent O
to O
any O
other O
candidates, O
according O
to O
the O
mention-linking O
paradigm. O
However, O
it O
results O
on O
losing O
non-anaphoric O
mentions O
that O
do O
not O
have O
referents O
inui, O
and O
yields O
a O
critical O
issue O
for O
online O
inference O
because O
mentions O
in O
uithat O
are O
currently O
singletons O
but O
potentially O
will O
ﬁnd O
referents O
in O
later O
utterances O
can O
get O
discarded O
too O
early. O
To O
address O
this O
issue, O
we O
adopt O
a O
simple O
strat- O
egy O
similar O
to O
(Xu O
and O
Choi, O
2021) O
that O
preserves O
any O
candidates O
whose O
mention B-HyperparameterName
scores I-HyperparameterName
are O
larger O
than O
a O
threshold O
of O
0, B-HyperparameterValue
denoted O
as O
sm>0, B-HyperparameterName
and O
cre- O
ates O
a O
singleton O
cluster O
for O
each O
of O
which O
have O
not O
yet O
found O
any O
referent O
(intermediate O
singletons). O
However, O
as O
many O
annotation O
schemes O
do O
not O
re- O
quire O
annotating O
singletons, O
e.g. O
CoNLL O
2012, O
we O
may O
not O
have O
“true” O
gold O
labels O
covering O
every O
valid O
mentions, O
similar O
to O
the O
“misguidance O
of O
unla- O
beled O
entities” O
problem O
in O
named O
entity O
recognition O
(NER) O
(Li O
et O
al., O
2021). O
Let O
Ψ+ O
mbe O
the O
set O
of O
sm B-HyperparameterName
of O
gold O
candidates O
according O
to O
the O
annotation, O
and O
Ψ− O
mbe O
the O
set O
of O
smof O
other O
candidates O
that O
may O
also O
contain O
certain O
valid O
mentions O
(singletons). O
We O
mitigate O
the O
FALSE O
negative O
issue O
of O
unlabeled O
mentions O
by O
applying O
dynamic O
negative O
sampling O
onΨ− O
m, O
denoted O
as O
Φ− O
m, O
where|Ψ+ O
m|≈|Φ− O
m|. O
Bi- B-HyperparameterName
nary I-HyperparameterName
cross-entropy I-HyperparameterName
( I-HyperparameterName
BCE) I-HyperparameterName
loss O
is O
then O
used O
for O
this O
optimization O
to O
aid O
the O
threshold O
requirement: O
Lm=BCE(Ψ+ O
m,Φ− O
m) O
-1 O
L=αc·Lc+αm·Lm O
-2 O
The O
ﬁnal O
lossLis O
estimated O
by O
the O
weighted O
sum O
of O
LmandLcusing O
the O
hyperparameters O
αcandαm. O
Online O
Resolution O
( O
OR)ORis O
designed O
specif- O
ically O
for O
online O
inference O
on O
dialogues. O
Distin- O
guished O
from O
BLthat O
takes O
the O
whole O
document O
as O
input O
in O
training, O
ORtakesUi O
kas O
input O
for O
both342training O
and O
decoding, O
closing O
the O
gap. O
To O
cap- O
ture O
subtle O
nuances O
from O
different O
speakers O
in O
the O
dialogue, O
we O
collect O
speaker O
names O
within O
each O
dialogue O
and O
assign O
a O
special O
token O
of O
position- O
based O
ID O
to O
each O
speaker O
(e.g. O
S1,S2) O
based O
on O
speaking O
orders, O
which O
is O
then O
prepended O
to O
its O
cor- O
responding O
utterance O
(Wu O
et O
al., O
2020). O
We O
also O
add[SEP] O
beforeuito O
signal O
the O
latest O
utterance. O
The O
following O
sequence O
is O
used O
as O
input O
for O
OR: O
{Sk}/sluraboveu/slurabove O
k···/slurabove{[SEP]}/slurabove{Si}/sluraboveui(3) O
During O
training O
upon O
the O
i’th O
turn, O
gold O
mentions O
inUi−1 O
kare O
used O
asMi−1 O
"k;" O
the O
lossesLmandLc O
are O
estimated O
only O
on O
candidates O
from O
ui. O
Gradient O
accumulation O
is O
applied O
across O
multiple O
utterance O
turns, O
and O
we O
warm-start O
ORby O
initializing O
from O
the O
parameters O
of O
SR, O
followed O
by O
the O
online O
train- O
ing O
described O
above. O
The O
decoding O
step O
for O
ORis O
kept O
the O
same O
as O
BLandSR. O
Speaker-Grounding O
( O
SG)SGadds O
a O
speaker- O
grounding O
subtask O
upon O
OR, O
which O
is O
to O
facilitate O
the O
encoding O
of O
multi-speaker O
interaction O
which O
is O
an O
important O
aspect O
in O
dialogues. O
In O
OR, O
although O
each O
input O
token O
is O
conditioned O
on O
speaker O
tokens O
as O
in O
Eq O
(3), O
it O
is O
not O
obvious O
to O
the O
model O
that O
each O
token O
is O
from O
which O
speaker, O
which O
can O
be O
a O
barrier O
to O
learn O
the O
speaker O
interaction. O
To O
ex- O
plicitly O
regularize O
the O
speaker O
encoding, O
we O
add O
a O
subtask O
to O
predict O
whether O
two O
candidates O
are O
from O
the O
same O
speaker O
based O
on O
their O
embeddings: O
the O
model O
gives O
a O
same-speaker O
score O
sssuch B-MetricName
that O
pairs O
from O
the O
same O
speaker O
have O
ss>0and B-MetricName
others O
ss≤0, B-MetricName
forcing O
the O
semantic O
representation O
to O
fuse O
the O
speaker O
interaction. O
Let O
Ψ+ O
sbe O
the O
set O
of O
ssof B-MetricName
pairs O
from O
the O
same O
"speaker;" O
Ψ− O
sbe O
the O
set O
of O
ss B-MetricName
of O
other O
pairs. O
We O
optimize O
ssbyBCE, B-MetricName
adding O
the O
loss O
in O
addition O
toLcandLm: O
ss(x,y) O
=ws·[gx⊕gy⊕(gx◦gy)⊕(gx−gy)] O
Ls=BCE(Ψ+ O
s,Ψ− O
s) O
-4 O
L=αc·Lc+αm·Lm+αs·Ls O
-5 O
gx/gydenotes O
the O
representation O
of O
a O
candidate O
and O
wsis B-MetricName
the O
scoring O
parameter. O
⊕denotes O
concatena- O
tion O
and◦is O
the O
element-wise O
multiplication. O
We O
also O
apply O
negative O
sampling O
to O
keep O
|Ψ+ O
s|≈|Ψ− O
s|. O
Span-Level O
Self-Attention O
( O
SA)SA O
is O
also O
added O
upon O
ORto O
achieve O
candidate O
contextual- O
ization. O
For O
each O
input O
Ui O
k, O
the O
representation O
ofall O
candidatesXis O
contextualized O
on O
the O
token- O
level O
because O
of O
Transformers’ O
encoding. O
How- O
ever,Mi−1 O
kis O
not O
used O
until O
the O
pairwise O
scoring. O
Therefore,Xiis O
not O
explicitly O
conditioned O
on O
the O
previously O
extracted O
mentions O
( O
Mi−1 O
k) O
on O
the O
span- O
level. O
To O
capture O
the O
dependency O
among O
all O
men- O
tion O
candidates O
across O
utterances, O
we O
pass O
Xto O
a O
scaled O
dot-product O
self-attention O
layer O
(Vaswani O
et O
al., O
2017) O
before O
the O
pairwise O
scoring: O
G/prime=softmax/parenleftbig(GWq)(GWk)T O
√ O
d/parenrightbig O
(GWv),(6) O
whereG∈R|X|×dis O
the O
embedding O
matrix O
of O
all O
candidates,dis O
the O
embedding O
size, O
Wq,Wk,Wv O
are O
the O
parameters. O
G/primeis O
the O
new O
candidate-aware O
embedding O
matrix, O
which O
provides O
enhanced O
can- O
didate O
representation O
for O
the O
pairwise O
scoring. O
3 O
Experiments O
Datasets O
All O
models O
are O
experimented O
on O
the O
fol- O
lowing O
three O
datasets. O
Friends O
contains O
transcripts O
from O
the O
TV O
show O
in O
which O
personal O
mentions O
are O
annotated O
for O
entity O
linking. O
Each O
scene O
is O
consid- O
ered O
an O
independent O
dialogue O
where O
utterances O
and O
speaker O
IDs O
are O
provided. O
We O
adapt O
the O
data O
split O
suggested O
by O
Zhou O
and O
Choi O
(2018). O
Onto-Conv O
consists O
of O
documents O
in O
three O
genres O
selected O
from O
OntoNotes B-DatasetName
5.0: I-DatasetName
broadcasting O
and O
telephone O
conver- O
sations, O
and O
web O
text O
including O
discussion O
forums. O
We O
adapt O
the O
data O
split O
provided O
by O
Pradhan O
et O
al. O
-2012 O
and O
treat O
each O
document O
as O
a O
dialogue O
and O
every O
sentence O
as O
an O
utterance. O
BOLT B-DatasetName
follows O
the O
same O
annotation O
guideline O
as O
OntoNotes B-DatasetName
although O
documents O
are O
from O
discussion O
forums, O
SNS B-DatasetName
chats, O
and O
telephone O
conversations O
(Li O
et O
al., O
2016). O
Since O
this O
is O
the O
ﬁrst O
work O
using O
BOLT B-DatasetName
for O
this O
task, O
we O
create O
a O
new O
data O
split O
for O
future O
replicability O
(see O
A.1). O
Out O
of O
these O
three O
datasets, O
only O
Friends B-DatasetName
provides O
annotation O
of O
singletons. O
The O
numbers O
of O
documents O
in O
the O
training, O
devel- O
opment, O
and O
test O
set O
of O
Friends B-DatasetName
,Onto-Conv B-DatasetName
,BOLT B-DatasetName
are O
provided O
in O
Table O
2, O
along O
with O
the O
averaged O
numbers O
of O
speakers, O
entity O
clusters O
and O
utterances O
per O
document O
of O
each O
dataset. O
More O
details O
regard- O
ing O
the O
datasets O
are O
provided O
in O
Appendix O
A.1. O
Settings O
Our O
implementation O
are O
based O
on O
the O
PyTorch O
coreference O
models O
from O
Xu O
and O
Choi O
(2020), O
and O
SpanBERT B-MethodName
BASE O
is O
adopted O
as O
the O
en- O
coder. O
The O
implementation O
and O
trained O
models O
have O
been O
partially O
integrated O
with O
the O
open O
source O
project O
ELIT1(He O
et O
al., O
2021). O
During O
inference, O
all O
predicted O
clusters O
are O
col- O
lected O
and O
merged O
accordingly O
across O
utterances, O
and O
get O
evaluated O
by O
comparing O
them O
to O
the O
ground O
truth O
(all O
gold O
non-singleton O
clusters) O
at O
the O
end O
of O
each O
dialogue, O
in O
the O
same O
way O
as O
the O
CoNLL’12 O
shared O
task O
protocol. O
Detailed O
experimental O
set- O
tings O
are O
provided O
in O
Appendix O
A.2. O
Results O
Table O
1 O
describes O
the O
performance O
of O
all O
models O
on O
the O
test O
sets O
in O
the O
three O
datasets. O
These O
results O
are O
averaged O
across O
3 O
repeated O
"experiments;" O
Avg-F1 B-MetricName
is O
used O
as O
the O
main O
evaluation O
metric. O
Each O
proposed O
model O
gives O
steady O
improvement, O
and O
the O
best O
result O
is O
achieved O
by O
the O
OR+SG+SA O
model, O
surpassing O
the O
BLmodel O
on O
all O
datasets O
by O
sig- O
niﬁcant O
margins O
of O
≈10%. B-MetricValue
Among O
these O
models, O
singleton O
recovery O
contributes O
the O
most O
upon O
BL, O
demonstrating O
that O
albeit O
simple O
and O
intuitive, O
the O
training O
and O
inference O
of O
intermediate O
singletons O
is O
essential O
in O
online O
coreference O
resolution. O
3.1 O
Analysis O
on O
Online O
Inference O
To O
identify O
how O
model O
predictions O
are O
affected O
by O
online O
inference, O
all O
mentions O
in O
the O
predicted O
clus- O
ters O
are O
examined O
against O
the O
gold O
clusters. O
Table O
3 O
1https://github.com/emorynlp/elitshows O
the O
results O
of O
mention O
precision B-MetricName
and O
recall B-MetricName
from O
the O
four O
experimental O
settings. O
Following O
observations O
are O
drawn O
by O
this O
analysis: O
(1)Comparing O
N:BL O
andO:BL O
, O
online O
inference O
indeed O
leads O
to O
a O
large O
drop O
on O
the O
mention O
recall O
as O
expected, O
without O
as O
much O
increase O
on O
precision, B-MetricName
due O
to O
the O
omission O
of O
intermediate O
singletons. O
(2)Comparing O
O:BL O
andO:SR O
, O
singleton O
recov- O
ery O
(SR) O
signiﬁcantly O
improves O
the O
mention B-MetricName
recall I-MetricName
(8% B-MetricValue
for O
Friends B-DatasetName
and O
13+% B-MetricValue
for O
others) O
without O
sac- O
riﬁcing O
much O
precision. B-MetricName
However, O
notice O
that O
the O
recall B-MetricName
of O
O:SR O
forFriends B-DatasetName
is O
even O
higher O
than O
that O
of O
non-online O
inference O
( O
N:BL O
), O
but O
the O
recall B-MetricName
for O
Onto-Conv B-DatasetName
andBOLT B-DatasetName
is O
still O
4+% B-MetricValue
lower O
than O
that O
ofN:BL O
. O
This O
is O
due O
to O
the O
fact O
that O
Friends B-DatasetName
does O
have O
singletons O
annotated O
while O
the O
other O
two O
do O
not. O
Thus, O
O:SR O
forFriends B-DatasetName
does O
not O
suffer O
from O
the O
“misguidance O
of O
unlabeled O
entities” O
problem. O
(3)Comparing O
O:SR O
andO:SR- O
, O
it O
illustrates O
the O
positive O
impact O
of O
applying O
negative O
sampling O
on O
mentions O
to O
alleviate O
the O
false-negative O
issue O
of O
unlabeled O
mentions, O
which O
improves O
recall B-MetricName
while O
maintaining O
similar O
precision O
for O
online O
inference. O
3.2 O
Analysis O
on O
Utterance O
Interaction O
As O
we O
aim O
to O
build O
a O
robust O
online B-TaskName
resolution I-TaskName
model O
in O
the O
dialogue O
domain, O
understanding O
of O
individual344speakers O
is O
important O
especially O
in O
multi-party O
inter- O
action. O
In O
comparison O
to O
the O
binary O
indicator O
used O
inBLandSRthat O
can O
handle O
only O
up O
to O
two O
speak- O
ers, O
adding O
the O
subtask O
for O
speaker-grounded O
en- O
coding O
is O
shown O
to O
perform O
better O
for O
multi-speaker O
dialogues: O
the O
improvement O
of O
OR+SG O
overSRis O
3.50% B-MetricValue
F1 B-MetricName
for O
Friends B-DatasetName
, O
but O
around O
1% B-MetricValue
F1 B-MetricName
for O
the O
other O
two. O
Our O
statistics O
show O
that O
43% O
dialogues O
inFriends B-DatasetName
have O
at O
least O
4 O
speakers, O
while O
being O
only O
15% O
and O
24% O
for O
the O
other O
two, O
suggesting O
that O
the O
multi-speaker O
environment O
indeed O
beneﬁts O
more O
from O
the O
new O
speaker O
encoding O
scheme. O
In O
addition, O
the O
percentages O
of O
pronouns O
in O
the O
gold O
mentions O
are O
80.3%, O
53.5%, O
and O
63.50% O
in O
Friends B-DatasetName
,Onto-Conv B-DatasetName
, O
and O
BOLT B-DatasetName
respectively, O
which O
also O
highlights O
the O
importance O
of O
a O
better O
encod- O
ing O
scheme O
to O
handle O
a O
large O
portion O
of O
pronouns O
present O
in O
dialogue. O
Thus, O
we O
suggest O
to O
employ O
a O
more O
advanced O
dialogue O
encoding O
that O
utilizes O
the O
speaker O
interaction O
clues O
as O
one O
of O
the O
future O
research O
direction O
for O
this O
online-decoding O
task. O
4 O
Conclusion O
This O
paper O
presents O
a O
new O
coreference B-TaskName
resolution I-TaskName
direction O
that O
aims O
towards O
an O
online B-TaskName
decoding I-TaskName
set- I-TaskName
ting I-TaskName
for I-TaskName
dialogue I-TaskName
processing. I-TaskName
A O
baseline O
and O
four O
incremental-updated O
models O
are O
proposed O
and O
eval- O
uated O
on O
three O
datasets O
of O
the O
dialogue O
domain, O
and O
the O
best-performing O
model O
shows O
signiﬁcant O
im- O
provement O
over O
the O
baseline O
by O
≈10% B-MetricValue
F1. B-MetricName
Further O
analysis O
suggests O
the O
importance O
of O
mention O
recall O
and O
speaker O
encoding, O
which O
could O
serve O
as O
the O
next O
future O
directions O
of O
this O
online O
setting. O
