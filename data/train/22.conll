Proceedings O
of O
the O
19th O
International O
Conference O
on O
Spoken O
Language O
Translation O
(IWSLT O
2022) O
, O
pages O
341 O
- O
350 O
May O
26-27, O
2022 O
c O
2022 O
Association O
for O
Computational O
Linguistics O
Controlling O
Formality O
in O
Low-Resource O
NMT O
with O
Domain O
Adaptation O
and O
Re-Ranking: O
SLT-CDT-UoS O
at O
IWSLT2022 O
Sebastian O
T. O
Vincent, O
Loïc O
Barrault, O
Carolina O
Scarton O
Department O
of O
Computer O
Science, O
University O
of O
Sheffield O
Regent O
Court, O
211 O
Portobello, O
Sheffield, O
S1 O
4DP, O
UK O
{stvincent1, O
l.barrault, O
c.scarton}@shef.ac.uk O
Abstract O
This O
paper O
describes O
the O
SLT-CDT-UoS O
group’s O
submission O
to O
the O
first O
Special O
Task O
on O
Formal- O
ity O
Control O
for O
Spoken B-TaskName
Language I-TaskName
Translation, I-TaskName
part O
of O
the O
IWSLT O
2022 O
Evaluation O
Campaign. O
Our O
efforts O
were O
split O
between O
two O
fronts: O
data O
engineering O
and O
altering O
the O
objective O
func- O
tion O
for O
best O
hypothesis O
selection. O
We O
used O
language-independent O
methods O
to O
extract O
for- O
mal O
and O
informal O
sentence O
pairs O
from O
the O
pro- O
vided O
corpora; O
using O
English O
as O
a O
pivot O
lan- O
guage, O
we O
propagated O
formality O
annotations O
to O
languages O
treated O
as O
zero-shot O
in O
the O
task; O
we O
also O
further O
improved O
formality O
controlling O
with O
a O
hypothesis O
re-ranking O
approach. O
On O
the O
test O
sets O
for O
English-to-German O
and O
English- O
to-Spanish, O
we O
achieved O
an O
average O
accuracy B-MetricName
of.935within B-MetricValue
the O
constrained O
setting O
and O
.995 B-MetricValue
within O
unconstrained O
setting. O
In O
a O
zero-shot O
setting O
for O
English-to-Russian O
and O
English-to- O
Italian, O
we O
scored O
average O
accuracy B-MetricName
of O
.590for B-MetricValue
constrained O
setting O
and O
.659for B-MetricValue
unconstrained. O
1 O
Introduction O
Formality-controlled O
machine O
translation O
enables O
the O
system O
user O
to O
specify O
the O
desired O
formality O
level O
at O
input O
so O
that O
the O
produced O
hypothesis O
is O
expressed O
in O
a O
formal O
or O
informal O
style. O
Due O
to O
discrepancies O
between O
different O
languages O
in O
for- O
mality O
expression, O
it O
is O
often O
the O
case O
that O
the O
same O
source O
sentence O
has O
several O
plausible O
hypotheses, O
each O
aimed O
at O
a O
different O
audience; O
leaving O
this O
choice O
to O
the O
model O
may O
result O
in O
an O
inappropriate O
translation. O
This O
paper O
describes O
our O
team’s O
submission O
to O
the O
first O
Special O
Task O
on O
Formality O
Control O
in O
SLT O
at O
IWSLT O
2022 O
(Anastasopoulos O
et O
al., O
2022), O
where O
the O
objective O
was O
to O
achieve O
control O
over O
bi- O
nary O
expression O
of O
formality O
in O
translation O
(enable O
the O
translation O
pipeline O
to O
generate O
formal O
or O
infor- O
mal O
translations O
depending O
on O
user O
input). O
The O
task O
evaluated O
translations O
from O
English O
( O
EN) O
into O
Ger- O
man O
( O
DE), O
Spanish O
( O
ES), O
Russian O
( O
RU), O
Italian O
( O
IT),Japanese O
( O
JA) O
and O
Hindi O
( O
HI). O
Among O
these, O
EN- O
{RU,IT}were O
considered O
zero-shot; O
for O
other O
pairs, O
small O
paired O
formality-annotated O
corpora O
were O
pro- O
vided. O
The O
task O
ran O
in O
two O
settings: O
constrained O
(limited O
data O
and O
pre-trained O
model O
resources) O
and O
unconstrained O
(no O
limitations O
on O
either O
resource). O
Submissions O
within O
both O
the O
constrained O
and O
un- O
constrained O
track O
were O
additionally O
considered O
in O
two O
categories: O
full O
supervision O
and O
zero-shot. O
Our O
submission O
consisted O
of O
four O
primary O
sys- O
tems, O
one O
for O
each O
track/subtrack O
combination, O
and O
we O
focused O
on O
the O
EN-{DE,ES,RU,IT} O
lan- O
guage O
directions. O
We O
were O
interested O
in O
lever- O
aging O
the O
provided O
formality-annotated O
triplets O
(src, O
tgt O
formal, O
tgt O
informal O
)to O
extract O
sufficiently O
large O
annotated O
datasets O
from O
the O
permitted O
training O
corpora, O
without O
using O
language-specific O
resources O
or O
tools. O
We O
built O
a O
multilingual O
translation O
model O
in O
the O
given O
translation O
directions O
and O
fine-tuned O
it O
on O
our O
collected O
data. O
Our O
zero-shot O
submis- O
sions O
used O
fine-tuning O
data O
only O
for O
the O
non-zero- O
shot O
pairs. O
To O
boost O
the O
formality O
control O
(espe- O
cially O
within O
the O
constrained O
track), O
we O
included O
a O
formality-focused O
hypothesis O
re-ranking O
step. O
Our O
submissions O
to O
both O
tracks O
followed O
the O
same O
con- O
cepts, O
with O
the O
unconstrained O
one O
benefitting O
from O
larger O
corpora, O
and O
thus O
more O
fine-tuning O
data. O
In O
Section O
2 O
we O
describe O
our O
submission O
to O
the O
constrained O
track, O
including O
the O
data O
extraction O
step O
(Section O
2.2, O
2.3). O
Our O
approach O
begins O
with O
ex- O
tending O
this O
small O
set O
to O
cover O
more O
samples O
by O
extracting O
them O
from O
the O
allowed O
corpora. O
We O
use O
a O
language-independent B-MethodName
approach I-MethodName
of I-MethodName
domain I-MethodName
adaptation I-MethodName
for O
this. O
Then, O
we O
extract O
samples O
for O
the O
zero-shot O
pairs O
( O
EN-{RU,IT}) O
based O
on O
data O
collected O
for O
( O
EN-{DE,ES}). O
We O
then O
experiment O
with O
re-ranking O
the O
top O
nmodel O
hypotheses O
with O
a O
formality-focused O
objective O
function. O
Within O
our O
systems, O
we O
provide O
the O
formality O
information O
as O
a O
tagappended O
to O
the O
input O
of O
the O
model. O
Through- O
out O
the O
paper O
we O
use O
Fto O
denote O
the O
formal O
style341andIto O
denote O
the O
informal O
style. O
All O
our O
models O
submitted O
to O
the O
“supervised” O
subtracks O
achieved O
an O
average O
of O
+.284accuracy O
point O
over O
a O
baseline O
for O
all O
EN-{DE,ES,RU,IT}test O
sets, O
while O
the O
“zero-shot” O
models O
achieved O
an O
aver- O
age O
improvement O
of O
.124points O
on O
the O
EN-{RU,IT} O
test O
sets. O
Our O
work O
highlights O
the O
potential O
of O
both O
data O
adaptation O
and O
re-ranking O
approaches O
in O
at- O
tribute O
control O
for O
NMT. O
2 O
Constrained O
Track O
The O
MuST-C B-DatasetName
textual O
corpus O
(Di O
Gangi O
et O
al., O
2019) O
with O
quantities O
listed O
in O
Table O
1 O
was O
the O
only O
data O
source O
allowed O
within O
the O
constrained O
track, O
along- O
side B-MethodName
the O
IWSLT O
corpus O
of O
formality-annotated O
sen- O
tences O
(Nadejde O
et O
al., O
2022). O
MuST-C B-DatasetName
is O
a O
collec- O
tion O
of O
transcribed O
TED O
talks, O
all O
translated O
from O
English. O
The O
IWSLT O
data O
itself O
came O
from O
two O
domains: O
telephone O
conversations O
and O
topical O
chat O
(Gopalakrishnan O
et O
al., O
2019). O
The O
data O
was O
ad- O
ditionally O
manually O
annotated O
at O
phrase O
level O
for O
formal O
and O
informal O
phrases, O
and O
the O
organisers O
provided O
an O
evaluation O
tool O
scorer.py O
which, O
given O
a O
set O
of O
hypotheses, O
used O
these O
annotations O
to O
match O
sought O
formal O
or O
informal O
phrases, O
yield- O
ing O
an O
accuracy O
score O
when O
the O
number O
of O
correct O
matches O
is O
greater O
than O
the O
number O
of O
incorrect O
matches1. O
This O
scorer O
skips O
test O
cases O
where O
no O
matches O
are O
found O
in O
the O
hypotheses. O
In O
all O
our O
experiments O
we O
used O
the O
multilingual O
Transformer O
model O
architecture O
provided O
within O
fairseq O
(Ott O
et O
al., O
2019). O
For O
our O
pre-training O
data O
we O
used O
the O
full O
MuST-C O
corpus. O
We O
ap- O
plied O
SentencePiece O
(Kudo O
and O
Richardson, O
2018) O
to O
build O
a O
joint O
vocabulary O
of O
32K O
tokens O
across O
all O
languages. O
We O
list O
the O
model O
specifications O
in O
Table O
2. O
Pre-training O
lasts O
100K O
iterations O
or O
63 O
epochs. O
We O
average O
checkpoints O
saved O
at O
roughly O
the O
last O
10epochs. O
2.1 O
Formality O
Controlling O
Once O
the O
model O
was O
pre-trained, O
we O
fine-tuned O
it O
on O
the O
supervised O
data O
to O
control O
the O
desired O
formality O
of O
the O
hypothesis O
with O
a O
tagging O
approach O
(Sen- O
nrich O
et O
al., O
2016), O
whereby O
a O
formality-indicating O
tag O
is O
appended O
to O
the O
source O
input. O
This O
method O
has O
been O
widely O
used O
in O
research O
in O
various O
control- O
ling O
tasks O
(e.g. O
Johnson O
et O
al., O
2017; O
Vanmassen- O
hove O
et O
al., O
2018; O
Lakew O
et O
al., O
2019). O
1https://github.com/amazon-research/ O
contrastive-controlled-mt/blob/main/ O
IWSLT2022/scorer.py O
, O
accessed O
8 O
April O
2022.2.2 O
Automatic O
Extraction O
of O
Formal O
and O
Informal O
Data O
Since O
our O
approach O
was O
strongly O
dependent O
on O
the O
availability O
of O
labelled O
data, O
our O
initial O
efforts O
fo- O
cused O
on O
making O
the O
training O
corpus O
larger O
by O
ex- O
tracting O
sentence O
pairs O
with O
formal O
and O
informal O
target O
sentences O
from O
the O
provided O
MuST-C B-DatasetName
corpus. O
We O
made O
the O
assumption O
that O
similar O
sentences O
would O
correspond O
to O
a O
similar O
formality O
level. O
Thus, O
we O
decided O
to O
use O
the O
data O
selection O
approach O
to O
select O
the O
most O
similar O
sentence O
pairs O
from O
the O
out- O
of-domain O
corpus O
(MuST-C) B-DatasetName
to O
both O
the O
formal O
and O
informal O
sides O
of O
the O
IWSLT O
corpus, O
which O
we O
consider O
our O
in-domain O
data O
(each O
side O
separately). O
Specifically, O
let O
G= O
(Gsrc, O
Gtgt)be O
the O
out-of-domain O
corpus O
(MuST-C), B-DatasetName
and O
let O
SF= O
(Ssrc, O
Stgt,F)andSI= O
(Ssrc, O
Stgt,I)be O
the O
in- O
domain O
corpora O
(IWSLT). O
For O
simplicity, O
let O
us O
focus O
on O
adaptation O
to O
SF. O
Our O
adaptation O
approach O
focuses O
on O
the O
target- O
side O
sentences O
because O
the O
IWSLT O
corpus O
is O
paired O
(for O
each O
English O
sentence O
there O
is O
a O
formal O
and O
informal O
variant O
in O
the O
target O
language). O
The O
ap- O
proach O
builds O
a O
vocabulary O
of O
non-singleton O
to- O
kens O
from O
Stgt,F, O
then O
builds O
two O
language O
models: O
LMSfrom O
Stgt,FandLMGfrom O
a O
random O
sam- O
ple O
of O
10K O
sentences O
from O
Gtgt; O
both O
language O
models O
use O
the O
originally O
extracted O
vocabulary. O
Then, O
we O
calculate O
the O
sentence-level O
perplexity B-MetricName
PP(LMG, O
Gtgt)andPP(LMS, O
Gtgt). O
Finally, O
the O
sentence O
pairs O
within O
Gare O
ranked O
by O
PP(LMS, O
Gtgt)−PP(LMG, O
Gtgt). O
LetGsorted O
_by_F,Gsorted O
_by_Idenote O
the O
resulting O
corpora O
sorted O
by O
the O
perplexity O
difference. O
The O
in- O
tuition O
behind O
this O
approach O
is O
that O
sentences O
which O
use O
a O
certain O
formality O
will O
naturally O
rank O
higher O
on O
the O
ranked O
list O
for O
that O
formality, O
due O
to O
similarities O
in O
the O
used O
vocabulary. O
To O
obtain O
the O
formal O
and O
informal O
corpora O
from O
the O
sorted O
data, O
we O
needed O
to O
decide O
on O
a O
criterion. O
LetFposandIposbe O
the O
position O
of O
a O
sentence O
pair O
in O
the O
formal/informal O
ranking, O
respectively. O
Our O
first O
approach O
was O
simple: O
let O
Cdenote O
the O
size O
of O
the O
out-of-domain O
corpus; O
we O
implemented O
an O
Assign O
θfunction O
which, O
for O
a O
θ∈[0,C), O
assigned O
a O
label O
to O
the O
sentence O
pair O
(src, O
tgt O
), O
using O
the O
following O
rules: O
Assign O
Corpora O
containing O
training O
data O
used O
in O
the O
constrained O
track. O
Values O
indicate O
number O
of O
sentence O
pairs O
after O
preprocessing. O
CUDA_VISIBLE_DEVICES O
0,1,2,3 O
-finetune-from-model O
* O
-max-update O
* O
-ddp-backend=legacy_ddp O
-task O
multilingual_translation O
-arch O
multilingual_transformer_iwslt_de_en O
-lang-pairs O
en-de,en-es,en-ru,en-it O
-encoder-langtok O
tgt O
-share-encoders O
-share-decoder-input-output-embed O
-optimizer O
adam O
-adam-betas B-HyperparameterName
’(0.9, O
0.98)’ O
-lr O
0.0005 O
-lr-scheduler O
inverse_sqrt O
-warmup-updates B-HyperparameterName
4000 B-HyperparameterValue
-warmup-init-lr B-MethodName
’1e-07’ B-HyperparameterValue
-label-smoothing B-HyperparameterName
0.1 B-HyperparameterValue
-criterion O
label_smoothed_cross_entropy O
-dropout B-MethodName
0.3 B-HyperparameterValue
-weight-decay B-MethodName
0.0001 B-HyperparameterValue
-save-interval-updates O
* O
-keep-interval-updates O
10 O
-no-epoch-checkpoints O
-max-tokens B-HyperparameterName
1000 B-HyperparameterValue
-update-freq O
2 O
-fp16 O
Table O
2: O
Parameters O
of O
fairseq-train O
for O
pre- O
training O
and O
fine-tuning O
all O
models. O
The O
starred O
( O
*) O
parameters O
depend O
on O
the O
track/subtrack O
and O
can O
be O
found O
in O
the O
paper O
description O
or O
in O
the O
implementation. O
We O
condition O
assignment O
on O
both O
positional O
lists O
since O
common O
phrases O
such O
as O
( O
Yes! O
– O
Ja! O
) O
may O
rank O
high O
on O
both O
sides, O
but O
should O
not O
get O
included O
in O
either O
corpus. O
We O
determine O
θempirically O
by O
selecting O
a O
value O
that O
yields O
the O
most O
data O
as O
a O
result. O
These O
values O
were O
selected O
dynamically O
for O
each O
language O
pair, O
and O
resulted O
in O
θ= O
0.45Cfor O
EN-DEandθ= O
0.5CforEN-ES. O
We O
refer O
to O
this O
approach O
as O
I B-MethodName
NFER I-MethodName
EASY. I-MethodName
We O
quickly O
observed O
that O
the O
selection O
method O
needed O
to O
take O
into O
account O
the O
relative O
ranking O
of O
a O
sentence O
pair O
for O
both O
formalities. O
To O
illustrate O
this, O
let O
θ= O
50 O
, O
the O
number O
of O
sentences O
n= O
100 O
; O
a O
sentence O
pair O
with O
rankings O
Fpos= O
49,Ipos= O
51will O
get O
included O
in O
the O
formal O
corpus, O
but O
with O
Fpos= O
1,Ipos= O
50 O
it O
will O
not, O
because O
Iposis O
in O
the O
top O
kfor O
the O
informal O
set, O
even O
though O
the O
relative O
difference O
between O
the O
two O
positions O
is O
large. O
To O
amend O
this, O
we O
introduced O
a O
classifica- O
tion O
by O
relative O
position O
difference O
: O
for O
any O
sen- O
tence O
pair O
with O
positions O
(Fpos,Ipos)we O
classify O
it O
as O
formal O
if O
Fpos−Ipos> O
α O
. O
We O
determine O
αempirically: O
using O
0.05Cand0.2Cas O
the O
lower O
and O
upper O
bound, O
respectively, O
for O
several O
values O
αin O
range O
we O
compute O
a O
language O
model O
from O
the O
resulting O
data O
and O
calculate O
average B-MetricName
perplexity I-MetricName
PP(LMCorpus O
(α),IWSLT O
). O
We O
select O
the O
αvalue O
which O
minimises O
this O
perplexity. B-MetricName
We O
refer O
to O
this O
approach O
as O
I O
NFER O
FULL. O
2.3 O
Generalisation O
for O
Zero-Shot O
Language O
Pairs O
For O
two O
language O
pairs O
( O
EN-{RU,IT}) O
no O
super- O
vised O
training O
data O
was O
provided, O
meaning O
we O
could O
only O
use O
the O
IWSLT O
corpus O
and O
our O
inferred O
data O
from O
EN-{DE,ES} O
to O
obtain O
data O
for O
these O
pairs. O
We O
decided O
to O
focus O
on O
comparisons O
on O
the O
source O
(EN) O
side, O
meaning O
we O
could O
not O
use O
the O
IWSLT O
corpus O
as O
it O
was O
paired. O
One O
observation O
we O
made O
at O
this O
point O
was O
that, O
contrary O
to O
intuition, O
the O
same O
source O
sentences O
within O
the O
MuST-C O
corpus O
had O
different O
formality O
expressions O
in O
the O
German O
and O
Spanish O
corpora, O
respectively. O
Let O
EN-DEXESbe O
a O
corpus O
of O
triplets O
of O
sen- O
tences O
(src O
EN, O
tgt O
DE, O
tgt O
ES)obtained O
by O
identifying O
English O
sentences O
which O
occur B-MethodName
in O
both O
the O
EN-DE O
and O
EN-EScorpora. O
Since O
there O
are O
many O
such O
sentences O
in O
the O
MuST-C B-DatasetName
corpus, O
the O
EN-DEXES O
contains O
85.72% O
of O
sentence O
pairs O
from O
the O
EN-DE O
and74.13% O
of O
pairs O
from O
the O
EN-EScorpus. O
After O
marking O
the O
target O
sides O
of O
the O
EN-DEXEScorpus O
for O
formality O
with O
INFER O
FULL, O
we O
quantified O
in O
how O
many O
cases O
both O
languages O
get O
the O
same O
label343(formal O
of O
informal), O
and O
in O
how O
many O
cases O
they O
get O
a O
different O
label O
(Table O
3). O
Out O
of O
all O
annotated O
triplets, O
only O
5.8% O
triplets O
were O
annotated O
in O
both O
target O
languages; O
this O
is O
a O
significantly O
smaller O
frac- O
tion O
than O
expected. O
Within O
that O
group, O
almost O
60% O
triplets O
had O
matching O
annotations. O
This O
implies O
that O
the O
same O
English O
sentence O
can O
sometimes O
(approx. O
2 O
out O
of O
5 O
times O
in O
our O
case) O
be O
expressed O
with O
dif- O
ferent O
formality O
in O
the O
target O
language O
in O
the O
same O
discourse O
situation. O
Context O
combinations O
for O
the O
EN-DEXEStriplet O
extracted O
from O
the O
MuST-C B-DatasetName
dataset. O
“ O
∅” O
denotes O
“no O
context”. O
Given O
the O
non-zero O
count O
of O
triplets O
with O
match- O
ing O
formalities, O
we O
make O
another O
assumption: O
namely O
that O
the O
English O
sentences O
of O
the O
triplets O
with O
matching O
formalities O
may O
be O
of O
“strictly O
for- O
mal” O
or O
“strictly O
informal” O
nature, O
meaning O
the O
translations O
of O
at O
least O
some O
of O
those O
sentences O
to O
Russian O
and O
Italian O
may O
express O
the O
same O
formality. O
To O
extract O
formal O
and O
informal O
sentences O
for O
the O
zero-shot O
pairs, O
we O
adapted O
the O
original O
method, O
but O
this O
time O
using O
English O
as O
a O
pivot O
to O
convey O
the O
formality O
information. O
As O
the O
in-domain O
corpus, O
we O
used O
the O
English O
sentences O
whose O
German O
and O
Spanish O
translations O
were O
both O
labelled O
as O
formal O
or O
both O
as O
informal, O
respectively O
(columns O
1,2in O
Table O
3). O
We O
ranked O
the O
EN-RUand O
EN-ITcorpora O
by O
their O
source O
sentences’ O
similarity O
to O
that O
inter- O
section O
(using O
the O
perplexity O
difference O
as O
before). O
To O
infer O
the O
final O
corpora O
with O
the O
INFER O
FULL O
method, O
we O
used O
the O
αwhich O
yielded O
corpora O
of O
similar O
quantity O
to O
the O
ones O
for O
EN-{DE,ES}, O
since O
we O
could O
not O
determine O
that O
value O
empirically. O
2.4 O
Relative B-MethodName
Frequency I-MethodName
Model I-MethodName
for I-MethodName
Reranking: I-MethodName
FORMALITY I-MethodName
RERANK I-MethodName
We O
observed O
that O
even O
when O
a O
model O
gets O
the O
for- O
mality O
wrong O
in O
its O
best O
hypothesis, O
the O
correct O
answer O
is O
sometimes O
found O
within O
the O
nbest O
hy-potheses, O
but O
at O
a O
lower O
position. O
We O
hypothesised O
that O
by O
re-ranking O
the O
n-best O
list O
according O
to O
a O
criterion O
different O
from O
the O
beam O
search O
log O
proba- O
bility O
we O
could O
push O
the O
hypothesis O
with O
the O
correct O
formality O
to O
the O
first O
position. O
We O
performed O
an O
oracle O
experiment O
with O
scorer.py O
to O
obtain O
an O
upper O
bound O
on O
what O
can O
be O
gained O
by O
re-scoring O
the O
n-best O
list O
per- O
fectly: O
we O
generated O
k-best O
hypotheses O
for O
k∈ O
{1,5,10,20,30, O
..100}2and O
from O
each O
list O
of O
k O
hypotheses O
we O
selected O
the O
first O
hypothesis O
(if O
any) O
which O
scorer.py O
deemed O
of O
correct O
formality. O
The O
results O
(Table O
4) O
show O
that O
as O
we O
expand O
the O
list O
of O
hypotheses, O
among O
them O
we O
can O
find O
more O
translations O
of O
correct O
formality, O
up O
to O
a O
.959aver- O
age O
accuracy O
( O
+.106w.r.t. O
the O
model) O
for O
k= O
100 O
. O
The O
column O
“# O
Cases” O
shows O
that O
on O
average O
in O
up O
to21cases O
a O
hypothesis O
of O
the O
correct O
formality O
could O
be O
found O
with O
re-ranking. O
Finally, O
for O
any O
k, O
selecting O
the O
hypotheses O
with O
the O
correct O
formality O
(Oracle) O
in O
place O
of O
the O
most O
probable O
ones O
does O
(Model) O
not O
decrease O
translation O
quality, O
and O
may O
improve O
it O
Results O
of O
the O
oracle O
experiment. O
The O
used O
model O
was O
constrained O
and O
trained O
with O
the O
INFER B-MethodName
FULL I-MethodName
method, O
provided O
values O
are O
averaged O
across O
the O
devel- O
opment O
set. O
δto_bestdescribes O
the O
average O
distance O
to O
the O
first O
hypothesis O
of O
correct O
formality O
for O
cases O
where O
the O
most O
probable O
hypothesis O
is O
incorrect. O
The O
column O
“# O
Cases” O
quantifies O
that O
phenomenon. O
To O
re-rank O
the O
hypotheses O
we O
built O
a O
simple O
rel- O
ative O
frequency O
model O
from O
Validation O
accuracy O
plot O
showing O
the O
effect O
of O
applying O
FORMALITY O
RERANK O
to O
a O
list O
of O
kmodel O
hypotheses. O
the O
two O
sets, O
we O
calculated O
the O
count O
difference O
ratio O
and O
used O
it O
as O
the O
weight O
β: O
β(ti) O
=|Fcount(ti)−Icount(ti)| O
max O
tk∈T|Fcount(tk)−Icount(tk)| O
We O
additionally O
nullified O
probabilities O
for O
terms O
for O
which O
the O
difference O
of O
the O
number O
of O
occurrences O
in O
the O
formal O
and O
informal O
sets O
was O
lower O
than O
the O
third O
of O
total O
occurrences: O
κ(ti) O
=/braceleftigg O
0,if|Fcount O
(ti)−Icount O
(ti)| O
Fcount O
(ti)+Icount O
(ti)<0.333; O
1,otherwise O
The O
probabilities O
could O
now O
be O
calculated O
as O
p(F|ti) O
=Fcount(ti) O
count O
(ti)∗β(ti)∗κ(ti) O
p(I|ti) O
=Icount(ti) O
count O
(ti)∗β(ti)∗κ(ti) O
For O
a O
hypothesis O
Y, O
a O
source O
sentence O
Sand O
con- O
textsc,ˆc∈ O
{F,I}, O
c̸= O
ˆc, O
our O
objective O
function O
in O
translation O
thus O
became O
p(Y|X, O
c) O
=p(Y|X) O
+p(c|Y)−p(ˆc|Y) O
where O
shows O
how O
validation O
accuracy O
in- O
creases O
when O
this O
method O
is O
used, O
and O
that O
the O
model O
is O
now O
able O
to O
match O
the O
oracle O
accuracy O
for O
nearly O
every O
k. O
For O
k= O
100 O
the O
average O
improve- O
ment O
in O
accuracy O
is O
.102. O
The O
effect O
of O
model’saccuracy O
sometimes O
surpassing O
the O
oracle O
accuracy O
(e.g. O
for O
k= O
30 O
) O
is O
a O
by-product O
of O
slight O
sample O
size O
variations: O
the O
evaluation O
script O
scorer.py O
depends O
on O
phrase O
matches, O
and O
a O
sample O
is O
only O
counted O
for O
evaluation O
if O
a O
hypothesis O
has O
at O
least O
one O
phrase O
match O
against O
the O
formality-annotated O
reference. O
2.5 O
Model O
Selection: O
B O
ESTACCAVERAGING O
We O
fine-tuned O
each O
model O
for O
100K O
iterations O
on O
the O
MuST-C O
corpus O
with O
formality O
tags O
appended O
to O
relevant O
sentences. O
We O
then O
evaluated O
every O
checkpoint O
(saved O
each O
epoch) O
with O
scorer.py O
on O
IWSLT O
data. O
Our O
initial O
approach O
to O
selecting O
a O
model O
assumed O
averaging O
the O
last O
10 O
checkpoints O
from O
training. O
We O
experimented O
with O
an O
alterna- O
tive O
method O
to O
finding O
which O
checkpoints O
to O
aver- O
age: O
we O
first O
computed O
the O
accuracy O
on O
the O
IWSLT O
dataset B-DatasetName
for O
each O
checkpoint, O
and O
then O
selected O
a O
window O
of O
10 O
consecutive O
checkpoints O
with O
the O
highest O
average O
accuracy O
( O
BESTACCAVERAGING O
). O
2.6 O
Development O
Results O
We O
report O
the O
validation O
results O
in O
Table O
5. O
The O
first O
result O
we O
observed O
was O
that O
in O
both O
language O
pairs O
the O
pre-trained O
model O
(a O
strong O
baseline) O
learned O
adominant O
formality: O
formal O
for O
EN-DE(.853 O
accuracy O
to O
.147) O
and O
informal O
for O
EN-ES(.632 O
accuracy O
to O
.368). O
We O
observed O
that O
both O
methods O
( O
INFER B-MethodName
EASY I-MethodName
andINFER B-MethodName
FULL) I-MethodName
yield O
consistently O
better O
accu- O
racy O
for O
dominant O
formalities O
than O
non-dominant O
ones. O
Nevertheless, O
with O
INFER B-MethodName
FULL I-MethodName
we O
obtain O
an O
average O
+.474accuracy O
points O
over O
the O
baseline O
for O
non-dominant O
formalities; O
INFER B-MethodName
EASY I-MethodName
fails O
to O
learn O
meaningful O
control O
for O
non-dominant O
formal- O
ities. O
Based O
on O
these O
results O
we O
focused O
out O
later O
efforts O
on O
I O
NFER B-MethodName
FULL I-MethodName
alone. O
Continuing O
with O
INFER B-MethodName
FULL, I-MethodName
we O
noticed O
a O
sig- O
nificant O
improvement O
of O
up O
to O
+.223 O
accuracy O
points O
for O
( O
EN-DE,I) O
when O
using O
FORMALITY O
R- O
ERANK O
on O
top O
of O
standard O
beam O
search O
( O
k= O
100 O
) O
without O
impacting O
the O
translation O
quality. O
Finally, O
BESTACCAVERAGING O
helped O
bring O
the O
average O
accuracy B-MetricName
score O
up O
to O
.961without B-MetricValue
impacting O
trans- O
lation O
quality. O
2.7 O
Submitted O
Models O
Based O
on O
the O
validation O
results, O
we O
submitted O
two O
models O
to O
the O
constrained O
track: O
to O
the O
full O
su- O
pervision O
subtrack, O
we O
submitted O
the O
INFER B-MethodName
FULL I-MethodName
model O
with O
FORMALITY O
RERANK O
Results O
on O
the O
development O
sets O
for O
models O
built O
within O
the O
constrained O
track. O
BESTACCAVERAGING O
upgrades; O
for O
the O
zero-shot O
subtrack, O
we O
fine-tuned O
an O
alternative O
version O
of O
the O
model O
where O
we O
skipped O
the O
EN-{RU,IT}fine- O
tuning O
data, O
effectively O
making O
inference O
for O
these O
zero-shot O
pairs4. O
We O
used O
the O
same O
augments O
as O
in O
full O
supervision O
. O
3 O
Unconstrained O
Track O
Our O
submission O
for O
the O
unconstrained O
track O
largely O
copies O
the O
constrained O
track O
one, O
but O
is O
applied O
to O
a O
larger O
training O
corpus. O
3.1 O
Data O
Collection O
and O
Preprocessing O
We O
collect O
all O
datasets O
permitted O
by O
the O
organisers O
for O
our O
selected O
language O
pairs, O
including: O
•MuST-C B-DatasetName
(v1.2) O
(Di O
Gangi O
et O
al., O
2019), O
•Paracrawl B-DatasetName
(v9) O
(Bañón O
et O
al., O
2020), O
•WMT B-DatasetName
Corpora I-DatasetName
(from O
the O
News O
Translation O
task) O
(Barrault O
et O
al., O
2021): O
–NewsCommentary B-DatasetName
(v16) O
(Tiedemann, O
2012), O
– O
CommonCrawl B-DatasetName
(Smith O
et O
al., O
2013), O
– O
WikiMatrix B-DatasetName
(Schwenk O
et O
al., O
2021), O
– O
WikiTitles B-DatasetName
(v3) O
(Barrault O
et O
al., O
2020), O
– O
Europarl O
(v7, O
v10) O
(Koehn, O
2005), O
– O
UN B-DatasetName
(v1) O
(Ziemski O
et O
al., O
2016), O
– O
Tilde O
Rapid O
(Rozis O
and O
Skadin O
,š, O
2017), O
– O
Yandex5. O
We O
list O
data O
quantities O
as O
well O
as O
availability O
for O
all O
language O
pairs O
in O
Table O
6. O
We O
preprocessed O
the O
WMT B-DatasetName
and O
Paracrawl B-DatasetName
corpora: O
for O
both O
we O
first O
4We O
labelled O
a O
small O
random O
sample O
of O
training O
data O
with O
a O
random O
formality O
tag O
so O
the O
model O
learned O
to O
recognise O
the O
symbol O
as O
part O
of O
the O
input. O
5https://translate.yandex.ru/corpus? O
lang=en O
, O
accessed O
4 O
Apr O
2022.ran O
a O
simple O
rule-based O
heuristic O
of O
removing O
sen- O
tence O
pairs O
with O
sentences O
longer O
than O
250 O
tokens, O
and O
with O
a O
source-target O
ratio O
greater O
than O
1.5; O
re- O
moving O
non-ASCII O
characters O
on O
the O
English O
side, O
pruning O
some O
problematic O
sentences O
(e.g. O
links). O
We O
normalised O
punctuation O
using O
the O
script O
from O
Moses O
(Koehn O
et O
al., O
2007). O
We O
removed O
cases O
where O
either O
sentence O
is O
empty O
or O
where O
the O
source O
is O
the O
same O
as O
the O
target. O
Finally, O
we O
asserted O
that O
the O
case O
(lower/upper) O
of O
the O
first O
characters O
must O
be O
the O
same O
between O
source O
and O
target O
and O
that O
if O
either O
sentence O
ends O
in O
a O
punctuation O
mark, O
its O
counterpart O
must O
end O
in O
the O
same O
one. O
As O
the O
last O
step, O
we O
removed O
identical O
and O
very O
similar O
sen- O
tence O
pairs. O
After O
the O
initial O
preprocessing, O
we O
ran O
the O
Bi- O
Cleaner O
tool O
(Ramírez-Sánchez O
et O
al., O
2020) O
on O
each O
corpus; O
the O
algorithm O
assigns O
a O
confidence O
score∈[0,1]to O
each O
pair, O
measuring O
whether O
the O
sentences O
are O
good O
translations O
of O
each O
other, O
ef- O
fectively O
removing O
potentially O
noisy O
sentences. O
We O
removed O
all O
sentence O
pairs O
from O
the O
corpora O
which O
scored O
below O
0.7confidence. O
The O
final O
training O
data O
quantities O
are O
reported O
in O
Table O
6. O
3.2 O
Data O
Labelling O
Before O
we O
applied O
the O
same O
method O
to O
obtain O
fine- O
tuning O
data O
for O
the O
unconstrained O
track, O
we O
ob- O
served O
that O
many O
sentence O
pairs O
in O
this O
corpus O
are O
not O
dialogue, O
and O
hence O
useless O
for O
fine-tuning. O
As O
the O
first O
step, O
we O
used O
the O
original O
perplexity-based O
re-ranking O
algorithm O
to O
prune O
the O
unconstrained O
corpus. O
We O
used O
the O
MuST-C B-DatasetName
corpus O
as O
in-domain O
and O
all O
the O
unconstrained O
data O
as O
out-of-domain. O
We O
truncated O
the O
unconstrained O
set O
to O
the O
top O
5M O
sentences O
most O
like O
the O
MuST-C O
data. O
We O
then O
applied O
INFER O
FULL O
withαthreshold O
adapted O
to O
the O
data O
volume. O
The O
resulting O
data O
quantities O
can O
be O
found O
in O
the O
last O
row O
of O
Corpora O
containing O
training O
data O
used O
in O
the O
unconstrained O
experiments. O
Values O
indicate O
number O
of O
sentence O
pairs O
after O
preprocessing. O
3.3 O
Pre-training O
and O
Fine-tuning O
We O
used O
an O
identical O
model O
architecture O
to O
the O
one O
from O
the O
constrained O
track O
but O
extended O
the O
training O
time: O
we O
pre-trained O
for O
1.5M O
iterations O
(approx. O
1.5epochs) O
and O
fine-tuned O
for O
0.25M O
iterations O
(approx. O
47epochs). O
For O
fine-tuning, O
we O
used O
the O
MuST-C O
corpus O
(to O
maintain O
high O
translation O
quality) O
concatenated O
with O
the O
inferred O
formality- O
annotated O
data O
(to O
learn O
formality O
control). O
We O
ap- O
plied O
FORMALITY O
RERANK O
withk= O
50 O
, O
but O
not O
BESTACCAVERAGING O
as O
we O
found O
that O
the O
differ- O
ences O
in O
average O
accuracy O
for O
most O
checkpoints O
is O
minimal O
(and O
nears O
100); O
instead, O
we O
averaged O
the O
last10checkpoints. O
3.4 O
Development O
Results O
The O
development O
results O
(Table O
7) O
surpassed O
those O
achieved O
in O
the O
constrained O
track, O
presumably O
thanks O
to O
richer O
corpora O
extracted O
for O
both O
formal- O
ities. O
INFER O
FULL O
yielded O
near-perfect O
accuracy O
for O
all O
sets O
but O
( O
EN-DE,I), O
and O
applying O
FORMALI O
- O
TYRERANK O
effectively O
brought O
all O
scores O
up O
to O
a O
mean O
accuracy O
of O
.999. O
Our O
pre-trained O
model O
for O
this O
track O
achieved O
lower O
BLEU B-MetricName
scores O
than O
for O
the O
constrained O
track, O
which O
is O
explained O
by O
the O
test O
set O
coming O
from O
the O
same O
domain O
as O
the O
constrained O
training O
data. O
3.5 O
Submitted O
model O
Similarly O
to O
the O
constrained O
track, O
we O
submit O
two O
models O
to O
the O
unconstrained O
track: O
to O
the O
full O
super-vision O
subtrack, O
we O
submit O
the O
INFER O
FULL O
model O
with O
FORMALITY O
RERANK O
(k= O
50 O
); O
for O
the O
zero- O
shot O
subtrack, O
we O
fine-tune O
an O
alternative O
version O
of O
that O
in O
which O
we O
skip O
the O
EN-{RU,IT}fine-tuning O
data, O
effectively O
making O
inference O
for O
these O
pairs O
zero O
shot. O
4 O
Final O
Results O
We O
report O
the O
final O
evaluation O
results O
in O
Table O
8 O
(translation O
quality) O
and O
Table O
9 O
(formality O
control). O
In O
the O
latter O
we O
also O
provide O
the O
performance O
of O
our O
baseline O
(pre-trained) O
model O
for O
reference. O
Within O
the O
constrained O
track, O
we O
achieved O
near- O
ideal O
accuracy O
for O
the O
dominant O
formality O
for O
each O
language O
pair O
(between O
.961and1.000) O
with O
the O
supervised O
model. O
Scores O
for O
non-dominant O
formal- O
ities O
are O
weaker O
but O
still O
impressive O
for O
EN-{DE,ES} O
with O
an O
average O
of O
.880. O
Our O
best O
model O
for O
EN- O
{RU,IT}improved O
by O
.193accuracy O
points O
over O
the O
baseline. O
The O
models O
submitted O
to O
the O
uncon- O
strained O
track O
again O
achieved O
an O
impressive O
average O
accuracy O
of O
.992for O
dominant O
formality; O
addition- O
ally, O
performance O
for O
non-dominant O
formality O
in O
EN-{DE,ES}improved O
significantly O
w.r.t. O
the O
con- O
strained O
model, O
also O
averaging O
.992. O
This O
means O
that O
with O
enough O
training O
data O
our O
methods O
were O
capable O
of O
matching O
the O
performance O
on O
a O
minority O
class O
w.r.t. O
a O
majority O
class. O
Finally, O
contrary O
to O
the O
constrained O
track, O
the O
unconstrained-zero-shot O
model O
achieved O
the O
best O
accuracy O
for O
zero-shot O
pairs, O
to O
an O
average O
of O
Results O
on O
the O
development O
sets O
for O
models O
built O
within O
the O
unconstrained O
track. O
Translation O
quality O
results O
on O
the O
testsets O
for O
all O
submitted O
models. O
Numbers O
in O
brackets O
indicate O
number O
of O
model O
submitted. O
Accuracy B-MetricName
results O
on O
the O
testdata O
as O
measured O
by O
scorer.py O
. O
5 O
Conclusions O
Overall O
results O
suggest O
that O
it O
is O
easy O
for O
a O
pre- O
trained O
translation O
model O
to O
learn O
controlled O
ex- O
pression O
of O
the O
dominant O
type O
within O
a O
dichoto- O
mous O
phenomenon O
while O
learning O
to O
render O
the O
less-expressed O
type O
is O
significantly O
harder, O
espe- O
cially O
in O
a O
low-resource O
scenario. O
Our O
methods O
applied O
to O
the O
supervised O
language O
pairs O
(English- O
to-German, O
English-to-Spanish) O
worked O
near O
un- O
failingly, O
but O
using O
English O
as O
a O
pivot O
language O
to O
propagate O
formality O
information O
did O
not O
help O
achieve O
similar O
results O
for O
the O
zero-shot O
pairs. O
We O
suspect O
that O
the O
significant O
accuracy O
gains O
from O
FORMALITY O
RERANKING O
may O
have O
been O
par- O
tially O
due O
to O
formality O
in O
the O
studied O
language O
pairs O
itself O
being O
expressed O
primarily O
via O
certain O
token O
words O
such O
as O
the O
honorific O
Siein O
German O
creating O
apivot O
effect O
(Fu O
et O
al., O
2019). O
As O
such, O
it O
may O
be O
of O
interest O
for O
future O
research O
to O
study O
such O
meth- O
ods O
applied O
to O
more O
complex O
phenomena, O
such O
as O
grammatical O
expression O
of O
gender.Finally, O
results O
for O
the O
EN-{RU,IT}language O
pairs O
may O
not O
have O
been O
as O
good O
as O
expected O
be- O
cause O
we O
used O
the O
inferred O
data O
from O
the O
con- O
strained O
track O
to O
build O
the O
relative O
frequency O
model, O
but O
the O
inferred O
data O
turned O
out O
to O
be O
not O
as O
high O
quality O
as O
we O
expected. O
Future O
work O
may O
inves- O
tigate O
a O
robust O
solution O
to O
this O
problem O
of O
propa- O
gating O
formality O
via O
a O
source O
(pivot) O
language O
to O
extract O
training O
data O
for O
other O
language O
pairs. O
Code O
used O
for O
our O
implementation O
can O
be O
accessed O
at O
https://github.com/ O
st-vincent1/iwslt_formality_slt_ O
cdt_uos/ O
. O
Acknowledgements O
This O
work O
was O
supported O
by O
the O
Centre O
for O
Doc- O
toral O
Training O
in O
Speech O
and O
Language O
Tech- O
nologies O
(SLT) O
and O
their O
Applications O
funded O
by O
UK O
Research O
and O
Innovation O
[grant O
number O