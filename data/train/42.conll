Proceedings O
of O
the O
2nd O
Workshop O
on O
Deriving O
Insights O
from O
User-Generated O
Text O
, O
pages O
20 O
- O
24 O
May O
27, O
2022 O
©2022 O
Association O
for O
Computational O
Linguistics O
Bi-Directional B-MethodName
Recurrent I-MethodName
Neural I-MethodName
Ordinary I-MethodName
Differential I-MethodName
Equations I-MethodName
for O
Social B-TaskName
Media I-TaskName
Text I-TaskName
Classification I-TaskName
Maunika O
Tamire, O
Srinivas O
Anumasa, O
P.K. O
Srijith O
Computer O
Science O
and O
Engineering O
Indian O
Institute O
of O
Technology O
Hyderabad, O
India O
cs18mds11026@iith.ac.in, O
cs16resch11004@iith.ac.in, O
srijith@cse.iith.ac.in O
Abstract O
Classification O
of O
posts O
in O
social O
media O
such O
as O
Twitter O
is O
difficult O
due O
to O
the O
noisy O
and O
short O
nature O
of O
texts. O
Sequence O
classification O
models O
based O
on O
recurrent O
neural O
networks O
(RNN) O
are O
popular O
for O
classifying O
posts O
that O
are O
sequential O
in O
nature. O
RNNs O
assume O
the O
hidden O
represen- O
tation O
dynamics O
to O
evolve O
in O
a O
discrete O
man- O
ner O
and O
do O
not O
consider O
the O
exact O
time O
of O
the O
posting. O
In O
this O
work, O
we O
propose O
to O
use O
recur- B-MethodName
rent I-MethodName
neural I-MethodName
ordinary I-MethodName
differential I-MethodName
equations I-MethodName
(RN- O
ODE) O
for O
social B-TaskName
media I-TaskName
post I-TaskName
classification I-TaskName
which O
consider O
the O
time O
of O
posting O
and O
allow O
the O
com- O
putation O
of O
hidden O
representation O
to O
evolve O
in O
a O
time-sensitive O
continuous O
manner. O
In O
addi- O
tion, O
we O
propose O
a O
novel O
model, O
Bi-directional B-MethodName
RNODE I-MethodName
(Bi-RNODE), I-MethodName
which O
can O
consider O
the O
information O
flow O
in O
both O
the O
forward O
and O
back- O
ward O
directions O
of O
posting O
times O
to O
predict O
the O
post O
label. O
Our O
experiments O
demonstrate O
that O
RNODE B-MethodName
and O
Bi-RNODE B-MethodName
are O
effective O
for O
the O
problem O
of O
stance O
classification B-TaskName
of I-TaskName
rumours I-TaskName
in I-TaskName
social I-TaskName
media. I-TaskName
1 O
Introduction O
Information O
disseminated O
in O
social O
media O
such O
as O
Twitter O
can O
be O
useful O
for O
addressing O
several O
real- O
world O
problems O
like O
rumour O
detection, O
disaster O
management, O
and O
opinion O
mining. O
Most O
of O
these O
problems O
involve O
classifying O
social O
media O
posts O
into O
different O
categories O
based O
on O
their O
textual O
con- O
tent. O
For O
example, O
classifying O
the O
veracity O
of O
tweets O
as O
False, O
True, O
or O
unverified O
allows O
one O
to O
debunk O
the O
rumours O
evolving O
in O
social O
media O
(Zubiaga O
et O
al., O
2018a). O
However, O
social O
media O
text O
is O
ex- O
tremely O
noisy O
with O
informal O
grammar, O
typograph- O
ical O
errors, O
and O
irregular O
vocabulary. O
In O
addition, O
the O
character O
limit O
(240 O
characters) O
imposed O
by O
so- O
cial O
media O
such O
as O
Twitter O
make O
it O
even O
harder O
to O
perform O
text O
classification. O
Social O
media O
text O
classification, O
such O
as O
ru- O
mour O
stance O
classification1(Qazvinian O
et O
al., O
1Rumour O
stance O
classification O
helps O
to O
identify O
the O
veracity2011; O
Zubiaga O
et O
al., O
2016; O
Lukasik O
et O
al., O
2019) O
can O
be O
addressed O
effectively O
using O
sequence O
la- O
belling O
models O
such O
as O
long O
short O
term O
memory O
(LSTM) O
networks O
(Zubiaga O
et O
al., O
2016; O
Augen- O
stein O
et O
al., O
2016; O
Kochkina O
et O
al., O
2017; O
Zubiaga O
et O
al., O
2018b,a; O
Dey O
et O
al., O
2018; O
Liu O
et O
al., O
2019; O
Tian O
et O
al., O
2020). O
Though O
they O
consider O
the O
se- O
quential O
nature O
of O
tweets, O
they O
ignore O
the O
temporal O
aspects O
associated O
with O
the O
tweets. O
The O
time O
gap O
between O
tweets O
varies O
a O
lot O
and O
LSTMs O
ignore O
this O
irregularity O
in O
tweet O
occurrences. O
They O
are O
discrete O
state O
space O
models O
where O
hidden O
representation O
changes O
from O
one O
tweet O
to O
another O
without O
con- O
sidering O
the O
time O
difference O
between O
the O
tweets. O
Considering O
the O
exact O
times O
at O
which O
tweets O
occur O
can O
play O
an O
important O
role O
in O
determining O
the O
label. O
If O
the O
time O
gap O
between O
tweets O
is O
large, O
then O
the O
corresponding O
labels O
may O
not O
influence O
each O
other O
but O
can O
have O
a O
very O
high O
influence O
if O
they O
are O
closer. O
We O
propose O
to O
use O
recurrent B-MethodName
neural I-MethodName
ordi- I-MethodName
nary I-MethodName
differential I-MethodName
equations I-MethodName
(RNODE) I-MethodName
(Rubanova O
et O
al., O
2019) O
and O
developed O
a O
novel O
approach O
bi- B-MethodName
directional I-MethodName
RNODE I-MethodName
(Bi-RNODE), I-MethodName
which O
can O
natu- O
rally O
consider O
the O
temporal B-TaskName
information I-TaskName
to I-TaskName
perform I-TaskName
time I-TaskName
sensitive I-TaskName
classification I-TaskName
of I-TaskName
social I-TaskName
media I-TaskName
posts. I-TaskName
NODE O
(Chen O
et O
al., O
2018) O
is O
a O
continuous O
depth O
deep O
learning O
model O
that O
performs O
transformation O
of O
feature O
vectors O
in O
a O
continuous O
manner O
using O
or- O
dinary O
differential O
equation O
solvers. O
NODEs B-MethodName
bring O
parameter O
efficiency O
and O
address O
model O
selection O
in O
deep O
learning O
to O
a O
great O
extent. O
RNODE O
gen- O
eralizes O
RNN O
by O
extending O
NODE B-MethodName
for O
time-series O
data O
by O
considering O
temporal O
information O
associ- O
ated O
with O
the O
sequential O
data. O
Hidden O
representa- O
tions O
are O
changed O
continuously O
by O
considering O
the O
temporal O
information. O
We O
propose O
to O
use O
RNODE B-MethodName
for O
the O
task O
of O
se- B-TaskName
quence I-TaskName
labeling I-TaskName
of O
posts, O
which O
considers O
arrival O
times O
of O
the O
posts O
for O
updating O
hidden O
representa- O
of O
a O
rumour O
post O
by O
classifying O
the O
reply O
tweets O
into O
different O
stance O
classes O
such O
as O
Support, O
Deny, O
Question, O
Comment20tions O
and O
for O
classifying O
the O
post. O
In O
addition, O
we O
propose O
a O
novel O
model, O
Bi-RNODE, B-MethodName
which O
con- O
siders O
not O
only O
information O
from O
the O
past O
but O
also O
from O
the O
future O
in O
predicting O
the O
label O
of O
the O
post. O
Here, O
continuously O
evolving O
hidden O
representations O
in O
the O
forward O
and O
backward O
directions O
in O
time O
are O
combined O
and O
used O
to O
predict O
the O
post O
label. O
We O
show O
the O
effectiveness O
of O
the O
proposed O
models O
on O
the O
rumour B-TaskName
stance I-TaskName
classification I-TaskName
problem O
in O
Twit- O
ter O
using O
the O
RumourEval-2019 B-DatasetName
(Derczynski O
et O
al., O
2019) O
dataset. B-DatasetName
We O
found O
RNODE B-MethodName
and O
Bi-RNODE B-MethodName
can O
improve O
the O
social B-TaskName
media I-TaskName
text I-TaskName
classification I-TaskName
by O
effectively O
making O
use O
of O
the O
temporal O
information O
and O
is O
better O
than O
LSTMs O
and O
gated O
recurrent O
units O
(GRU) O
with O
temporal O
features. O
2 O
Background O
We O
consider O
the O
problem O
of O
classifying O
social O
me- O
dia O
posts O
into O
different O
classes. O
Let O
Dbe O
a O
collec- O
tion O
of O
Nposts, O
D={pi}N O
i=1. O
Each O
post O
piis O
assumed O
to O
be O
a O
tuple O
containing O
information O
such O
as O
textual O
and O
contextual O
features O
xi, O
time O
of O
the O
posttiand O
the O
label O
associated O
with O
the O
post O
yi, O
thuspi={(xi, O
ti, O
yi)}. O
Our O
aim O
is O
to O
develop O
a O
sequence O
classification O
model O
which O
considers O
the O
temporal O
information O
tialong O
with O
xifor O
classify- O
ing O
a O
social O
media O
post. O
In O
particular, O
we O
consider O
the O
rumour O
stance O
classification O
problem O
in O
Twitter O
where O
one O
classifies O
tweets O
into O
Support, O
Query, O
Deny, O
and O
Comment O
class, O
thus O
yi∈Y={Support, O
Query, O
Deny, O
Comment}. O
2.1 O
Neural O
Ordinary O
Differential O
Equations O
NODE B-MethodName
were O
introduced O
as O
a O
continuous O
depth O
alternative O
to O
Residual B-MethodName
Networks I-MethodName
(ResNets) I-MethodName
(He O
et O
al., O
2016). O
ResNets O
uses O
skip O
connections O
to O
avoid O
vanishing O
gradient O
problems O
when O
networks O
grow O
deeper. O
Residual O
block O
output O
is O
computed O
asht+1=ht+f(ht, O
θt), O
where O
f()is O
a O
neural O
network O
(NN) O
parameterized O
by O
θtandhtrepre- O
senting O
the O
hidden O
representation O
at O
depth O
t. O
This O
update O
is O
similar O
to O
a O
step O
in O
Euler O
numerical O
technique O
used O
for O
solving O
ordinary O
differential O
equations O
(ODE)dh(t) O
dt=f(h(t), O
t, O
θ). O
The O
se- O
quence O
of O
residual O
block O
operations O
in O
ResNets O
can O
be O
seen O
as O
a O
solution O
to O
this O
ODE. O
Consequently, O
NODEs B-MethodName
can O
be O
interpreted O
as O
a O
continuous O
equiva- O
lent O
of O
ResNets O
modeling O
the O
evolution O
of O
hidden O
representations O
h(t)over O
time. O
For O
solving O
ODE, O
one O
can O
use O
fixed O
step- O
size O
numerical O
techniques O
such O
as O
Euler, O
Runge- O
Kutta O
or O
adaptive O
step-size O
methods O
like O
Do- O
pri5(Dormand O
and O
Prince, O
1980). O
Solving O
an O
Figure O
1: O
Architecture O
details O
of O
RNODE B-MethodName
ODE O
requires O
one O
to O
specify O
an O
initial O
value O
h(0)(input O
xor O
its O
transformation) O
and O
can O
compute O
the O
value O
at O
tusing O
an O
ODE O
solver O
ODESolverCompute O
(fθ,h(0),0, O
t). O
An O
ODE O
is O
solved O
until O
some O
end-time O
Tto O
obtain O
the O
fi- O
nal O
hidden O
representation O
h(T)which O
is O
used O
to O
predict O
class O
labels O
ˆy. O
For O
classification O
problems, O
cross-entropy O
loss O
is O
used O
and O
parameters O
are O
learnt O
through O
adjoint O
sensitivity O
method O
(Zhuang O
et O
al., O
2020; O
Chen O
et O
al., O
2018) O
which O
provides O
efficient O
back-propagation O
and O
gradient O
computations. O
3 O
Bi-Directional B-MethodName
Recurrent I-MethodName
NODE I-MethodName
LSTMs I-MethodName
are O
popular O
for O
sequence O
classification O
but O
only O
considers O
the O
sequential O
nature O
of O
the O
data O
and O
ignore O
the O
temporal O
features O
associated O
with O
the O
data O
in O
its O
standard O
setting. O
As O
the O
posts O
occur O
in O
irregular O
intervals O
of O
time, O
the O
nature O
of O
a O
new O
post O
will O
be O
influenced O
by O
the O
recent O
posts, O
influence O
will O
be O
inversely O
proportional O
to O
the O
time O
gap. O
In O
these O
situations, O
it O
will O
be O
beneficial O
to O
use O
a O
model O
where O
the O
number O
of O
transformations O
depend O
on O
the O
time O
gap. O
We O
propose O
to O
use O
RNODE B-MethodName
which O
considers O
the O
arrival O
time O
and O
accordingly O
the O
hidden O
representa- O
tions O
are O
transformed O
across O
time. O
In O
RNODE, B-MethodName
the O
transformation O
of O
a O
hidden O
representation O
h(ti−1) O
at O
time O
ti−1toh(ti)at O
time O
tiis O
governed O
by O
an O
ODE O
parameterized O
by O
a O
NN O
f(). O
Unlike O
standard O
LSTMs O
where O
h(ti)is O
obtained O
from O
h(ti−1)as O
a O
single O
NN O
transformation, O
RNODE O
first O
obtains O
a O
hidden O
representation O
h′(ti)as O
a O
solution O
to O
an O
ODE O
at O
time O
tiwith O
initial O
value O
h(ti−1). O
The O
number O
of O
update O
steps O
in O
the O
numerical O
technique O
used O
to O
solve O
this O
ODE O
depends O
on O
the O
time O
gap O
ti−ti−1between O
the O
consecutive O
posts. O
The O
hidden O
representation O
h′(ti)and O
input O
post O
xiat O
time O
ti O
are O
passed O
through O
neural O
network O
transformation O
(RNNCell()) O
to O
obtain O
final O
hidden O
representation O
h(ti), O
i.e., O
h(ti)= O
RNNCell( O
h′(ti),xi). O
The O
pro- O
cess O
is O
repeated O
for O
every O
element O
(xi, O
ti)in O
the O
sequence. O
The O
hidden O
representations O
associated O
with O
the O
elements O
in O
the O
sequence O
are O
then O
passed O
to O
a O
neural O
network O
(NN()) O
to O
obtain O
the O
post O
labels. O
Using O
standard O
cross-entropy O
loss, O
the O
parameters O
of O
the O
models O
are O
learnt O
through O
backpropagation. O
Figure O
1 O
provides O
the O
detailed O
architecture O
of O
the21Figure O
2: O
Bi-RNODE O
Architecture O
RNODE O
model. O
Bi-directional O
RNNs O
(Schuster O
and O
Paliwal, O
1997) O
such O
as O
Bi-LSTMS O
(Graves O
et O
al., O
2013) O
were O
proven O
to O
be O
successful O
in O
many O
sequence O
labeling O
tasks O
in O
natural O
language O
processing O
such O
as O
POS O
tagging O
(Huang O
et O
al., O
2015). O
They O
use O
the O
infor- O
mation O
from O
the O
past O
and O
future O
to O
predict O
the O
label O
while O
standard O
LSTMs O
consider O
only O
information O
from O
the O
past. O
We O
propose O
a O
Bi-RNODE B-HyperparameterName
model, O
which O
uses O
the O
sequence O
of O
input O
observations O
from O
past O
and O
from O
the O
future O
to O
predict O
the O
post O
label O
at O
any O
time O
t. O
It O
assumes O
the O
hidden O
representation O
dynamics O
are O
influenced O
not O
only O
by O
the O
past O
posts O
but O
also O
by O
the O
futures O
posts. O
Unlike O
Bi-LSTMs, O
Bi- B-MethodName
RNODE I-MethodName
considers O
the O
exact O
time O
of O
the O
posts O
and O
their O
inter-arrival O
times O
in O
determining O
the O
transfor- O
mations O
in O
the O
hidden O
representations. O
Bi-RNODE B-MethodName
consists O
of O
two O
RNODE B-MethodName
blocks, O
one O
performing O
transformations O
in O
the O
forward O
direction O
(in O
the O
or- O
der O
of O
posting O
times) O
and O
the O
other O
in O
the O
backward O
direction. O
The O
hidden O
representations O
HandHb O
computed O
by O
forward O
and O
backward O
RNODE B-MethodName
re- O
spectively O
are O
aggregated O
either O
by O
concatenation O
or O
by O
averaging O
appropriately O
to O
obtain O
a O
final O
hid- O
den O
representation O
and O
is O
passed O
through O
a O
NN O
to O
obtain O
the O
post O
labels. O
Bi-RNODE B-MethodName
is O
useful O
when O
a O
sequence O
of O
posts O
with O
their O
time O
of O
occurrence O
needs O
to O
be O
classified O
together. O
Figure O
2 O
provides O
an O
overview O
of O
Bi-RNODE B-MethodName
model O
for O
post O
classification. O
For B-MethodName
Bi-RNODE, I-MethodName
an O
extra O
neural O
network O
fθ′()is O
required O
to O
compute O
hidden O
representations O
hb(t′ O
i)in O
the O
backward O
di- O
rection. O
Training O
in O
Bi-RNODE B-MethodName
is O
done O
in O
a O
similar O
manner O
to O
RNODE, B-HyperparameterName
with O
cross-entropy O
loss O
and O
back-propagation O
to O
estimate O
parameters. O
4 O
Experiments O
To O
demonstrate O
the O
effectiveness O
of O
the O
proposed O
approaches, O
we O
consider O
the O
stance O
classification O
problem O
in O
Twitter B-DatasetName
and I-DatasetName
RumourEval-2019 I-DatasetName
(Der- O
czynski O
et O
al., O
2019) O
data O
set. O
This O
Twitter O
data O
set O
consists O
of O
rumours O
associated O
with O
eight O
events. O
Each O
event O
has O
a O
collection O
of O
tweets O
labelled O
with O
one O
of O
the O
four O
labels O
- O
Support, O
Query, O
Denyand O
Comment. O
We O
picked O
four O
major O
events O
Charliehebdo, O
Ferguson, O
Ottawashooting O
and O
Syd- O
neysiege O
(each O
with O
approximately O
1000 O
tweets O
per O
event) O
from O
RumourEval-2019 B-DatasetName
to O
perform O
experi- O
ments. O
Features O
: O
For O
dataset O
preparation, O
each O
data O
point O
xiassociated O
with O
a O
Tweet O
includes O
text O
em- O
bedding, O
retweet O
count, O
favourites O
count, O
punctu- O
ation O
features, O
negative O
and O
positive O
word O
count, O
presence O
of O
hashtags, O
user O
mentions, O
URLs O
etc. O
ob- O
tained O
from O
the O
tweet. O
The O
text O
embedding O
of O
the O
tweet O
is O
obtained O
by O
concatenating O
the O
word O
em- O
beddings2. O
Each O
tweet O
timestamp O
is O
converted O
to O
epoch O
time O
and O
Min-Max B-HyperparameterName
normalization I-HyperparameterName
is O
applied O
over O
the O
time O
stamps O
associated O
with O
each O
event O
to O
keep O
the O
duration O
of O
the O
event O
in O
the O
interval O
[0,1]. B-HyperparameterValue
4.1 O
Experimental O
setup O
We O
conducted O
experiments O
to O
predict O
the O
stance O
of O
social O
media O
posts O
propagating O
in O
seen O
events O
and O
unseen O
events O
. O
-Seen O
Event O
Here O
we O
train, O
validate O
and O
test O
on O
tweets O
of O
the O
same O
event. O
Each O
event O
data O
is O
split B-HyperparameterName
60:20:20 B-HyperparameterValue
ratio O
in O
sequence O
of O
time. O
This O
setup O
helps O
in O
predicting O
the O
stance O
of O
unseen O
tweets O
of O
the O
same O
event. O
-Unseen O
Event O
: O
This O
setup O
helps O
in O
evaluating O
performance O
on O
an O
unseen O
event O
and O
training O
on O
a O
larger O
dataset. O
Here, O
training O
and O
validation O
data O
are O
formed O
using O
data O
from O
3 O
events O
and O
testing O
is O
done O
on O
the O
4thevent. O
Last O
20% O
of O
the O
training O
data O
(after O
ordering O
based O
on O
time) O
are O
set O
aside O
for O
validation. O
During O
training, O
mini-batches O
are O
formed O
only O
from O
the O
tweets O
belonging O
to O
the O
same O
event. O
Baselines O
: O
We O
compared O
results O
of O
our O
proposed O
RNODE B-MethodName
and O
Bi-RNODE B-MethodName
models O
with O
RNN O
based O
baselines O
such O
LSTM O
(Kochkina O
et O
al., O
2017), O
Bi- O
LSTM O
(Augenstein O
et O
al., O
2016), O
GRU O
(Cho O
et O
al., O
2014), O
Bi-GRU, O
and O
Majority O
(labelling O
with O
most O
frequent O
class) O
baseline O
models. O
We O
also O
use O
a O
variant O
of O
LSTM O
baseline O
considering O
temporal O
in- O
formation O
(Zubiaga O
et O
al., O
2018b), O
LSTM-timeGap O
where O
the O
time O
gap O
of O
consecutive O
data O
points O
is O
included O
as O
part O
of O
the O
input O
data. O
Evaluation O
Metrics O
: O
We O
consider O
the O
standard O
evaluation O
metrics O
such O
as O
precision, B-MetricName
recall, B-MetricName
F1 B-MetricName
and O
in O
addition O
the O
AUC B-MetricName
score O
to O
account O
for O
the O
data O
imbalance. O
We O
consider O
a O
weighted O
average O
of O
the O
2Using O
pre-trained O
word2vec O
vectors O
which O
are O
trained O
on O
Google B-DatasetName
News I-DatasetName
dataset: O
https://code.google.com/p/word2vec, O
each O
word O
is O
represented O
as O
an O
embedding O
of O
size O
15.22(a) O
RNODE O
(b) O
LSTM O
(c) O
GRU O
(d) O
Bi-RNODE O
(e) O
Bi-LSTM O
(f) O
Bi-GRU O
Figure O
3: O
ROC O
curves O
of O
different O
models O
trained O
on O
sydneysiege O
event O
for O
seen O
event O
experimental O
setup. O
Bi-RNODE O
exhibits O
better O
AUC O
and O
class O
separability O
overall O
classes. O
First O
and O
second O
rows O
of O
each O
model O
represents O
seen O
event O
andunseen O
event O
experiment O
results O
respectively. O
evaluation O
metrics O
to O
compare O
the O
performance O
of O
models. O
Hyperparameters O
: O
All O
the O
models O
are O
trained O
for O
50 B-HyperparameterValue
epochs B-HyperparameterName
with O
0.01 B-HyperparameterValue
learning O
rate, O
Adam B-HyperparameterValue
op- I-HyperparameterValue
timizer, I-HyperparameterValue
dropout(0.2) B-HyperparameterName
regularizer, B-HyperparameterName
batchsize B-HyperparameterName
of O
50, B-HyperparameterValue
hidden O
representation O
size O
of O
64 B-HyperparameterValue
and O
cross B-HyperparameterValue
entropy I-HyperparameterValue
as O
the O
loss B-HyperparameterName
function. I-HyperparameterName
Different O
hyperparameters O
like O
neural O
network O
layers O
(1, O
2), O
numerical O
methods O
(Euler, O
RK4, O
Dopri5 O
for O
RNODE O
and O
Bi-RNODE) O
and O
aggregation O
strategy O
(concatenation O
or O
aver- O
aging O
for O
Bi-LSTM O
Bi-GRU O
and O
Bi-RNODE) O
are O
used O
for O
all O
the O
models O
and O
the O
best O
configuration O
is O
selected O
from O
the O
validation O
data O
for O
different O
experimental O
setups O
and O
train/test O
data O
splits. O
4.2 O
Results O
and O
Analysis O
The O
results O
of O
seen O
event O
andunseen O
event O
experi- O
ment O
setup O
can O
be O
found O
in O
Table O
1, O
where O
the O
first O
and O
second O
rows O
for O
each O
model O
provides O
results O
on O
seen O
event O
andunseen O
event O
respectively. O
We O
can O
observe O
from O
Table O
1 O
that O
for O
both O
seen O
event O
and O
unseen O
event O
experiment O
setup, O
RNODE B-MethodName
and O
Bi-RNODE B-MethodName
models O
performed O
better O
than O
the O
baseline O
models O
in O
general O
for O
all O
the O
3 O
events3. O
In O
particular O
for O
the O
seen O
event O
setup, O
Bi-RNODE O
gives O
the O
best O
result O
outperforming O
RNODE B-MethodName
and O
other O
models O
for O
most O
of O
the O
data O
sets O
and O
measures. O
Under O
seen O
event O
experiment O
on O
Syndneysiege O
event, O
we O
plot O
the O
ROC O
curve O
for O
all O
the O
models O
in O
Figure O
3. O
We O
can O
observe O
that O
AUC O
for O
Figures O
3(a) O
and O
3(e) O
corresponding O
to O
RNODE B-MethodName
and O
Bi-RNODE B-MethodName
respec- O
tively O
are O
higher O
than O
LSTM, B-MethodName
GRU, B-MethodName
Bi-LSTM B-MethodName
, O
and O
Bi-GRU. B-MethodName
5 O
Conclusion O
We O
proposed O
RNODE, B-MethodName
Bi-RNODE B-MethodName
models O
for O
se- O
quence O
classification O
of O
social O
media O
posts. O
These O
models O
consider O
temporal O
information O
of O
the O
posts O
and O
hidden O
representation O
are O
evolved O
as O
solution O
to O
ODE. O
Through O
experiments, O
we O
show O
these O
mod- O
els O
perform O
better O
than O
LSTMs B-MethodName
on O
rumour O
stance O
classification O
problem O
in O
Twitter O
3Due O
to O
space O
constraint, O
Table O
1 O
presents O
results O
for O
3 O
events, O
Syndneysiege O
results O
in O
Figure O